[
  {
    "objectID": "reading stats/Kobo Stats Output.html",
    "href": "reading stats/Kobo Stats Output.html",
    "title": "Jordan Chong",
    "section": "",
    "text": "# ---\n# title: My Reading Stats Dashboard\n# hide_code: true\n# ---\n\n# ==============================================================================\n# Cell 1: Setup, Data Loading, and Helper Functions\n# ==============================================================================\n# Import all the necessary libraries for data analysis and visualization.\nimport json\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport calmap\nimport warnings\nfrom collections import defaultdict\nimport io\nimport base64\n\n# Import Panel for interactive dashboarding\nimport panel as pn\npn.extension() # Initialize Panel extension for Jupyter\n\n# Suppress pandas warnings for cleaner output\nwarnings.filterwarnings('ignore', category=UserWarning, module='calmap')\n\n# Load the JSON data from the file.\njson_file_path = '/Users/jordanchong/my-website/reading stats/tc_readstat.json'\ndata = None # Initialize data to None\ntry:\n    with open(json_file_path, 'r') as f:\n        data = json.load(f)\n    print(f\"‚úÖ Data loaded successfully from {json_file_path}.\")\nexcept FileNotFoundError:\n    print(f\"‚ùå Error: '{json_file_path}' not found. Please ensure the file is in the specified directory.\")\n\n# --- Helper Functions ---\ndef get_title(book_id):\n    \"\"\"Fetches a book title from its ID.\"\"\"\n    if data:\n        return data['contents'].get(book_id, {}).get('title', 'Unknown Title')\n    return 'Unknown Title'\n\ndef get_author(book_id):\n    \"\"\"Fetches a book's author from its ID.\"\"\"\n    if data:\n        return data['contents'].get(book_id, {}).get('author', 'Unknown Author')\n    return 'Unknown Author'\n\ndef get_words(book_id):\n    \"\"\"Fetches a book's word count from its ID.\"\"\"\n    if data:\n        return data['contents'].get(book_id, {}).get('words', 0)\n    return 0\n\ndef is_article(book_id):\n    \"\"\"Checks if the content is an article (book == False).\"\"\"\n    if data:\n        return not data['contents'].get(book_id, {}).get('book', True)\n    return False\n\ndef format_duration(seconds):\n    \"\"\"Formats seconds into a human-readable string (e.g., '1 hours 23 minutes 45 seconds').\"\"\"\n    td = timedelta(seconds=int(seconds))\n    hours, remainder = divmod(td.seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    return f\"{hours} hours {minutes} minutes {seconds} seconds\"\n\ndef format_duration_short(seconds):\n    \"\"\"Formats seconds into a short h m s string (e.g., '1h23m45s').\"\"\"\n    td = timedelta(seconds=int(seconds))\n    hours, remainder = divmod(td.seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    return f\"{hours}h{minutes}m{seconds}s\"\n\ndef estimate_reading_time(word_count, wpm=200):\n    \"\"\"Estimates reading time in a human-readable format (e.g., '4h 23min').\"\"\"\n    if wpm &lt;= 0 or word_count &lt;= 0:\n        return \"N/A\"\n    \n    total_minutes = word_count / wpm\n    hours = int(total_minutes / 60)\n    minutes = int(total_minutes % 60)\n    \n    if hours &gt; 0 and minutes &gt; 0:\n        return f\"{hours}h {minutes}min\"\n    elif hours &gt; 0:\n        return f\"{hours}h\"\n    elif minutes &gt; 0:\n        return f\"{minutes}min\"\n    else:\n        return \"&lt; 1min\"\n\n\n# ==============================================================================\n# Cell 2: Data Preprocessing\n# (This cell runs all calculations and stores results in processed_data)\n# ==============================================================================\nprocessed_data = {}\nif data:\n    # Reading Speed (WPM) for Finished Books\n    finished_books_ids = {\n        book_id for book_id, events in data['events'].items()\n        if any(e['event'] == 'Finish' for e in events)\n    }\n    processed_data['finished_books_ids'] = finished_books_ids\n\n    wpm_results = {}\n    for book_id in finished_books_ids:\n        total_duration = sum(e['duration'] for e in data['events'].get(book_id, []) if e['event'] == 'Read')\n        word_count = get_words(book_id)\n        if total_duration &gt; 60 and word_count &gt; 100:\n            wpm = (word_count / total_duration) * 60\n            wpm_results[get_title(book_id)] = f\"{wpm:.2f} WPM\"\n        else:\n            wpm_results[get_title(book_id)] = \"WPM not available\"\n    processed_data['wpm_results'] = wpm_results\n\n    # Session Data (for Longest/Average Session)\n    read_durations = [\n        event['duration'] for events in data['events'].values()\n        for event in events if event['event'] == 'Read'\n    ]\n    processed_data['read_durations'] = read_durations\n    \n    processed_data['avg_duration_sec'] = sum(read_durations) / len(read_durations) if read_durations else 0\n    processed_data['max_duration_sec'] = max(read_durations) if read_durations else 0\n\n    # Daily Reading Streak\n    read_dates = {\n        datetime.fromisoformat(event['time'].replace('.000', '')).date()\n        for events in data['events'].values()\n        for event in events if event['event'] == 'Read'\n    }\n    processed_data['read_dates'] = sorted(list(read_dates))\n\n    # Daily Reading Time for Heatmap\n    daily_read_time = {}\n    for events in data['events'].values():\n        for event in events:\n            if event['event'] == 'Read':\n                read_time = datetime.fromisoformat(event['time'].replace('.000', ''))\n                date = read_time.date()\n                daily_read_time[date] = daily_read_time.get(date, 0) + (event['duration'] / 60) # minutes\n    processed_data['daily_read_time'] = daily_read_time\n\n    sorted_dates = processed_data['read_dates']\n    longest_streak = 0\n    current_streak = 0\n    if sorted_dates:\n        longest_streak = 1\n        current_streak = 1\n        for i in range(1, len(sorted_dates)):\n            if sorted_dates[i] == sorted_dates[i-1] + timedelta(days=1):\n                current_streak += 1\n            else:\n                longest_streak = max(longest_streak, current_streak)\n                current_streak = 1\n        longest_streak = max(longest_streak, current_streak)\n        \n        today = datetime.now().date()\n        if sorted_dates and (today - sorted_dates[-1]).days &gt; 1:\n            current_streak = 0\n        elif sorted_dates and len(sorted_dates) == 1 and (today - sorted_dates[-1]).days == 0:\n             current_streak = 1\n    \n    # For heatmap legend\n    if daily_read_time:\n        min_daily_read = min(daily_read_time.values())\n        max_daily_read = max(daily_read_time.values())\n        processed_data['min_daily_read_time_min'] = min_daily_read\n        processed_data['max_daily_read_time_min'] = max_daily_read\n    else:\n        processed_data['min_daily_read_time_min'] = 0\n        processed_data['max_daily_read_time_min'] = 0\n\n    # Time of Day Analysis (Total Duration)\n    time_of_day_total_durations = defaultdict(int)\n    for events in data['events'].values():\n        for event in events:\n            if event['event'] == 'Read':\n                hour = datetime.fromisoformat(event['time'].replace('.000', '')).hour\n                duration = event['duration']\n                if 5 &lt;= hour &lt; 12: period = 'Morning (5am-12pm)'\n                elif 12 &lt;= hour &lt; 17: period = 'Afternoon (12pm-5pm)'\n                elif 17 &lt;= hour &lt; 21: period = 'Evening (5pm-9pm)'\n                else: period = 'Night (9pm-5am)'\n                time_of_day_total_durations[period] += duration\n    processed_data['time_of_day_total_durations'] = time_of_day_total_durations\n\n    # Vocabulary Builder\n    processed_data['word_list'] = data.get('word_list', [])\n\n    # Highlight & Note Analysis\n    highlights_raw = data.get('bookmark', {})\n    all_highlights_formatted = []\n    for book_id, book_highlights in highlights_raw.items():\n        title = get_title(book_id)\n        author = get_author(book_id)\n        for highlight in book_highlights:\n            if 'text' in highlight:\n                all_highlights_formatted.append({\n                    'book_id': book_id,\n                    'title': title,\n                    'author': author,\n                    'text': highlight['text'],\n                    'time': datetime.fromisoformat(highlight['created'].replace('.000', '')) # For sorting\n                })\n    processed_data['all_highlights_formatted'] = sorted(all_highlights_formatted, key=lambda x: (x['book_id'], x['time']))\n\n    # Books Currently Being Read (formerly DNF)\n    books_with_reads = {\n        book_id for book_id, events in data['events'].items()\n        if any(e['event'] == 'Read' for e in events)\n    }\n    processed_data['books_currently_being_read'] = books_with_reads - finished_books_ids\n\n    # Detailed Yearly and Monthly Breakdown (raw stats)\n    book_stats_raw = defaultdict(lambda: {\n        'read_sessions': [],\n        'finish_date': None,\n        'is_article': False\n    })\n    for book_id, events in data['events'].items():\n        for event in events:\n            dt = datetime.fromisoformat(event['time'].replace('.000', ''))\n            if event['event'] == 'Read':\n                book_stats_raw[book_id]['read_sessions'].append({'date': dt, 'duration': event['duration']})\n            elif event['event'] == 'Finish':\n                book_stats_raw[book_id]['finish_date'] = dt\n            book_stats_raw[book_id]['is_article'] = is_article(book_id)\n    for book_id, stats in book_stats_raw.items():\n        if stats['read_sessions']:\n            stats['start_date'] = min(s['date'] for s in stats['read_sessions'])\n            stats['total_duration'] = sum(s['duration'] for s in stats['read_sessions'])\n            stats['session_count'] = len(stats['read_sessions'])\n    processed_data['book_stats_raw'] = book_stats_raw\nelse:\n    processed_data = None # Ensure processed_data is None if data loading failed\n\n# ==============================================================================\n# Cell 3: Define Dashboard Sections (Panel Objects)\n# ==============================================================================\n# We define each \"tab\" content as a Panel object (pn.Column or pn.Row)\n# These will be passed to pn.Tabs later.\n\nyearly_monthly_report_pane = pn.Column()\nstreaks_calendar_pane = pn.Column()\ntime_of_day_pane = pn.Column()\nhighlights_vocab_pane = pn.Column()\nbooks_being_read_pane = pn.Column()\n\nif processed_data:\n    # --- Yearly/Monthly Report Content ---\n    now = datetime.now() # Get current date for filtering\n    year_to_process = now.year\n    current_month = now.month\n\n    yearly_total_book_time = 0\n    yearly_total_article_time = 0\n    yearly_finished_books = 0\n    yearly_finished_articles = 0\n    \n    monthly_data = defaultdict(lambda: {\n        'book_time': 0, 'article_time': 0, 'finished_books': set(), \n        'finished_articles': set(), 'started_books': set(), 'continued_books': set()\n    })\n\n    for book_id, stats in processed_data['book_stats_raw'].items():\n        for session in stats['read_sessions']:\n             if session['date'].year == year_to_process and session['date'].month &lt;= current_month:\n                if stats['is_article']:\n                    yearly_total_article_time += session['duration']\n                else:\n                    yearly_total_book_time += session['duration']\n\n        if stats['finish_date'] and stats['finish_date'].year == year_to_process and stats['finish_date'].month &lt;= current_month:\n            if stats['is_article']:\n                yearly_finished_articles += 1\n            else:\n                yearly_finished_books += 1\n        \n        for session in stats['read_sessions']:\n            if session['date'].year == year_to_process and session['date'].month &lt;= current_month:\n                month = session['date'].month\n                if stats['is_article']:\n                    monthly_data[month]['article_time'] += session['duration']\n                else:\n                    monthly_data[month]['book_time'] += session['duration']\n\n        if stats.get('start_date') and stats['start_date'].year == year_to_process and stats['start_date'].month &lt;= current_month:\n            start_month = stats['start_date'].month\n            monthly_data[start_month]['started_books'].add(book_id)\n\n        if stats['finish_date'] and stats['finish_date'].year == year_to_process and stats['finish_date'].month &lt;= current_month:\n            finish_month = stats['finish_date'].month\n            if stats['is_article']:\n                monthly_data[finish_month]['finished_articles'].add(book_id)\n            else:\n                monthly_data[finish_month]['finished_books'].add(book_id)\n    \n    for month in range(1, current_month + 1):\n        books_read_this_month = {\n            book_id for book_id, stats_raw in processed_data['book_stats_raw'].items()\n            for s in stats_raw['read_sessions'] if s['date'].year == year_to_process and s['date'].month == month\n        }\n        started_this_month = monthly_data[month]['started_books']\n        monthly_data[month]['continued_books'] = books_read_this_month - started_this_month - monthly_data[month]['finished_books']\n\n    yearly_monthly_report_pane.append(pn.pane.Markdown(\n        f\"### Report for {year_to_process} up to {now.strftime('%B')}\\n\"\n        f\"Finished books: {yearly_finished_books}\\n\"\n        f\"Finished articles: {yearly_finished_articles}\\n\"\n        f\"Time reading books: {format_duration(yearly_total_book_time)} (hours: {yearly_total_book_time/3600:.2f})\\n\"\n        f\"Time reading articles: {format_duration(yearly_total_article_time)} (hours: {yearly_total_article_time/3600:.2f})\\n\"\n        f\"Total time reading: {format_duration(yearly_total_book_time + yearly_total_article_time)} (hours: {(yearly_total_book_time + yearly_total_article_time)/3600:.2f})\"\n    ))\n    yearly_monthly_report_pane.append(pn.pane.Markdown(\"---\"))\n\n    yearly_monthly_report_pane.append(pn.pane.Markdown(\n        f\"### Daily Reading Streaks\\n\"\n        f\"üî• Longest Reading Streak: {longest_streak} days\\n\"\n        f\"üöÄ Current Reading Streak: {current_streak} days\"\n    ))\n    yearly_monthly_report_pane.append(pn.pane.Markdown(\"---\"))\n\n\n    for month in range(1, current_month + 1):\n        month_name = datetime(year_to_process, month, 1).strftime('%B')\n        yearly_monthly_report_pane.append(pn.pane.Markdown(\n            f\"#### {month_name} {year_to_process} - Finished books: {len(monthly_data[month]['finished_books'])}, articles: {len(monthly_data[month]['finished_articles'])}, \"\n            f\"time spend reading books: {format_duration(monthly_data[month]['book_time'])} (hours: {monthly_data[month]['book_time']/3600:.2f}) and \"\n            f\"articles: {format_duration(monthly_data[month]['article_time'])} (hours: {monthly_data[month]['article_time']/3600:.2f})\"\n        ))\n        \n        # Finished Books\n        if monthly_data[month]['finished_books']:\n            yearly_monthly_report_pane.append(pn.pane.Markdown(\"**Finished Books:**\"))\n            finished_books_list_items = []\n            sorted_finished_books = sorted(list(monthly_data[month]['finished_books']), \n                                           key=lambda b: processed_data['book_stats_raw'][b]['finish_date'] or processed_data['book_stats_raw'][b]['start_date'] or get_title(b))\n            for book_id in sorted_finished_books:\n                book_info = processed_data['book_stats_raw'][book_id]\n                num_days = \"\"\n                if book_info['start_date'] and book_info['finish_date']:\n                    delta = book_info['finish_date'] - book_info['start_date']\n                    num_days = f\" No. of Days: {delta.days + 1}\"\n                words = get_words(book_id)\n                estimated_time = estimate_reading_time(words)\n                finished_books_list_items.append(\n                    f\"- {get_title(book_id)} ({words} words, est. {estimated_time}) - {get_author(book_id)} \"\n                    f\"(Duration: {format_duration_short(book_info['total_duration'])} over {book_info['session_count']} Sessions) \"\n                    f\"Started: {book_info['start_date'].strftime('%Y-%m-%d')} Finished: {book_info['finish_date'].strftime('%Y-%m-%d')}{num_days}\"\n                )\n            yearly_monthly_report_pane.append(pn.pane.Markdown(\"\\n\".join(finished_books_list_items)))\n        \n        # Started Books (not finished this month)\n        started_not_finished = monthly_data[month]['started_books'] - monthly_data[month]['finished_books']\n        if started_not_finished:\n            yearly_monthly_report_pane.append(pn.pane.Markdown(\"**Started Books (not finished this month):**\"))\n            started_list_items = []\n            sorted_started_not_finished = sorted(list(started_not_finished), \n                                                 key=lambda b: processed_data['book_stats_raw'][b]['start_date'] or get_title(b))\n            for book_id in sorted_started_not_finished:\n                book_info = processed_data['book_stats_raw'][book_id]\n                duration_this_month = sum(s['duration'] for s in book_info['read_sessions'] if s['date'].month == month and s['date'].year == year_to_process)\n                sessions_this_month = sum(1 for s in book_info['read_sessions'] if s['date'].month == month and s['date'].year == year_to_process)\n                words = get_words(book_id)\n                estimated_time = estimate_reading_time(words)\n                started_list_items.append(\n                    f\"- {get_title(book_id)} ({words} words, est. {estimated_time}) - {get_author(book_id)} \"\n                    f\"(Duration: {format_duration_short(duration_this_month)} over {sessions_this_month} Sessions) \"\n                    f\"Started: {book_info['start_date'].strftime('%Y-%m-%d')}\"\n                )\n            yearly_monthly_report_pane.append(pn.pane.Markdown(\"\\n\".join(started_list_items)))\n                  \n        # Continued Books\n        if monthly_data[month]['continued_books']:\n            yearly_monthly_report_pane.append(pn.pane.Markdown(\"**Continued Books:**\"))\n            continued_list_items = []\n            sorted_continued_books = sorted(list(monthly_data[month]['continued_books']), \n                                            key=lambda b: processed_data['book_stats_raw'][b]['start_date'] or get_title(b))\n            for book_id in sorted_continued_books:\n                book_info = processed_data['book_stats_raw'][book_id]\n                duration_this_month = sum(s['duration'] for s in book_info['read_sessions'] if s['date'].month == month and s['date'].year == year_to_process)\n                sessions_this_month = sum(1 for s in book_info['read_sessions'] if s['date'].month == month and s['date'].year == year_to_process)\n                words = get_words(book_id)\n                estimated_time = estimate_reading_time(words)\n                continued_list_items.append(\n                    f\"- {get_title(book_id)} ({words} words, est. {estimated_time}) - {get_author(book_id)} \"\n                    f\"(Duration: {format_duration_short(duration_this_month)} over {sessions_this_month} Sessions) \"\n                    f\"Started: {book_info['start_date'].strftime('%Y-%m-%d')}\"\n                )\n            yearly_monthly_report_pane.append(pn.pane.Markdown(\"\\n\".join(continued_list_items)))\n    \n    yearly_monthly_report_pane.append(pn.pane.Markdown(\"---\")) # Final separator\n\n    # --- Streaks & Calendar Content ---\n    \n\n    streaks_calendar_pane.append(pn.pane.Markdown(\"### Reading Calendar Heatmap\"))\n    heatmap_img_base64 = \"\"\n    if processed_data['daily_read_time']:\n        all_days = pd.Series(processed_data['daily_read_time'])\n        all_days.index = pd.to_datetime(all_days.index)\n        \n        buffer = io.BytesIO()\n        fig, ax = plt.subplots(figsize=(16, 8))\n        calmap.yearplot(all_days, year=2025, cmap='Blues', ax=ax)\n        ax.set_title('Reading Heatmap for 2025 (Minutes Read)', fontsize=16)\n        fig.savefig(buffer, format='png', bbox_inches='tight')\n        plt.close(fig)\n        buffer.seek(0)\n        heatmap_img_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n        \n        # FIX: Use pn.pane.HTML to embed the Base64 image string\n        streaks_calendar_pane.append(pn.pane.HTML(f'&lt;img src=\"data:image/png;base64,{heatmap_img_base64}\" style=\"width:100%; height:auto;\"&gt;', sizing_mode='scale_width'))\n\n        if processed_data['min_daily_read_time_min'] &gt; 0 or processed_data['max_daily_read_time_min'] &gt; 0:\n            streaks_calendar_pane.append(pn.pane.Markdown(\n                f\"&lt;p style='text-align:center; font-style:italic; margin-top:10px;'&gt;Scale: Lighter Blue ({processed_data['min_daily_read_time_min']:.0f} min) to Darker Blue ({processed_data['max_daily_read_time_min']:.0f} min)&lt;/p&gt;\"\n            ))\n    else:\n        streaks_calendar_pane.append(pn.pane.Markdown(\"No data to generate heatmap.\"))\n\n    streaks_calendar_pane.append(pn.pane.Markdown(\"---\"))\n    \n    streaks_calendar_pane.append(pn.pane.Markdown(\"### Finished Books & Reading Speed (WPM)\"))\n    wpm_output = processed_data['wpm_results']\n    if wpm_output:\n        wpm_list_items = [f'- \"{title}\": {wpm}' for title, wpm in wpm_output.items()]\n        streaks_calendar_pane.append(pn.pane.Markdown(\"\\n\".join(wpm_list_items)))\n    else:\n        streaks_calendar_pane.append(pn.pane.Markdown(\"No finished books with sufficient data.\"))\n\n\n    # --- Time of Day Content ---\n    time_of_day_pane.append(pn.pane.Markdown(\"### Total Reading Time by Time of Day\"))\n    ordered_periods = ['Morning (5am-12pm)', 'Afternoon (12pm-5pm)', 'Evening (5pm-9pm)', 'Night (9pm-5am)']\n    sorted_total_durations = {period: processed_data['time_of_day_total_durations'][period] for period in ordered_periods}\n\n    time_of_day_plot_img_base64 = \"\"\n    if any(sorted_total_durations.values()):\n        buffer = io.BytesIO()\n        fig, ax = plt.subplots(figsize=(5, 3))\n        total_durations_hours = [d / 3600 for d in sorted_total_durations.values()]\n        ax.bar(sorted_total_durations.keys(), total_durations_hours, color=['skyblue', 'orange', 'salmon', 'darkblue'])\n        ax.set_title('Total Reading Time by Time of Day', fontsize=16)\n        ax.set_ylabel('Total Reading Time (Hours)')\n        plt.xticks(rotation=15)\n        fig.savefig(buffer, format='png', bbox_inches='tight')\n        plt.close(fig)\n        buffer.seek(0)\n        time_of_day_plot_img_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n        \n        # FIX: Use pn.pane.HTML to embed the Base64 image string\n        time_of_day_pane.append(pn.pane.HTML(f'&lt;img src=\"data:image/png;base64,{time_of_day_plot_img_base64}\" style=\"width=\"450\"; height=\"270\";\"&gt;', sizing_mode='scale_width'))\n    else:\n        time_of_day_pane.append(pn.pane.Markdown(\"No reading session data found for time of day analysis.\"))\n\n    time_of_day_list_items = [f\"- {period}: {duration_sec / 3600:.2f} hours\" for period, duration_sec in sorted_total_durations.items()]\n    time_of_day_pane.append(pn.pane.Markdown(\"\\n\".join(time_of_day_list_items)))\n\n\n    # --- Highlights & Vocabulary Content ---\n    highlights_vocab_pane.append(pn.pane.Markdown(\"### My Word List\"))\n    vocab_list = processed_data['word_list']\n    if vocab_list:\n        vocab_list_items = [f\"- {word}\" for word in vocab_list]\n        highlights_vocab_pane.append(pn.pane.Markdown(\"\\n\".join(vocab_list_items)))\n    else:\n        highlights_vocab_pane.append(pn.pane.Markdown(\"No words in your vocabulary builder yet.\"))\n    highlights_vocab_pane.append(pn.pane.Markdown(\"---\"))\n    \n    highlights_vocab_pane.append(pn.pane.Markdown(\"### All Highlights\"))\n    if processed_data['all_highlights_formatted']:\n        current_book_id = None\n        for highlight in processed_data['all_highlights_formatted']:\n            if highlight['book_id'] != current_book_id:\n                if current_book_id is not None:\n                    highlights_vocab_pane.append(pn.pane.Markdown(\"---\")) # Separator\n                highlights_vocab_pane.append(pn.pane.Markdown(f\"##### Book: \\\"{highlight['title']}\\\" by {highlight['author']}\"))\n                current_book_id = highlight['book_id']\n            \n            # IMPROVED HIGHLIGHT FORMATTING\n            highlight_text = highlight['text']\n            highlight_time = highlight['time'].strftime('%Y-%m-%d %H:%M')\n            \n            # Use a div with CSS for better spacing and readability\n            highlight_html = f\"\"\"\n            &lt;div style=\"\n                margin-left: 20px; \n                margin-bottom: 8px; \n                padding: 5px 10px; \n                border-left: 3px solid #007bff; /* A nice blue color */\n                background-color: #f8f9fa; /* Light background */\n                font-size: 0.95em;\n                line-height: 1.4;\n            \"&gt;\n                \"{highlight_text}\"&lt;br&gt;\n                &lt;em style=\"font-size: 0.9em; color: #6c757d;\"&gt;(Time: {highlight_time})&lt;/em&gt;\n            &lt;/div&gt;\n            \"\"\"\n            highlights_vocab_pane.append(pn.pane.HTML(highlight_html))\n\n    else:\n        highlights_vocab_pane.append(pn.pane.Markdown(\"No highlights found.\"))\n\n\n    # --- Books Currently Being Read Content ---\n    books_being_read_pane.append(pn.pane.Markdown(\"### Books Currently Being Read\"))\n    currently_reading_list_items = []\n    if processed_data['books_currently_being_read']:\n        books_being_read_pane.append(pn.pane.Markdown(\"These are the books you have started but not yet finished:\"))\n        # Build list items as Markdown strings\n        for book_id in processed_data['books_currently_being_read']:\n            title = get_title(book_id)\n            author = get_author(book_id)\n            words = get_words(book_id)\n            estimated_time = estimate_reading_time(words)\n            \n            currently_reading_list_items.append(f\"- {title} ({words} words, est. {estimated_time}) by {author}\")\n        books_being_read_pane.append(pn.pane.Markdown(\"\\n\".join(currently_reading_list_items))) # Join and add as a single Markdown pane\n    else:\n        books_being_read_pane.append(pn.pane.Markdown(\"No books currently being read. You finish everything or haven't started anything yet!\"))\n\n\nelse: # If no data loaded at all, display an error message in each pane\n    yearly_monthly_report_pane.append(pn.pane.Markdown(\"### Error: Data not loaded. Please check the JSON file path and content.\"))\n    streaks_calendar_pane.append(pn.pane.Markdown(\"### Error: Data not loaded. Please check the JSON file path and content.\"))\n    time_of_day_pane.append(pn.pane.Markdown(\"### Error: Data not loaded. Please check the JSON file path and content.\"))\n    highlights_vocab_pane.append(pn.pane.Markdown(\"### Error: Data not loaded. Please check the JSON file path and content.\"))\n    books_being_read_pane.append(pn.pane.Markdown(\"### Error: Data not loaded. Please check the JSON file path and content.\"))\n\n\n# ==============================================================================\n# Cell 4: Create Panel Tabs and Save to HTML\n# ==============================================================================\n# Create the main dashboard using pn.Tabs\ndashboard = pn.Tabs(\n    ('Yearly/Monthly Report', yearly_monthly_report_pane),\n    ('Streaks & Calendar', streaks_calendar_pane),\n    ('Time of Day', time_of_day_pane),\n    ('Highlights & Vocabulary', highlights_vocab_pane),\n    ('Books Currently Being Read', books_being_read_pane)\n)\n\n# Display the dashboard in the notebook (optional, for direct Jupyter viewing)\ndashboard # Uncomment this line if you want to see it rendered directly in Jupyter\n\n# Save the dashboard to a standalone HTML file\noutput_html_file = \"/Users/jordanchong/my-website/reading stats/reading_stats_dashboard.html\" # New output file name\nprint(f\"Saving interactive dashboard to: {output_html_file}\")\ndashboard.save(output_html_file, embed=True) # embed=True is crucial for a self-contained file\nprint(\"‚úÖ Dashboard saved successfully.\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n‚úÖ Data loaded successfully from /Users/jordanchong/Documents/Books/tc_readstat.json.\nSaving interactive dashboard to: /Users/jordanchong/Documents/Books/reading_stats_dashboard.html\n‚úÖ Dashboard saved successfully.\n\n\n\ndashboard"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "My Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAn Analysis of Market Trends\n\n\n\nQuant\n\nFinance\n\nPython\n\n\n\nA deep dive into recent market trends using Python‚Äôs data analysis libraries to uncover hidden patterns.\n\n\n\n\n\nSep 26, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jordan Chong",
    "section": "",
    "text": "I am a [Your Role] passionate about [Your Interests]. This website contains my projects, research, and writings. I am currently a student at Singapore Management University.\n##READ MORE ‚Üí"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "My Projects",
    "section": "",
    "text": "Fama-French LASSO\n\n\n\nPython\n\nRegularisation\n\n\n\nEmpirically Validating Fama-French with Machine Learning\n\n\n\n\n\n\n\n\n\n\n\n\nGap Challenge Solver\n\n\n\nPython\n\nComputer Vision\n\nWeb App\n\n\n\nAn interactive web tool to solve the Gap Challenge puzzle using Python and OpenCV.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/gap-challenge-solver/AON Gap Challenge.html",
    "href": "projects/gap-challenge-solver/AON Gap Challenge.html",
    "title": "JavaScript",
    "section": "",
    "text": "import streamlit as st\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nimport base64\nfrom io import BytesIO\n\n# --- MODIFIED: Import using the new alias as requested ---\nfrom streamlit_paste_button import paste_image_button as pbutton\n\n2025-09-26 17:10:59.671 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n# --- App Configuration ---\nst.set_page_config(page_title=\"Gap Challenge Solver\", layout=\"wide\")\n\n# ==============================================================================\n# --- CONFIGURATION ---\n# ==============================================================================\nCONFIDENCE_THRESHOLD_SHAPE = 0.60\nBLANK_STD_DEV_THRESHOLD = 15.0   \nTEMPLATE_DIR = \"templates\"\nWHITE_THRESHOLD = 240\n\n2025-09-26 17:10:59.679 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode."
  },
  {
    "objectID": "projects/gap-challenge-solver/AON Gap Challenge.html#image-recognition-and-solver",
    "href": "projects/gap-challenge-solver/AON Gap Challenge.html#image-recognition-and-solver",
    "title": "JavaScript",
    "section": "Image Recognition and Solver",
    "text": "Image Recognition and Solver\n\ndef find_empty(board):\n    for r in range(len(board)):\n        for c in range(len(board[0])):\n            if board[r][c] == 'blank':\n                return (r, c)\n    return None\n\ndef solve_with_backtracking(board, all_shapes):\n    find = find_empty(board)\n    if not find: return True\n    row, col = find\n    for shape in all_shapes:\n        if shape in board[row] or shape in [board[i][col] for i in range(len(board))]: continue\n        board[row][col] = shape\n        if solve_with_backtracking(board, all_shapes): return True\n        board[row][col] = 'blank'\n    return False\n\ndef find_question_mark_solution(board, universe_of_shapes):\n    question_pos = None\n    for r in range(len(board)):\n        for c in range(len(board[0])):\n            if board[r][c] == '6question':\n                question_pos = (r, c)\n                break\n        if question_pos: break\n    if not question_pos:\n        st.error(\"No '6question' mark found on the board.\")\n        return None, None\n    board_copy = [row[:] for row in board]\n    qr, qc = question_pos\n    board_copy[qr][qc] = 'blank'\n    if solve_with_backtracking(board_copy, universe_of_shapes):\n        solution_shape = board_copy[qr][qc]\n        return solution_shape, board_copy\n    else:\n        return None, None\n\ndef crop_to_grid(source_image: np.ndarray):\n    gray = cv2.cvtColor(source_image, cv2.COLOR_BGR2GRAY)\n    _, thresh = cv2.threshold(gray, WHITE_THRESHOLD, 255, cv2.THRESH_BINARY_INV)\n    non_zero_pixels = cv2.findNonZero(thresh)\n    if non_zero_pixels is None: return source_image\n    x, y, w, h = cv2.boundingRect(non_zero_pixels)\n    return source_image[y:y+h, x:x+w]\n\ndef recognize_shape_in_cell(cell_roi_color, templates):\n    cell_roi_gray = cv2.cvtColor(cell_roi_color, cv2.COLOR_BGR2GRAY)\n    std_dev = np.std(cell_roi_gray)\n    if std_dev &lt; BLANK_STD_DEV_THRESHOLD: return \"blank\"\n    best_match = {'label': 'blank', 'score': -1.0}\n    for label, template_with_alpha in templates.items():\n        if template_with_alpha.shape[2] == 4:\n            mask = template_with_alpha[:,:,3]\n            template_color = cv2.cvtColor(template_with_alpha, cv2.COLOR_BGRA2BGR)\n        else:\n            template_color = template_with_alpha\n            template_gray_for_mask = cv2.cvtColor(template_color, cv2.COLOR_BGR2GRAY)\n            _, mask = cv2.threshold(template_gray_for_mask, 10, 255, cv2.THRESH_BINARY)\n        h, w, _ = template_color.shape\n        for scale in np.linspace(0.5, 1.0, 10):\n            scaled_h, scaled_w = int(h * scale), int(w * scale)\n            if not (scaled_h &gt; 0 and scaled_w &gt; 0 and scaled_h &lt;= cell_roi_color.shape[0] and scaled_w &lt;= cell_roi_color.shape[1]): continue\n            scaled_template = cv2.resize(template_color, (scaled_w, scaled_h))\n            scaled_mask = cv2.resize(mask, (scaled_w, scaled_h))\n            result = cv2.matchTemplate(cell_roi_color, scaled_template, cv2.TM_CCOEFF_NORMED, mask=scaled_mask)\n            _, max_val, _, _ = cv2.minMaxLoc(result)\n            if not np.isfinite(max_val): max_val = -1.0\n            if max_val &gt; best_match['score']:\n                best_match.update({'score': max_val, 'label': label})\n    final_label = 'blank'\n    if best_match['score'] &gt; CONFIDENCE_THRESHOLD_SHAPE:\n        final_label = best_match['label']\n    return final_label\n\ndef recognize_grid_and_options(image: Image.Image, grid_size: int, templates):\n    original_image = np.array(image.convert('RGB'))\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR)\n    border_size = 20\n    bordered_image = cv2.copyMakeBorder(original_image, top=border_size, bottom=border_size, left=border_size, right=border_size, borderType=cv2.BORDER_CONSTANT, value=[255, 255, 255])\n    cropped_color = crop_to_grid(bordered_image)\n    img_h, img_w, _ = cropped_color.shape\n    estimated_total_rows = grid_size + 1.1 \n    estimated_cell_h = img_h / estimated_total_rows\n    estimated_grid_h = int(estimated_cell_h * grid_size)\n    puzzle_area = cropped_color[:estimated_grid_h, :]\n    options_area = cropped_color[estimated_grid_h:, :]\n    puzzle_area = crop_to_grid(puzzle_area)\n    standard_size = 600\n    aligned_color_image = cv2.resize(puzzle_area, (standard_size, standard_size))\n    cell_height, cell_width = standard_size // grid_size, standard_size // grid_size\n    output_grid = [[\"\" for _ in range(grid_size)] for _ in range(grid_size)]\n    for r in range(grid_size):\n        for c in range(grid_size):\n            cell_roi = aligned_color_image[r*cell_height:(r+1)*cell_height, c*cell_width:(c+1)*cell_width]\n            output_grid[r][c] = recognize_shape_in_cell(cell_roi, templates)\n    universe_of_shapes = []\n    if options_area.shape[0] &gt; 10:\n        option_cell_h = options_area.shape[0]\n        option_cell_w_est = option_cell_h \n        num_options = round(options_area.shape[1] / option_cell_w_est)\n        cell_w_options = options_area.shape[1] // num_options\n        for i in range(num_options):\n            option_roi = options_area[:, i*cell_w_options:(i+1)*cell_w_options]\n            option_roi = cv2.resize(option_roi, (cell_width, cell_height))\n            shape_in_option = recognize_shape_in_cell(option_roi, templates)\n            if shape_in_option not in ['blank', '6question']:\n                universe_of_shapes.append(shape_in_option)\n    for r in range(grid_size):\n        for c in range(grid_size):\n            shape_in_grid = output_grid[r][c]\n            if shape_in_grid not in ['blank', '6question']:\n                universe_of_shapes.append(shape_in_grid)\n    if not universe_of_shapes:\n        universe_of_shapes = ['1circle','2triangle','3square','4cross','5star']\n    return output_grid, sorted(list(set(universe_of_shapes)))\n\ndef decode_base64_image(base64_string):\n    \"\"\"Decodes a base64 string into a PIL Image.\"\"\"\n    # Remove the \"data:image/png;base64,\" prefix\n    if \",\" in base64_string:\n        base64_string = base64_string.split(',')[1]\n    image_data = base64.b64decode(base64_string)\n    return Image.open(BytesIO(image_data))"
  },
  {
    "objectID": "projects/gap-challenge-solver/AON Gap Challenge.html#gap-solver",
    "href": "projects/gap-challenge-solver/AON Gap Challenge.html#gap-solver",
    "title": "JavaScript",
    "section": "Gap Solver",
    "text": "Gap Solver\n\n# def find_empty(board):\n#     \"\"\"Finds the first empty cell ('blank') in the board.\"\"\"\n#     for r in range(len(board)):\n#         for c in range(len(board[0])):\n#             if board[r][c] == 'blank':\n#                 return (r, c)\n#     return None\n\n# def solve_with_backtracking(board, all_shapes):\n#     \"\"\"Solves the puzzle using a recursive backtracking algorithm.\"\"\"\n#     find = find_empty(board)\n#     if not find:\n#         return True\n#     else:\n#         row, col = find\n\n#     for shape in all_shapes:\n#         if shape in board[row] or shape in [board[i][col] for i in range(len(board))]:\n#             continue\n#         board[row][col] = shape\n#         if solve_with_backtracking(board, all_shapes):\n#             return True\n#         board[row][col] = 'blank'\n#     return False\n\n# def find_question_mark_solution(board, universe_of_shapes):\n#     \"\"\"Finds the single shape that should replace the '6question' mark.\"\"\"\n#     print(\"\\n--- Starting Question Mark Solver ---\")\n#     question_pos = None\n#     for r in range(len(board)):\n#         for c in range(len(board[0])):\n#             if board[r][c] == '6question':\n#                 question_pos = (r, c)\n#                 break\n#         if question_pos: break\n            \n#     if not question_pos:\n#         print(\"No '6question' mark found on the board.\"); return None\n\n#     print(f\"Question mark found at position: {question_pos}\")\n#     board_copy = [row[:] for row in board]\n#     qr, qc = question_pos\n#     board_copy[qr][qc] = 'blank'\n\n#     print(f\"Dynamically detected shapes for this puzzle: {universe_of_shapes}\")\n\n#     if solve_with_backtracking(board_copy, universe_of_shapes):\n#         print(\"Solver found a complete solution for the grid.\")\n#         solution_shape = board_copy[qr][qc]\n#         return solution_shape\n#     else:\n#         print(\"Solver could not find a valid solution for the entire grid.\"); return None\n\n\n# ==============================================================================\n# --- STREAMLIT UI (MODIFIED TO USE PASTE BUTTON) ---\n# ==============================================================================\n\nst.title(\"üß© Gap Challenge Solver\")\nst.info(\"To solve, take a screenshot of the puzzle, then click the button below and press Ctrl+V (or Cmd+V).\")\n\n# --- Sidebar for Controls ---\nwith st.sidebar:\n    st.header(\"Controls\")\n    grid_size = st.radio(\"Grid Size\", (4, 5), index=0)\n    is_aon = st.toggle(\"AON Puzzle\", value=True, help=\"This toggle is for future use and does not change the current shape templates.\")\n    \n# --- Main App Body ---\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.subheader(\"Your Puzzle\")\n    \n    # --- MODIFIED: Use the new pbutton component syntax ---\n    paste_result = pbutton(\"üìã Paste an image\")\n\n    # Use session state to keep track of the image\n    if \"pasted_image\" not in st.session_state:\n        st.session_state.pasted_image = None\n    \n    if paste_result.image_data is not None:\n        st.session_state.pasted_image = paste_result.image_data\n\n    if st.session_state.pasted_image:\n        st.image(st.session_state.pasted_image, caption=\"Pasted from Clipboard\", use_column_width=True)\n    else:\n        st.write(\"Awaiting a pasted image...\")\n\nwith col2:\n    st.subheader(\"Solution\")\n    # Run the analysis only if there is an image\n    if st.session_state.pasted_image:\n        with st.spinner(\"Analyzing puzzle...\"):\n            image = st.session_state.pasted_image\n            \n            shape_labels = ['1circle','2triangle','3square','4cross','5star','6question']\n            try:\n                templates = {\n                    label: cv2.imread(os.path.join(TEMPLATE_DIR, f\"{label}.png\"), cv2.IMREAD_UNCHANGED)\n                    for label in shape_labels\n                }\n                if any(t is None for t in templates.values()):\n                    st.error(f\"One or more template files are missing from the '{TEMPLATE_DIR}' directory.\")\n                    st.stop()\n            except Exception as e:\n                st.error(f\"Error loading template files: {e}\"); st.stop()\n            \n            initial_grid, detected_shapes = recognize_grid_and_options(image, grid_size, templates)\n            \n            if initial_grid and detected_shapes:\n                st.write(\"Detected Initial Grid:\")\n                st.table(initial_grid)\n                solution_shape, solved_grid = find_question_mark_solution(initial_grid, detected_shapes)\n                if solution_shape and solved_grid:\n                    st.success(\"Solution Found!\")\n                    st.metric(label=\"The shape for the '?' is\", value=solution_shape)\n                    st.write(\"Completed Grid:\")\n                    st.table(solved_grid)\n                else:\n                    st.error(\"Could not find a valid solution for this puzzle.\")\n    else:\n        st.write(\"Click the paste button and press Ctrl+V to see the solution.\")\n\n\n2025-09-26 17:10:59.778 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.801 \n\n  Warning: to view this Streamlit app on a browser, run it with the following\n\n  command:\n\n\n\n    streamlit run /Users/jordanchong/opt/anaconda3/lib/python3.13/site-packages/ipykernel_launcher.py [ARGUMENTS]\n\n2025-09-26 17:10:59.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.803 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.803 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.803 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.803 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.803 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.804 Session state does not function when running a script without `streamlit run`\n\n2025-09-26 17:10:59.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.940 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.940 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.940 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.940 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n2025-09-26 17:10:59.940 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n\n\n\n\n\nstreamlit run /Users/jordanchong/opt/anaconda3/lib/python3.13/site-packages/ipykernel_launcher.py\n\n\n  Cell In[6], line 1\n    streamlit run /Users/jordanchong/opt/anaconda3/lib/python3.13/site-packages/ipykernel_launcher.py\n              ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Contact \n\n\n\n&lt;label for=\"full-name\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Full Name&lt;/label&gt;\n&lt;input type=\"text\" id=\"full-name\" name=\"name\" required style=\"width: 100%; padding: 0.5em;\"&gt;\n\n\n&lt;label for=\"email-address\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Email Address&lt;/label&gt;\n&lt;input type=\"email\" id=\"email-address\" name=\"_replyto\" required style=\"width: 100%; padding: 0.5em;\"&gt;\n\n\n&lt;label for=\"message\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Message&lt;/label&gt;\n&lt;textarea id=\"message\" name=\"message\" rows=\"6\" required style=\"width: 100%; padding: 0.5em;\"&gt;&lt;/textarea&gt;\n\n\nSend Message"
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html",
    "href": "projects/gap-challenge-solver/index.html",
    "title": "Gap Challenge Solver",
    "section": "",
    "text": "This is where you write the main content describing your project‚Ä¶"
  },
  {
    "objectID": "projects/Fama-French LASSO/index.html",
    "href": "projects/Fama-French LASSO/index.html",
    "title": "Fama-French LASSO",
    "section": "",
    "text": "The Fama-French 3-Factor Model is a pillar of modern finance, but does it hold up when pitted against a wide universe of other signals in a data-driven test? In this project, I put it to the test using a modern machine learning approach to see if we can empirically rediscover the key drivers of stock returns."
  },
  {
    "objectID": "projects/Fama-French LASSO/index.html#the-goal-a-factor-horse-race",
    "href": "projects/Fama-French LASSO/index.html#the-goal-a-factor-horse-race",
    "title": "Fama-French LASSO",
    "section": "The Goal: A Factor ‚ÄúHorse Race‚Äù",
    "text": "The Goal: A Factor ‚ÄúHorse Race‚Äù\nThe goal was to move beyond simply confirming the Fama-French factors and instead conduct an unbiased ‚Äúhorse race.‚Äù I built a universe of over 20 quantitative factors‚Äîincluding classic signals like Momentum and modern ones like Net Stock Issuance‚Äîand used a LASSO (L1) regularized regression to let the data itself decide which factors were the most important and consistent explainers of stock returns."
  },
  {
    "objectID": "projects/Fama-French LASSO/index.html#the-intuition-behind-the-method",
    "href": "projects/Fama-French LASSO/index.html#the-intuition-behind-the-method",
    "title": "Fama-French LASSO",
    "section": "The Intuition Behind the Method",
    "text": "The Intuition Behind the Method\nInstead of manually selecting factors, LASSO regression automatically shrinks the coefficients of less important factors to zero. By running this analysis across 25 different portfolios (representing the entire spectrum from small-growth to big-value stocks), we can count which factors ‚Äúsurvive‚Äù the penalty most often. The survivors are the ones that are most pervasively important across the entire market."
  },
  {
    "objectID": "projects/Fama-French LASSO/index.html#data-and-cleaning",
    "href": "projects/Fama-French LASSO/index.html#data-and-cleaning",
    "title": "Fama-French LASSO",
    "section": "Data and Cleaning",
    "text": "Data and Cleaning\nThe analysis used institutional-grade data from Wharton Research Data Services (WRDS), primarily the CRSP and Compustat databases. The factor universe was constructed by creating long-short portfolios from the backtest results of various signals, while the testbed consisted of the 25 Fama-French portfolios sorted on size and book-to-market. All data was carefully aligned by date to ensure a robust time-series analysis."
  },
  {
    "objectID": "projects/Fama-French LASSO/index.html#the-results-the-data-speaks",
    "href": "projects/Fama-French LASSO/index.html#the-results-the-data-speaks",
    "title": "Fama-French LASSO",
    "section": "The Results: The Data Speaks",
    "text": "The Results: The Data Speaks\nAfter running the 25 LASSO regressions, the results were clear. The factors that were most consistently selected as significant were overwhelmingly the Fama-French 5 Factors (Mkt-RF, SMB, HML, RMW, CMA).\nWhen we group these surviving factors into broad economic categories, the story becomes even clearer. The dimensions of risk identified by Fama and French‚ÄîMarket, Size, Value, Profitability, and Investment‚Äîare the dominant themes that emerge from the data."
  },
  {
    "objectID": "projects/Fama-French LASSO/index.html#conclusion-and-shortcomings",
    "href": "projects/Fama-French LASSO/index.html#conclusion-and-shortcomings",
    "title": "Fama-French LASSO",
    "section": "Conclusion and Shortcomings",
    "text": "Conclusion and Shortcomings\nThis experiment successfully demonstrates the robustness of the Fama-French framework. Using an impartial, data-driven machine learning approach, we can empirically rediscover that the key dimensions of risk they identified are, in fact, the most consistent and powerful drivers of cross-sectional stock returns in our test universe.\nOf course, no model is perfect. The main limitations of this study are:\nOmitted Variables: The universe of factors is vast, and a yet-undiscovered factor could exist with even more explanatory power.\nSample Specificity: The results are specific to the U.S. market and the time period analyzed. The ‚Äúbest‚Äù factors could change in different market regimes or geographies.\nThis project was a fantastic exercise in applying modern data science techniques to a classic problem in quantitative finance.\nTo see the full academic-style write-up, you can download the research paper here.\nTo see the complete source code for this analysis, visit the project repository on GitHub."
  },
  {
    "objectID": "licence.html",
    "href": "licence.html",
    "title": "Licence",
    "section": "",
    "text": "The content on this website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n\n\nCreative Commons License Logo"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I‚Äôm Jordan Chong. This is where you can write a more detailed biography‚Ä¶"
  },
  {
    "objectID": "reading stats/index.html",
    "href": "reading stats/index.html",
    "title": "Books Read",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAn Analysis of Market Trends\n\n\n\nQuant\n\nFinance\n\nPython\n\n\n\nA deep dive into recent market trends using Python‚Äôs data analysis libraries to uncover hidden patterns.\n\n\n\n\n\nSep 26, 2025\n\n\n\n\n\nNo matching items"
  }
]