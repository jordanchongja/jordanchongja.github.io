[
  {
    "objectID": "readings/index.html",
    "href": "readings/index.html",
    "title": "Readings & Reflections",
    "section": "",
    "text": "Order By\n      Default\n      \n        Subtitle\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEast of Eden\n\n\n‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\n\n\n\n\n\n\nJohn Steinbeck\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/sg-insurance-model/index.html#act-1-the-deterministic-baseline",
    "href": "projects/sg-insurance-model/index.html#act-1-the-deterministic-baseline",
    "title": "Singapore Insurance Strategy Dashboard",
    "section": "Act 1: The Deterministic Baseline",
    "text": "Act 1: The Deterministic Baseline\nThe first layer builds the standard financial projection.\n- Liquidity Analysis: Comparing the Surrender Value of WL against the liquid portfolio of BTID.\n- Legacy Visualization: Tracking the Death Benefit sum assured over time.\n- The ‚ÄòStrategy Frontier‚Äô: A dynamic calculation that pinpoints the exact age where BTID overtakes WL based on user-defined investment returns (e.g., 4% vs 7%)."
  },
  {
    "objectID": "projects/sg-insurance-model/index.html#act-2-behavioral-stress-testing",
    "href": "projects/sg-insurance-model/index.html#act-2-behavioral-stress-testing",
    "title": "Singapore Insurance Strategy Dashboard",
    "section": "Act 2: Behavioral Stress Testing",
    "text": "Act 2: Behavioral Stress Testing\nData shows that most retail investors do not reinvest their savings perfectly. I introduced a ‚ÄúDiscipline Coefficient‚Äù, a slider allowing users to see what happens if they only invest 50%, 70%, or 90% of their premium savings.\n\nKey Insight: Through this feature, I discovered that if an investor‚Äôs discipline drops below ~60%, Whole Life plans often outperform BTID due to their ‚Äúforced savings‚Äù nature, despite lower internal rates of return."
  },
  {
    "objectID": "projects/sg-insurance-model/index.html#act-3-stochastic-reality-monte-carlo",
    "href": "projects/sg-insurance-model/index.html#act-3-stochastic-reality-monte-carlo",
    "title": "Singapore Insurance Strategy Dashboard",
    "section": "Act 3: Stochastic Reality (Monte Carlo)",
    "text": "Act 3: Stochastic Reality (Monte Carlo)\nLife isn‚Äôt linear. Using Monte Carlo simulations, I modeled:\n- Actuarial Risk: Probabilities of Death and Total Permanent Disability (TPD) using Singapore‚Äôs S0408 tables.\n- Sequence of Returns Risk: Instead of a flat 7% return, the model simulates market volatility to show the range of possible outcomes for the BTID portfolio."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSingapore Insurance Strategy Dashboard\n\n\n\nPython\n\nStreamlit\n\nActuarial Science\n\nData Visualization\n\nMonte Carlo\n\nfeatured\n\n\n\nA Streamlit-powered actuarial dashboard that pits ‚ÄòBuy Term, Invest the Difference‚Äô against Whole Life plans, accounting for human behavior and stochastic risk.\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nGap Challenge Solver\n\n\n\nComputer Vision\n\nPython\n\nStreamlit\n\nAlgorithms\n\nfeatured\n\n\n\nEvolution of a Computer Vision pipeline: From brittle Template Matching to robust SVMs, integrated with a Backtracking solver.\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production\n\n\n\nQuantitative Finance\n\nVolatility Modeling\n\nPython\n\nOOP\n\nStreamlit\n\nOptimization\n\nfeatured\n\n\n\nA comprehensive case study bridging Quantitative Research and Software Engineering. From calibrating Heston and Bates models using noisy S&P 500 tick data to deploying an‚Ä¶\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/posts/Black-Sholes-Derivation/index.html#the-full-mathematical-derivation-of-the-call-formula",
    "href": "notes/posts/Black-Sholes-Derivation/index.html#the-full-mathematical-derivation-of-the-call-formula",
    "title": "Concept: The Black-Scholes Model",
    "section": "The Full Mathematical Derivation of the Call Formula",
    "text": "The Full Mathematical Derivation of the Call Formula\nTo truly understand Black-Scholes, we must walk step-by-step from the assumption of the asset‚Äôs random walk to the final pricing formula. We do this by deriving It√¥‚Äôs Lemma, establishing the Partial Differential Equation (PDE), transforming it into an expected value, and integrating.\n\n\n1. The Stochastic Framework\nSuppose we have a stochastic process \\(X_t\\) that models our asset price, following a generalized It√¥ drift-diffusion process:\n\\[\ndX_t = \\mu(X_t, t)dt + \\sigma(X_t, t)dW_t\n\\]\nWhere: * \\(\\mu(X_t, t)\\) is the predictable, deterministic drift. * \\(\\sigma(X_t, t)\\) is the volatility (diffusion) scaling factor. * \\(dW_t\\) is the increment of a standard Wiener process (Brownian motion), representing random market shocks.\nWe want to find the differential \\(df\\) of a scalar function \\(f(X_t, t)\\)‚Äîsuch as an option pricing function‚Äîthat is at least twice-differentiable in \\(x\\) and once-differentiable in \\(t\\).\n\n\n\n2. The Taylor Series Expansion (The Divergence from Ordinary Calculus)\nWe begin by examining the change in our function \\(f\\) over an infinitesimally small time increment \\(\\Delta t\\). We use a two-variable Taylor expansion:\n\\[\n\\Delta f \\approx \\frac{\\partial f}{\\partial t}\\Delta t + \\frac{\\partial f}{\\partial x}\\Delta X + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(\\Delta X)^2 + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial t^2}(\\Delta t)^2 + \\frac{\\partial^2 f}{\\partial x \\partial t}(\\Delta X \\Delta t) + \\dots\n\\]\nThe Core Difference: In standard, deterministic calculus, as \\(\\Delta t \\to 0\\), any terms of order higher than \\(\\Delta t\\) (like \\(\\Delta t^2\\) or \\(\\Delta x^2\\)) shrink to zero so rapidly that they become negligible. We simply drop them.\nHowever, in the stochastic world, the path of \\(X_t\\) is driven by the random walk of \\(W_t\\). Brownian motion is continuous but nowhere differentiable‚Äîit is infinitely jagged.\nBecause of this extreme jaggedness, the squared variation \\((\\Delta X)^2\\) does not shrink to zero faster than \\(\\Delta t\\). It is large enough that it must be kept in our equation.\n\n\n\n3. Analyzing \\((\\Delta X)^2\\) and the Rules of Stochastic Calculus\nTo see why the second-order term survives, we substitute our definition of \\(\\Delta X\\) into the squared term:\n\\[\n(\\Delta X)^2 = (\\mu \\Delta t + \\sigma \\Delta W)^2\n\\] \\[\n(\\Delta X)^2 = \\mu^2 (\\Delta t)^2 + 2\\mu\\sigma (\\Delta t \\Delta W) + \\sigma^2 (\\Delta W)^2\n\\]\nNow, we take the limit as \\(\\Delta t \\to 0\\) and apply the formal multiplication rules of stochastic calculus (It√¥ multiplication table): * \\(dt \\cdot dt = 0\\) (Shrinks too fast) * \\(dt \\cdot dW_t = 0\\) (Shrinks too fast, scales as \\(dt^{3/2}\\)) * \\(dW_t \\cdot dW_t = dt\\) (The critical survival)\n\nDeep Dive: Why does \\((dW_t)^2 = dt\\)?\nThe jump to \\((dW_t)^2 = dt\\) is the most unintuitive part of stochastic calculus. How can a random variable squared equal a deterministic, predictable passage of time? We can break this down statistically, behaviorally, and heuristically.\n1. The Statistical Properties\nHere is the exact step-by-step logic chain to prove this equivalence: * Definition: By definition, an increment of a Wiener process follows a normal distribution centered at zero: \\(\\Delta W \\sim \\mathcal{N}(0, \\Delta t)\\). * Variance Formula: From fundamental statistics, the variance of any random variable is \\(Var[\\Delta W] = E[(\\Delta W)^2] - (E[\\Delta W])^2\\). * Plug in the numbers: Since the mean expected value \\(E[\\Delta W]\\) is exactly \\(0\\), squaring it \\((E[\\Delta W])^2\\) is also \\(0\\). * The Reduction: This leaves us with a perfectly simplified equation: \\(Var[\\Delta W] = E[(\\Delta W)^2]\\). * The Result: Because the variance of a Wiener process over time \\(\\Delta t\\) is defined as \\(\\Delta t\\), it must be mathematically true that the expected value of the squared increment is the time step itself: \\(E[(\\Delta W)^2] = \\Delta t\\).\n2. The Law of Large Numbers (Quadratic Variation)\nOne might naturally ask: ‚ÄúIf \\(E[(\\Delta W)^2] = \\Delta t\\), isn‚Äôt \\((\\Delta W)^2\\) still a random variable around that expected value?‚Äù\nYes, for a single, isolated step, it is. However, It√¥‚Äôs Lemma deals with continuous time, where the limit as \\(\\Delta t \\to 0\\). If we divide a time interval \\([0, T]\\) into \\(n\\) microscopic pieces, the total sum of these squared increments is called the Quadratic Variation: \\[[W, W]_T = \\lim_{n \\to \\infty} \\sum_{i=1}^{n} (W_{t_i} - W_{t_{i-1}})^2\\]\nAs \\(n\\) becomes infinitely large, we must look at the variance of this sum. For a normal distribution, the fourth moment allows us to calculate the variance of the square: \\[Var[(\\Delta W)^2] = E[(\\Delta W)^4] - (E[(\\Delta W)^2])^2 = 3(\\Delta t)^2 - (\\Delta t)^2 = 2(\\Delta t)^2\\]\nBecause the variance of \\((\\Delta W)^2\\) is proportional to \\((\\Delta t)^2\\), it shrinks to zero vastly faster than the mean (\\(\\Delta t\\)). In the continuous limit, the randomness completely ‚Äúwashes out,‚Äù leaving only the deterministic expected value. The sum of squares behaves perfectly like a ticking clock.\n3. The Heuristic Comparison\nTo visualize why this is unique to random walks, compare it to a standard smooth function, \\(g(t) = t\\): * Standard Calculus: \\(\\Delta g = \\Delta t\\). Therefore, \\((\\Delta g)^2 = (\\Delta t)^2\\). As \\(\\Delta t \\to 0\\), this squared term essentially vanishes to zero. * Stochastic Calculus: \\(\\Delta W \\approx \\sqrt{\\Delta t}\\). Therefore, \\((\\Delta W)^2 \\approx \\Delta t\\).\nBecause Brownian motion is inherently ‚Äúsquare-root-of-time‚Äù risky, its square is of the exact same ‚Äúsize‚Äù as time itself.\n\n\n\n\n4. Final Substitution\nNow we return to our original Taylor expansion. Knowing which terms survive the limit as \\(\\Delta t \\to 0\\), we truncate the series, keeping only the terms of order \\(dt\\) or \\(dW_t\\):\n\\[\ndf = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial x}dX_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(dX_t)^2\n\\]\nNext, we substitute our initial definitions: * \\(dX_t = \\mu dt + \\sigma dW_t\\) * \\((dX_t)^2 = \\sigma^2 dt\\)\n\\[\ndf = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial x}(\\mu dt + \\sigma dW_t) + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(\\sigma^2 dt)\n\\]\n\n\n\n5. The Result: It√¥‚Äôs Lemma\nFinally, we group the deterministic \\(dt\\) terms together and separate the stochastic \\(dW_t\\) term. This yields the foundational equation of stochastic calculus‚ÄîIt√¥‚Äôs Lemma:\n\\[\ndf = \\left( \\frac{\\partial f}{\\partial t} + \\mu \\frac{\\partial f}{\\partial x} + \\frac{1}{2} \\sigma^2 \\frac{\\partial^2 f}{\\partial x^2} \\right) dt + \\sigma \\frac{\\partial f}{\\partial x} dW_t\n\\]\n(Note for Black-Scholes: In our options pricing model, \\(f\\) is the option price \\(V(S, t)\\), the asset \\(X_t\\) is the stock price \\(S_t\\), the drift \\(\\mu\\) becomes \\(\\mu S_t\\), and the diffusion \\(\\sigma\\) becomes \\(\\sigma S_t\\).)\n\n\n\nStep 2: Constructing the Riskless Portfolio (Delta Hedging)\nNow that we have It√¥‚Äôs Lemma to describe how the option price \\(V(S, t)\\) changes over time, we need a way to find its fair value. Fischer Black and Myron Scholes realized that if you hold the right combination of the option and the underlying stock, you can completely cancel out the unpredictable market shocks (\\(dW_t\\)).\nWe construct a hypothetical portfolio, denoted as \\(\\Pi\\). In this portfolio, we hold exactly one long position in our option, and we short sell a specific number of shares of the underlying stock. We will call this number of shares \\(\\Delta\\) (not to be confused with a time step \\(\\Delta t\\)).\nThe value of our portfolio at any given time is: \\[\n\\Pi = V - \\Delta S\n\\]\nWe want to observe how the value of this portfolio changes over a microscopic time step \\(dt\\). By applying basic differentiation: \\[\nd\\Pi = dV - \\Delta dS\n\\]\nNow, we substitute the two massive equations we have built so far: 1. \\(dV\\): The change in the option price (from It√¥‚Äôs Lemma). 2. \\(dS\\): The change in the stock price (from our Geometric Brownian Motion assumption: \\(dS = \\mu S dt + \\sigma S dW_t\\)).\nSubstituting these into our portfolio equation yields: \\[\nd\\Pi = \\left[ \\left( \\frac{\\partial V}{\\partial t} + \\mu S \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt + \\sigma S \\frac{\\partial V}{\\partial S} dW_t \\right] - \\Delta \\left[ \\mu S dt + \\sigma S dW_t \\right]\n\\]\nThis looks messy, but we can organize it by grouping all the deterministic parts (attached to \\(dt\\)) and all the random, volatile parts (attached to \\(dW_t\\)):\n\\[\nd\\Pi = \\left( \\frac{\\partial V}{\\partial t} + \\mu S \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - \\Delta \\mu S \\right) dt + \\left( \\sigma S \\frac{\\partial V}{\\partial S} - \\Delta \\sigma S \\right) dW_t\n\\]\n\nEliminating the Randomness\nThe second set of parentheses contains the \\(dW_t\\) term. This is the sole source of risk in our portfolio. If the market suddenly crashes or spikes, this is the term that dictates our losses or gains.\nThe brilliance of the Black-Scholes model lies in a simple algebraic observation: we can force that entire term to equal zero. We do this by continuously adjusting the number of shares we short (\\(\\Delta\\)).\nIf we deliberately choose: \\[\n\\Delta = \\frac{\\partial V}{\\partial S}\n\\]\nThen the random term becomes: \\[\n\\left( \\sigma S \\frac{\\partial V}{\\partial S} - \\frac{\\partial V}{\\partial S} \\sigma S \\right) dW_t = 0 \\cdot dW_t = 0\n\\]\nBy setting our stock position equal to the partial derivative of the option with respect to the stock (which traders now call the option‚Äôs ‚ÄúDelta‚Äù), the random shocks completely cancel out. The portfolio becomes completely immune to small movements in the underlying asset.\nWhat we are left with is a portfolio whose change in value is entirely deterministic, driven only by the passage of time: \\[\nd\\Pi = \\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt\n\\]\n\n\n\n\nStep 3: The Principle of No-Arbitrage and the Black-Scholes PDE\nAt the end of Step 2, we successfully engineered a portfolio \\(\\Pi\\) that is completely insulated from the random shocks of the stock market. Because we chose to hold \\(\\Delta = \\frac{\\partial V}{\\partial S}\\) shares of stock short against our long option, the \\(dW_t\\) term was mathematically forced to zero.\nWe were left with a portfolio whose change in value is entirely deterministic, driven only by the smooth passage of time: \\[\nd\\Pi = \\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt\n\\]\nNow, we must ask an economic question: If a portfolio has absolutely zero risk, how much return should it generate?\n\nThe Financial Law of Gravity: No-Arbitrage\nIn an efficient market, there is a fundamental law known as the Principle of No-Arbitrage. An ‚Äúarbitrage‚Äù is a guaranteed, risk-free profit that requires zero net investment.\nIf our completely riskless portfolio \\(\\Pi\\) earned a return higher than a risk-free government bond (yielding the risk-free rate, \\(r\\)), an arbitrageur could borrow infinite amounts of money at rate \\(r\\) from the bank, invest it into our portfolio \\(\\Pi\\), and pocket the difference completely risk-free. Conversely, if \\(\\Pi\\) earned less than \\(r\\), an arbitrageur would short our portfolio and put the cash into the bank, again locking in a risk-free profit.\nBecause market forces instantly exploit and close these loopholes, a fundamental truth emerges: Any portfolio with zero risk must earn exactly the risk-free interest rate.\nMathematically, the growth of our portfolio over the time step \\(dt\\) must equal the continuously compounded risk-free rate \\(r\\) multiplied by the current value of the portfolio: \\[\nd\\Pi = r \\Pi dt\n\\]\n\n\nEquating the Math and the Economics\nWe now have two completely different ways to describe the change in our portfolio (\\(d\\Pi\\)). 1. The Calculus View (from It√¥‚Äôs Lemma): \\(d\\Pi = \\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt\\) 2. The Economics View (from No-Arbitrage): \\(d\\Pi = r \\Pi dt\\)\nBecause these describe the exact same portfolio, we can set them equal to each other: \\[\n\\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt = r \\Pi dt\n\\]\nRecall from Step 2 that the total value of our portfolio is \\(\\Pi = V - \\Delta S\\), and we defined \\(\\Delta = \\frac{\\partial V}{\\partial S}\\). Let‚Äôs substitute that back in: \\[\n\\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt = r \\left( V - \\frac{\\partial V}{\\partial S} S \\right) dt\n\\]\nSince both sides are simply multiplying a rate by the time step \\(dt\\), we can divide both sides by \\(dt\\) to remove it entirely: \\[\n\\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} = rV - rS \\frac{\\partial V}{\\partial S}\n\\]\nFinally, let‚Äôs rearrange this equation by moving everything to the left side so that it equals zero.\n\\[\n\\frac{\\partial V}{\\partial t} + rS \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - rV = 0\n\\]\n\n\nThe Grand Reveal: Deconstructing the PDE\nThis is it. This is the Black-Scholes Partial Differential Equation. It is one of the most famous equations in finance, and every single term has a physical, trading interpretation, often mapped directly to the ‚ÄúGreeks‚Äù:\n\n\\(\\frac{\\partial V}{\\partial t}\\) (Theta - Time Decay): This is negative for options. As time ticks closer to expiration, the option loses value because there is less time for the stock to make a favorable move.\n\\(rS \\frac{\\partial V}{\\partial S}\\) (Risk-Neutral Drift): This term represents the cost of financing the Delta-hedged position in the stock.\n\\(\\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2}\\) (Gamma - Convexity): This is the engine of the option‚Äôs value. Because the second derivative (\\(\\Gamma\\)) is positive, volatility (\\(\\sigma^2\\)) directly adds value to the option. This perfectly offsets the bleed from time decay (\\(\\Theta\\)).\n\\(- rV\\) (Discounting): This simply discounts the final payoff of the option back to present value at the risk-free rate.\n\nThe beauty of the Black-Scholes PDE is that the expected return of the stock (\\(\\mu\\)) has completely vanished! Because we can hedge away the directional risk of the stock, the option‚Äôs price depends only on the stock‚Äôs volatility (\\(\\sigma\\)), not its expected direction.\n\n\n\n\nInterlude: The Fork in the Road (How do we solve this PDE?)\nAt the end of Step 3, we successfully derived the Black-Scholes PDE: \\[\n\\frac{\\partial V}{\\partial t} + rS \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - rV = 0\n\\]\nThis equation dictates exactly how the option price must behave to prevent arbitrage. But an equation is not a price. To get a dollar value, we have to solve this PDE subject to a boundary condition: at maturity (\\(T\\)), a call option is worth exactly \\(\\max(S_T - K, 0)\\).\nFrom here, quantitative finance splits into two distinct paths: Numerical Methods and Analytical Methods.\n\nPath A: The Numerical Approach (Finite Difference Methods)\nInstead of looking for a perfect algebraic formula, Numerical Methods use brute-force computation. We chop time and asset prices into a massive grid (a matrix). We know the exact payoff at expiration (the rightmost edge of the grid), and we mathematically step backward through time, cell by cell, until we reach ‚Äútoday‚Äù (the leftmost edge).\nTo do this, we approximate the continuous derivatives from our PDE using discrete ‚Äúdifferences‚Äù on our grid. Let \\(V_i^j\\) be the option price at stock price node \\(i\\) and time step \\(j\\). * Delta (First Derivative of Space): \\(\\frac{\\partial V}{\\partial S} \\approx \\frac{V_{i+1}^j - V_{i-1}^j}{2\\Delta S}\\) * Gamma (Second Derivative of Space): \\(\\frac{\\partial^2 V}{\\partial S^2} \\approx \\frac{V_{i+1}^j - 2V_i^j + V_{i-1}^j}{(\\Delta S)^2}\\) * Theta (First Derivative of Time): \\(\\frac{\\partial V}{\\partial t} \\approx \\frac{V_i^{j+1} - V_i^j}{\\Delta t}\\)\nBy substituting these grid approximations into the Black-Scholes PDE, we can build a programmatic solver. There are three main ways to construct this grid, trading off between computational speed and mathematical stability.\n\n1. The Explicit Scheme\nThe Explicit method is the simplest. To find the unknown option price at a specific node one step back in time (\\(V_i^j\\)), it relies only on the already-known information from the nodes immediately ahead of it in the future (\\(V_{i-1}^{j+1}, V_i^{j+1}, V_{i+1}^{j+1}\\)).\n\nPros: It is incredibly easy to code. The calculation for each node is a simple linear equation.\nCons: It is conditionally stable. If your time steps (\\(\\Delta t\\)) are too large compared to your price steps (\\(\\Delta S\\)), the mathematical errors compound exponentially. The grid ‚Äúblows up,‚Äù and your computer will output an option price of infinity.\n\n\n\n2. The Implicit Scheme\nThe Implicit method flips the logic. Instead of expressing the current time step as a function of the future, it expresses the future known boundary conditions as a function of the current unknown time step.\nBecause multiple unknown nodes depend on each other simultaneously, you cannot calculate them one by one. You must solve the entire column of the grid at the exact same time using a system of linear equations (specifically, by inverting a tridiagonal matrix).\n\nPros: It is unconditionally stable. No matter how large you make your time steps, the math will never blow up.\nCons: It is computationally expensive. Solving a massive matrix at every single time step requires significantly more processing power.\n\n\n\n3. The Crank-Nicolson Scheme (The Industry Standard)\nIn 1947, John Crank and Phyllis Nicolson realized that if you take an exact 50/50 average of the Explicit and Implicit methods, magical things happen to the error terms.\nThe Crank-Nicolson method centers the time difference exactly halfway between step \\(j\\) and step \\(j+1\\). Like the Implicit method, it requires solving a tridiagonal matrix (often using a lightning-fast technique called the Thomas Algorithm).\n\nThe Verdict: This is the gold standard for PDE options pricing. It is unconditionally stable (like Implicit) but converges on the true price much faster and with significantly higher accuracy than the other two methods.\n\nWhy use Finite Difference Methods? If we just want to price a standard European option, FDM is overkill. But for American Options, which can be exercised early, FDM is mandatory. Because we step backward through time cell by cell, we can stop at every single node and ask the computer: ‚ÄúIs the theoretical option price here lower than the immediate payoff of early exercise? If yes, replace the theoretical price with the exercise value.‚Äù\n\n\n\nPath B: The Analytical Approach\nWhile grids are powerful, for European options (which cannot be exercised early), we do not need to step through time mechanically. We can use advanced probability theory to solve the PDE perfectly, instantly, and in one continuous leap.\nThis brings us back to our derivation, and the genius of Richard Feynman and Mark Kac.\n\n\n\n\nStep 4: The Risk-Neutral Expectation (Feynman-Kac)\nSolving this PDE directly requires complex substitutions to turn it into a heat equation. Instead, we use the Feynman-Kac theorem, which states that the solution to this PDE, subject to the European Call boundary condition at maturity \\(V(S, T) = \\max(S_T - K, 0)\\), can be written as a discounted expected value under the risk-neutral measure \\(\\mathbb{Q}\\):\n\\[\nC(S,t) = e^{-r(T-t)} \\mathbb{E}^{\\mathbb{Q}}[\\max(S_T - K, 0)]\n\\]\n\nThe Risk-Neutral Measure (\\(\\mathbb{Q}\\))\nNotice the \\(\\mathbb{Q}\\) above the expectation operator. This indicates that we are calculating the expectation under the Risk-Neutral Measure, not the real-world probability measure (\\(\\mathbb{P}\\)).\nWhy? Remember when we delta-hedged our portfolio in Step 2? The real-world expected return of the stock (\\(\\mu\\)) completely disappeared from our math. Because the option price does not care about the stock‚Äôs true expected growth, we can pretend we live in a simplified ‚Äúrisk-neutral‚Äù universe. In this universe, all assets‚Äîincluding the risky stock‚Äîare expected to grow at exactly the risk-free rate \\(r\\).\nSo, we rewrite our stock‚Äôs stochastic differential equation (SDE), replacing \\(\\mu\\) with \\(r\\): \\[\ndS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}_t\n\\]\n\n\nSolving for the Terminal Stock Price (\\(S_T\\))\nTo evaluate our expected payoff, we need an explicit formula for \\(S_T\\). We solve the risk-neutral SDE by applying It√¥‚Äôs Lemma to the natural log of the stock price, \\(f(S_t) = \\ln(S_t)\\).\nUsing It√¥‚Äôs Lemma on \\(\\ln(S_t)\\) yields: \\[\nd(\\ln S_t) = \\left( r - \\frac{1}{2}\\sigma^2 \\right) dt + \\sigma dW^{\\mathbb{Q}}_t\n\\]\n(Notice the \\(-\\frac{1}{2}\\sigma^2\\) term appearing! This is the exact mathematical origin of the volatility drag we discussed in Section 1. The variance mathematically pulls down the expected geometric growth rate.)\nBecause the drift and volatility terms are now constants, we can cleanly integrate both sides from today (\\(t\\)) to expiration (\\(T\\)). Let \\(\\tau = T - t\\) (the time to maturity). \\[\n\\ln(S_T) - \\ln(S_t) = \\left( r - \\frac{1}{2}\\sigma^2 \\right)\\tau + \\sigma (W_T - W_t)\n\\]\nBy taking the exponential of both sides, we get the explicit, log-normal distribution for the stock price at maturity: \\[\nS_T = S_t \\exp\\left( \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma \\sqrt{\\tau} Z \\right)\n\\]\nWhere \\(Z \\sim \\mathcal{N}(0,1)\\) is a standard normal random variable representing the total accumulated randomness between \\(t\\) and \\(T\\).\n\n\n\n\nStep 5: Splitting and Solving the Integrals\nWe can now rewrite our expectation as an integral over the standard normal Probability Density Function (PDF), which is \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}\\):\n\\[\nC(S,t) = e^{-r\\tau} \\int_{-\\infty}^{\\infty} \\max(S_T(z) - K, 0) \\phi(z) dz\n\\]\n\nFinding the Boundary Condition (Dropping the Max Function)\nThe option only has a payoff when \\(S_T &gt; K\\). We need to find the critical value of our normal variable \\(Z\\) where the stock price exactly equals the strike price (\\(S_T = K\\)):\n\\[\nS_t \\exp\\left( \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma \\sqrt{\\tau} Z \\right) &gt; K\n\\] Take the natural log of both sides: \\[\n\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma \\sqrt{\\tau} Z &gt; \\ln\\left(\\frac{K}{S_t}\\right)\n\\] Isolate \\(Z\\): \\[\nZ &gt; \\frac{\\ln(K/S_t) - (r - \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}}\n\\]\nLet‚Äôs define this entire right-side boundary threshold as \\(-d_2\\). Thus, the option is ‚Äúin-the-money‚Äù and pays off whenever \\(Z &gt; -d_2\\). Because we know the lower bound, we can drop the \\(\\max()\\) function and restrict our integral bounds:\n\\[\nC(S,t) = e^{-r\\tau} \\int_{-d_2}^{\\infty} (S_T(z) - K) \\phi(z) dz\n\\]\n\n\nSplitting the Integral\nWe expand the terms and split this into two separate integrals: \\[\nC(S,t) = \\underbrace{e^{-r\\tau} \\int_{-d_2}^{\\infty} S_T(z) \\phi(z) dz}_{\\text{Part A (Stock)}} \\quad - \\quad \\underbrace{K e^{-r\\tau} \\int_{-d_2}^{\\infty} \\phi(z) dz}_{\\text{Part B (Strike)}}\n\\]\n\n\nSolving Part B (The Strike Portion)\nPart B is incredibly straightforward. It is simply the discounted strike price multiplied by the probability that \\(Z &gt; -d_2\\). Because the standard normal distribution is perfectly symmetric, the probability of being greater than \\(-d_2\\) is identical to the probability of being less than \\(d_2\\). Let \\(N(\\cdot)\\) denote the standard normal Cumulative Distribution Function (CDF). \\[\nP(Z &gt; -d_2) = P(Z &lt; d_2) = N(d_2)\n\\]\nSo, Part B simplifies beautifully to: \\[\nK e^{-r\\tau} N(d_2)\n\\]\n\n\nSolving Part A (The Stock Portion)\nPart A is trickier because \\(S_T(z)\\) also contains an exponential function. Let‚Äôs substitute \\(S_T(z)\\) and \\(\\phi(z)\\) into the integral: \\[\ne^{-r\\tau} \\int_{-d_2}^{\\infty} S_t e^{(r - \\sigma^2/2)\\tau + \\sigma \\sqrt{\\tau} z} \\left( \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\right) dz\n\\]\nWe pull the constants out of the integral and combine the \\(e\\) exponents: \\[\nS_t e^{-r\\tau} e^{r\\tau - \\sigma^2\\tau/2} \\int_{-d_2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}z^2 + \\sigma \\sqrt{\\tau} z} dz\n\\]\nThe \\(e^{-r\\tau}\\) and \\(e^{r\\tau}\\) cancel out outside the integral. Inside the integral, we must complete the square in the exponent to turn it back into a recognizable normal distribution: \\[\n-\\frac{1}{2}z^2 + \\sigma \\sqrt{\\tau} z = -\\frac{1}{2}(z^2 - 2\\sigma \\sqrt{\\tau} z)\n\\] \\[\n(z^2 - 2\\sigma \\sqrt{\\tau} z) = (z - \\sigma \\sqrt{\\tau})^2 - \\sigma^2 \\tau\n\\]\nSubstituting this completed square back into the exponent: \\[\nS_t e^{-\\sigma^2\\tau/2} \\int_{-d_2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}(z - \\sigma \\sqrt{\\tau})^2 + \\sigma^2\\tau/2} dz\n\\]\nThe \\(e^{-\\sigma^2\\tau/2}\\) on the outside and \\(e^{\\sigma^2\\tau/2}\\) on the inside cancel perfectly! We are left with: \\[\nS_t \\int_{-d_2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}(z - \\sigma \\sqrt{\\tau})^2} dz\n\\]\nThis integral is just a standard normal distribution that has been shifted by a mean of \\(\\sigma \\sqrt{\\tau}\\). Let‚Äôs make a final substitution: let \\(y = z - \\sigma \\sqrt{\\tau}\\), so \\(dy = dz\\). When \\(z = -d_2\\), our new lower bound becomes \\(y = -d_2 - \\sigma \\sqrt{\\tau}\\).\nBy mathematical definition, we call this new boundary \\(-d_1\\). Therefore: \\[\nd_1 = d_2 + \\sigma \\sqrt{\\tau}\n\\]\nThe integral becomes: \\[\nS_t \\int_{-d_1}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2} dy\n\\]\nJust like in Part B, this is simply \\(S_t \\times P(Y &gt; -d_1)\\). Due to symmetry, this equals \\(S_t N(d_1)\\).\n\n\n\n\nStep 6: The Final Black-Scholes Formula\nBy recombining our solved integrals (Part A - Part B), we finally arrive at the glorious, closed-form Black-Scholes formula for a European Call option:\n\\[\nC(S, t) = S_t N(d_1) - K e^{-r\\tau} N(d_2)\n\\]\nWhere: \\[\nd_1 = \\frac{\\ln(S_t/K) + (r + \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}}\n\\] \\[\nd_2 = d_1 - \\sigma \\sqrt{\\tau} = \\frac{\\ln(S_t/K) + (r - \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}}\n\\]\n\nFinancial Intuition of the Formula\nThe math is beautiful, but what does it actually mean for a trader? * \\(N(d_2)\\) is the risk-neutral probability that the option will expire ‚Äúin-the-money‚Äù (that \\(S_T &gt; K\\)). Therefore, \\(K e^{-r\\tau} N(d_2)\\) is simply the expected cost of having to pay the strike price at expiration, discounted to today. * \\(N(d_1)\\) is the option‚Äôs Delta (\\(\\Delta\\)). It represents the probability-weighted expected value of the stock, conditional on the option expiring in the money. * Therefore, the formula simply says: Call Price = (Expected Benefit of receiving the stock) - (Expected Cost of paying the strike)."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "The content on this website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n\n\nCreative Commons License Logo"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Jordan's Projects",
    "section": "",
    "text": "hello"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I‚Äôm [Your Name]. This is where you can write a more detailed biography‚Ä¶"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Contact \n\n\n\n&lt;label for=\"full-name\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Full Name&lt;/label&gt;\n&lt;input type=\"text\" id=\"full-name\" name=\"name\" required style=\"width: 100%; padding: 0.5em;\"&gt;\n\n\n&lt;label for=\"email-address\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Email Address&lt;/label&gt;\n&lt;input type=\"email\" id=\"email-address\" name=\"_replyto\" required style=\"width: 100%; padding: 0.5em;\"&gt;\n\n\n&lt;label for=\"message\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Message&lt;/label&gt;\n&lt;textarea id=\"message\" name=\"message\" rows=\"6\" required style=\"width: 100%; padding: 0.5em;\"&gt;&lt;/textarea&gt;\n\n\nSend Message"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jordan Chong",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Email\n  \n  \n    \n     CV\n  \n\n  \n  \nStudied at Singapore Management University (SMU), Bachelor of Business Administration (Finance) with Second Major in Data Science.\nPreviously worked in Debt Capital Markets at DBS Bank and Advanced Manufacturing Development at EDB.\nCurrently, I am focused on building robust pricing models, researching market microstructure, and applying ML algorithms to financial datasets.\n\n\n\n\n\nFeatured Projects\n\nView all projects ‚Üí\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingapore Insurance Strategy Dashboard\n\n\n\nPython\n\nStreamlit\n\nActuarial Science\n\nData Visualization\n\nMonte Carlo\n\nfeatured\n\n\n\nA Streamlit-powered actuarial dashboard that pits ‚ÄòBuy Term, Invest the Difference‚Äô against Whole Life plans, accounting for human behavior and stochastic risk.\n\n\n\n\n\n\n\n\n\n\n\n\nGap Challenge Solver\n\n\n\nComputer Vision\n\nPython\n\nStreamlit\n\nAlgorithms\n\nfeatured\n\n\n\nEvolution of a Computer Vision pipeline: From brittle Template Matching to robust SVMs, integrated with a Backtracking solver.\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production\n\n\n\nQuantitative Finance\n\nVolatility Modeling\n\nPython\n\nOOP\n\nStreamlit\n\nOptimization\n\nfeatured\n\n\n\nA comprehensive case study bridging Quantitative Research and Software Engineering. From calibrating Heston and Bates models using noisy S&P 500 tick data to deploying an‚Ä¶\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Technical Notes",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nConcept: The Black-Scholes Model\n\n\n\nStochastic Calculus\n\nOptions Pricing\n\nQuantitative Finance\n\n\n\nA complete mathematical derivation of the Black-Scholes call option formula, from Geometric Brownian Motion to the final PDE and Feynman-Kac solution.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html",
    "href": "projects/gap-challenge-solver/index.html",
    "title": "Gap Challenge Solver",
    "section": "",
    "text": "Source idea credited to Dianyi Yang: kv9898/gap_challenge_solver\nYou can try the exercises yourself here:\n- Gap Challenge\n- Gap Challenge Practice"
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#the-challenge",
    "href": "projects/gap-challenge-solver/index.html#the-challenge",
    "title": "Gap Challenge Solver",
    "section": "The Challenge",
    "text": "The Challenge\nThe ‚ÄúGap‚Äù puzzle is a logic game where players must fill a missing cell in a grid, ensuring no symbol repeats in any row or column. My goal was to build a tool that could solve this instantly from a simple screenshot.\n\n\n\n\n\n\n4x4 Grid\n\n\n\n\n\n\n\n5x5 Grid\n\n\n\n\n\n\nGap Challenge - Each shape can only appear in a row or column once. The player must use logic by elimination to find the shape that belongs in the question mark.\n\n\n\nWhile the logic puzzle itself is a standard Constraint Satisfaction Problem, digitizing the grid from a raw image proved to be the real engineering challenge. This post documents the three major iterations (‚ÄúActs‚Äù) of my Computer Vision pipeline and the lessons learned from each:\n\nAct 1: Naive Template Matching\n\nAct 2: Geometric Determination of Shapes\n\nAct 3: Image Classification with SVM\n\n\nMajor Problem Faced with Image Recognition\nA major issue was the different forms that the shapes could take based on different Gap Challenge applications. Some shapes were hollow, some backgrounds were black and some bounding grids were just lines.\n\n\n\n\n\n\nNormal Version\n\n\n\n\n\n\n\nLine Version\n\n\n\n\n\n\n\nDark-mode Version\n\n\n\n\n\n\nNote the difference in shapes and grid colors. The hard part was building a robust solution that would be able to handle all these different versions."
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#act-i-the-naive-approach-template-matching",
    "href": "projects/gap-challenge-solver/index.html#act-i-the-naive-approach-template-matching",
    "title": "Gap Challenge Solver",
    "section": "Act I: The Naive Approach (Template Matching)",
    "text": "Act I: The Naive Approach (Template Matching)\nMy first instinct was to use Template Matching. I cropped reference images of each shape (Star, Circle, Square) and used cv2.matchTemplate to slide them over the screenshot, looking for pixel-perfect matches. I tried to account for the different size of the screenshots as well, scaling the reference shapes to match the screenshots.\n\n\n\nReference Images used for template matching\n\n\n\n\n\nHeatmap matching of the reference images. Green boxes show a &gt;90% match score against the reference image.\n\n\n\nThe Cause of Failure\nThis method worked well on the base example but failed when tested on other forms of the puzzle. Although I implemented multi-scale matching to handle zoom levels, the approach hit a hard ceiling when the puzzle‚Äôs visual style changed.\nWhen the game switched to ‚ÄúDark Mode‚Äù or used ‚ÄúHollow‚Äù shapes, my solid-colored templates failed immediately. To fix this, I would have needed to crop and save a new template for every possible theme variation.\n\n\n\n\n\n\nFailure Case 1: Hollow Shapes\n\n\n\n\n\n\n\nFailure Case 2: Dark Mode\n\n\n\n\n\n\nLesson: Hard-coding pixel comparisons doesn‚Äôt work if the inputs vary significantly. I needed a more robust method and decided to try and use the geometric differences in shapes to differentiate them."
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#act-ii-the-geometric-pivot-heuristics",
    "href": "projects/gap-challenge-solver/index.html#act-ii-the-geometric-pivot-heuristics",
    "title": "Gap Challenge Solver",
    "section": "Act II: The Geometric Pivot (Heuristics)",
    "text": "Act II: The Geometric Pivot (Heuristics)\nFor the second iteration, I moved to Feature Extraction. Instead of matching images, I used cv2.findContours to extract the shapes and analyze their geometric properties.\nI built a Decision Tree based on metrics like:\n- Vertices: cv2.approxPolyDP (e.g., 3 vertices = Triangle).\n- Solidity: Area divided by Convex Hull Area (distinguishes solid Squares from hollow Crosses).\n- Circularity: \\(4\\pi \\times \\frac{Area}{Perimeter^2}\\) (distinguishes Circles from Triangles).\n\nThe ‚ÄúLive Scanner‚Äù Debugger\nTo refine my thresholds, I built a visualization tool that calculated these metrics for every cell so that I could debug the errors efficiently.\n\n\n\nThe Logic Dashboard. Watching the metrics update in real-time allowed me to tune the decision tree.\n\n\n\n\nThe Bottleneck\nWhile robust against scale, this method suffered from ‚ÄúWhack-a-Mole‚Äù logic, where more issues would pop-up after fixing one and I realised I was trying to fix symptoms instead of fixing the root cause of the recognition issues:\n- The ‚ÄúHollow‚Äù Edge Case: A hollow square has low solidity, confusing the algorithm.\n- The ‚ÄúQuestion Mark‚Äù Problem: The question mark consists of two separate blobs (hook + dot), breaking the single-contour logic.\n\n\n\n\n\n\nFailure Case: Hollow Shapes\n\n\n\n\n\n\n\nFailure Case: Dark Mode\n\n\n\n\n\nThe ‚ÄúWhack-a-Mole‚Äù logic resulted in decision trees like:\n# Attempts to handle edge cases manually\nif vertices &gt; 8 and circularity &lt; 0.6 and num_blobs == 1:\n    return \"Star\"\nelif num_blobs == 2:\n    return \"Question\"\nThis resulted in more and more rules that I wasn‚Äôt even sure of and I decided a needed a system to learn these rules itself."
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#act-iii-the-robust-solution-support-vector-machines",
    "href": "projects/gap-challenge-solver/index.html#act-iii-the-robust-solution-support-vector-machines",
    "title": "Gap Challenge Solver",
    "section": "Act III: The Robust Solution (Support Vector Machines)",
    "text": "Act III: The Robust Solution (Support Vector Machines)\nIn the final phase, I treated this as a classification problem. I chose a Support Vector Machine (SVM) because they perform exceptionally well on high-dimensional binary data (pixel maps) with smaller datasets.\n\n1. The Data Pipeline (Solving Distribution Shift)\nA mistake that I made at the start was training on ‚Äúperfect‚Äù data but testing on ‚Äúmessy‚Äù data.\nI manually took multiple screenshots of the grids, with minor variations, to create a large train dataset for the SVM.\nHowever, I realised that the SVM was not performing well, because the cropping method used for the test data was different from the perfectly cropped screenshots i was taking for the train data.\nTo fix this, I used the test cropping algorithm to build an automated pipeline (collect_feedback_data.py) that extracted thousands of training examples directly from screenshots.\nThis ensured my training data included all the real-world noise (grid lines, anti-aliasing) that the model would see in production. I utilized Otsu‚Äôs Binarization here to automatically find the optimal threshold for separating shapes from the background, solving the ‚ÄúDark Mode‚Äù issue.\n\n\n\nAutomatic cropping and extraction of train data and grids for the SVM to train on.\n\n\n\n\n2. The ‚ÄúUniversal Translator‚Äù (Preprocessing)\nTo make the model immune to lighting changes and user cropping errors, I engineered a standardization pipeline: 1. Resize to 64x64.\n2. Corner Difference Strategy: Calculate the average background color from the image corners and subtract it. This isolates the shape regardless of whether the theme is Dark or Light.\n3. Centering: Find the bounding box of the shape and center it on a black canvas.\n\n\n3. Fine-Tuning the Brain (Grid Search)\nAn SVM is only as good as its hyperparameters. Default settings often lead to overfitting, where the model memorizes the training data but fails on new inputs.\nTo solve this, I implemented a Grid Search to exhaustively test combinations of C (Regularization) and Gamma (Kernel Coefficient). This allowed me to mathematically find the configuration that ignored minor pixel noise while maintaining a distinct decision boundary between similar shapes (like the Circle and the hollow Ring).\n\n\n4. X-Ray Vision (Verification)\nMachine Learning models can be ‚Äúblack boxes.‚Äù To trust the system, I built a side-by-side debugger comparing the Human View (RGB) against the Machine View (Preprocessed). This allowed me to catch preprocessing errors‚Äîlike the dot of the ‚Äò?‚Äô being filtered out as noise‚Äîbefore retraining the model.\n\n\n\nSVM Debugger. Left is the game input; Right is what the model actually sees."
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#the-solver-logic-backtracking",
    "href": "projects/gap-challenge-solver/index.html#the-solver-logic-backtracking",
    "title": "Gap Challenge Solver",
    "section": "The Solver Logic (Backtracking)",
    "text": "The Solver Logic (Backtracking)\nWith a reliable Vision pipeline returning a 2D matrix (e.g., [['Star', 'Empty'], ['Circle', 'Square']]), the problem reduced to a standard algorithm.\nI implemented a Recursive Backtracking solver. It works by:\n1. Scanning for an empty cell.\n2. Guessing a shape from the available options.\n3. Validating row/column constraints.\n4. Recursively attempting to solve the rest of the grid.\n5. Backtracking if a guess leads to a contradiction.\ndef solve_with_backtracking(board, universe):\n    # Base Case: No empty spots left\n    if not find_empty(board): return True \n    \n    row, col = find_empty(board)\n    \n    for shape in universe:\n        if is_valid_move(board, shape, row, col):\n            board[row][col] = shape\n            # Recursive Step\n            if solve_with_backtracking(board, universe): return True\n            # Backtrack\n            board[row][col] = 'blank'\n            \n    return False"
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#conclusion-tech-stack",
    "href": "projects/gap-challenge-solver/index.html#conclusion-tech-stack",
    "title": "Gap Challenge Solver",
    "section": "Conclusion & Tech Stack",
    "text": "Conclusion & Tech Stack\nThis project was a lesson in selecting the right tool for the job. While deep learning (CNNs) could have solved this, it would have been overkill for a 5-class problem. An SVM combined with robust Data Engineering provided a lightweight, instant, and highly accurate solution (99% accuracy on test set).\n\nLanguage: Python 3.10\nVision: OpenCV (Canny, Otsu‚Äôs Binarization, Contours)\nML: Scikit-Learn (SVM, GridSearch)\nFrontend: Streamlit"
  },
  {
    "objectID": "projects/quant-structuring-workbench/index.html#the-full-stack-quant-philosophy",
    "href": "projects/quant-structuring-workbench/index.html#the-full-stack-quant-philosophy",
    "title": "End-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production",
    "section": "The ‚ÄúFull-Stack Quant‚Äù Philosophy",
    "text": "The ‚ÄúFull-Stack Quant‚Äù Philosophy\nIn the financial industry, there is often a strict divide between Quantitative Researchers (who derive the math and calibrate models in isolated Jupyter notebooks) and Quantitative Developers (who translate that math into scalable, object-oriented code for the trading desk).\nThis project was built to bridge that exact gap. It is divided into two distinct phases, documenting the iterative journey from raw mathematical theory to a live, interactive production environment.\n\nüöÄ Launch Live Structuring App"
  },
  {
    "objectID": "projects/quant-structuring-workbench/index.html#phase-i-the-quantitative-research-lab-mathematics-calibration",
    "href": "projects/quant-structuring-workbench/index.html#phase-i-the-quantitative-research-lab-mathematics-calibration",
    "title": "End-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production",
    "section": "Phase I: The Quantitative Research Lab (Mathematics & Calibration)",
    "text": "Phase I: The Quantitative Research Lab (Mathematics & Calibration)\nBefore an options pricing engine can be trusted by a structuring desk, it must be validated against historical market data. The goal of this phase was to calibrate three advanced models‚ÄîMerton Jump-Diffusion, Heston Stochastic Volatility, and the Bates (SVJ) Model‚Äîto raw S&P 500 options data from the Wharton Research Data Services (WRDS).\n\nIteration 1: Navigating the Data Pipeline\nReal market data is inherently noisy and structurally idiosyncratic. Building the MarketDataLoader required several critical formatting iterations: 1. The WRDS Strike Quirk: WRDS stores strike prices multiplied by 1,000. Initial pricing runs returned catastrophic NaNs because a $4,780 strike was being read as $4,780,000. A normalization layer (chain['strike_price'].values / 1000.0) had to be injected. 2. Trimming the Fat: An option chain contains thousands of highly illiquid, deep Out-of-the-Money strikes with zero bids. Feeding these into an optimizer guarantees failure. I implemented a dynamic filtering mechanism (strike_bound_pct=0.15) to isolate only the highly liquid strikes within a 15% radius of the Spot price, ensuring the optimizer only trained on pure market signal.\n\n\nIteration 2: The Heston Model and The Gibbs Phenomenon\nThe most mathematically demanding phase of the project was implementing the Heston model‚Äôs Characteristic Function. Unlike Black-Scholes, Heston requires complex Fourier inversion to evaluate the pricing integral.\n\nThe Bug: During the initial calibration, the plotted Implied Volatility curve exhibited violent, unnatural sine-wave artifacts on the extreme wings of the smile.\nThe Diagnosis: This was a classic manifestation of the Gibbs Phenomenon . The scipy.integrate.quad function was truncating the integration of the probability wave too early (at an upper limit of 100). Because short-dated options (\\(T = 0.10\\)) have very slowly decaying characteristic functions, chopping the math off artificially created literal ripples in the pricing space.\nThe Fix: I re-engineered the integration block, pushing limit_max to 2000 and tightening the absolute and relative error tolerances (epsabs=1e-4, epsrel=1e-4). The artifacts vanished, resulting in a flawless, continuous pricing curve.\n\n\n\nIteration 3: Overcoming the 20-Minute Bottleneck\nWith the math corrected, I deployed a Hybrid Optimizer: a Global Differential Evolution algorithm to scout the 5-dimensional parameter space, followed by a Local L-BFGS-B sniper to pinpoint the exact local minimum.\n\nThe Bottleneck: The initial run took over 21 minutes to calibrate. Profiling the math engine revealed the ‚ÄúSilent Killer‚Äù of quant optimization: nested root-finding loops. To calculate the Mean Squared Error (MSE), the objective function had to calculate a Heston price, and then run a Black-Scholes root-finder (Brent‚Äôs Method) to reverse-engineer the IV for all 350 strikes, thousands of times.\nThe 10x Speed Fix: Recognizing that a strike at 4780 provides the exact same structural information as a strike at 4785, I implemented a data Downsampling Algorithm (target_strikes[::sample_step]). By forcing the optimizer to train on 35 representative strikes instead of 350, calibration time plummeted from 21 minutes to under 2 minutes with zero loss in curve fidelity.\n\n\n\nIteration 4: The Bates Model & Parameter Redundancy\nThe ‚ÄúFinal Boss‚Äù of the research phase was the Bates (SVJ) model, which merges Heston‚Äôs stochastic volatility with Merton‚Äôs Poisson jumps. * Parameter Seeding: To optimize this 8-parameter monster, I seeded the algorithm with the winning parameters from the individual Heston and Merton runs, allowing it to calibrate in under 60 seconds. * The Over-parameterization Trap: The optimizer revealed a fascinating quantitative reality. It forced the average jump size (\\(\\mu_J\\)) to exactly \\(0.0\\). Because Heston‚Äôs correlation parameter (\\(\\rho = -0.65\\)) was already dragging the left side of the skew upward sufficiently, the optimizer deemed the Merton jumps mathematically redundant for this specific 30-day slice. This served as a masterclass in the dangers of over-parameterization in quantitative modeling.\n\n\n\n\n\n\nNotePart I: The Theoretical Frameworks (The ‚ÄúTextbook‚Äù)\n\n\n\nBefore touching real data, I built Monte Carlo simulators to visually prove the mathematical intuition behind each model.\n\nModel 1: Black-Scholes & The Flaw of Constant Volatility\nModel 2: Merton Jump-Diffusion & Market Shocks\nModel 3: The Heston Model & The Leverage Effect (\\(\\rho\\))\nModel 4: The Bates Model (SVJ) - The Holy Grail\n\n\n\n\n\n\n\n\n\nTipPart II: Real Market Calibration (The ‚ÄúLab‚Äù)\n\n\n\nIngesting raw WRDS data for SPX options, I built objective functions to reverse-engineer Implied Volatilities and minimize pricing errors.\n\nCalibrating Black-Scholes (Proving the Disconnect)\nCalibrating Merton (Fitting the Short-Term Skew)\nCalibrating Heston (Hybrid Global-to-Local Search)\nCalibrating Bates (Parameter Seeding)\n\n\n\n\n\n\n\n\n\nImportantPart III: Synthesis & Predictive Power\n\n\n\nCurve-fitting today‚Äôs data is easy; predicting tomorrow‚Äôs is the true test of a quant.\n\nThe Master Comparison: Heston vs.¬†Merton vs.¬†Bates\nTime-Series Out-of-Sample (OOS) Testing"
  },
  {
    "objectID": "projects/quant-structuring-workbench/index.html#phase-ii-the-interactive-structuring-desk-deployment",
    "href": "projects/quant-structuring-workbench/index.html#phase-ii-the-interactive-structuring-desk-deployment",
    "title": "End-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production",
    "section": "Phase II: The Interactive Structuring Desk (Deployment)",
    "text": "Phase II: The Interactive Structuring Desk (Deployment)\nA robust math engine is useless if a trader cannot interact with it. Phase II focused on abstracting the math into a scalable software architecture and deploying it via Streamlit.\n\nIteration 1: The Object-Oriented Refactor\nJupyter notebooks run linearly, which makes them terrible for building dynamic portfolios. To solve this, I entirely refactored the pricing engine using Object-Oriented Programming (OOP). * Abstract Base Classes: I designed a core Instrument class enforcing strict polymorphic protocols (price(), payoff()). * Encapsulation: Subclasses like ZeroCouponBond and EuropeanOption inherited this base, encapsulating their specific Black-Scholes or intrinsic math. This allows the application to aggregate entirely different asset classes natively without complex if/else routing.\n\n\nIteration 2: Session State and Dynamic Construction\nA structuring desk needs to build multi-leg positions (e.g., Iron Condors, Strangles). * Using Streamlit‚Äôs st.session_state, I engineered a persistent memory ledger. Users can continuously inject new Instrument objects into their portfolio. * The backend loops through the ledger in real-time, dynamically aggregating the individual payoff profiles into a net portfolio matrix, plotting the exact PnL horizon instantly.\n\n\nIteration 3: 3D Volatility Surface Visualization\nTraders rely on visual heuristics to identify mispricings across the term structure. * I integrated plotly.graph_objects to render a high-performance WebGL 3D Volatility Surface. * Fuzzy Matching: To allow traders to inspect specific maturity slices, I implemented a 2D Smile extractor. Because floating-point math often causes mismatch errors (e.g., \\(T=0.1000001\\)), I utilized np.isclose to fuzzy-match the slider input to the underlying DataFrame, ensuring the 2D chart rendered perfectly every time."
  },
  {
    "objectID": "projects/quant-structuring-workbench/index.html#conclusion",
    "href": "projects/quant-structuring-workbench/index.html#conclusion",
    "title": "End-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production",
    "section": "Conclusion",
    "text": "Conclusion\nThis workbench stands as a complete, end-to-end demonstration of the quantitative lifecycle. It proves the ability to not only write complex Fourier transformations and navigate local optimization minima but to immediately pivot into software engineering, encapsulating that research into a live, Object-Oriented application ready for a global markets floor."
  },
  {
    "objectID": "readings/East of Eden/template.html#summary",
    "href": "readings/East of Eden/template.html#summary",
    "title": "East of Eden",
    "section": "Summary",
    "text": "Summary\nA multi-generational saga retelling the biblical story of Cain and Abel‚Ä¶"
  },
  {
    "objectID": "readings/East of Eden/template.html#key-takeaways",
    "href": "readings/East of Eden/template.html#key-takeaways",
    "title": "East of Eden",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nTimshel: The Hebrew word meaning ‚ÄúThou Mayest.‚Äù It signifies that humans have the choice to overcome sin; we are not condemned by our nature.\nInheritance: We are not bound by the sins of our fathers."
  },
  {
    "objectID": "readings/East of Eden/template.html#favorite-quotes",
    "href": "readings/East of Eden/template.html#favorite-quotes",
    "title": "East of Eden",
    "section": "Favorite Quotes",
    "text": "Favorite Quotes\n\n‚ÄúAnd now that you don‚Äôt have to be perfect, you can be good.‚Äù"
  },
  {
    "objectID": "readings/East of Eden/template.html#reflection",
    "href": "readings/East of Eden/template.html#reflection",
    "title": "East of Eden",
    "section": "Reflection",
    "text": "Reflection\nThis book shifted my perspective on agency. In a deterministic world (or in a quantitative model), we assume inputs dictate outputs. Steinbeck argues for the ‚Äúerror term‚Äù‚Äîthe human capacity to choose against the grain.c"
  }
]