[
  {
    "objectID": "readings/index.html",
    "href": "readings/index.html",
    "title": "Readings & Reflections",
    "section": "",
    "text": "Order By\n      Default\n      \n        Subtitle\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEast of Eden\n\n\n★★★★★\n\n\n\n\n\n\nJohn Steinbeck\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/sg-insurance-model/index.html#act-1-the-deterministic-baseline",
    "href": "projects/sg-insurance-model/index.html#act-1-the-deterministic-baseline",
    "title": "Singapore Insurance Strategy Dashboard",
    "section": "Act 1: The Deterministic Baseline",
    "text": "Act 1: The Deterministic Baseline\nThe first layer builds the standard financial projection.\n- Liquidity Analysis: Comparing the Surrender Value of WL against the liquid portfolio of BTID.\n- Legacy Visualization: Tracking the Death Benefit sum assured over time.\n- The ‘Strategy Frontier’: A dynamic calculation that pinpoints the exact age where BTID overtakes WL based on user-defined investment returns (e.g., 4% vs 7%)."
  },
  {
    "objectID": "projects/sg-insurance-model/index.html#act-2-behavioral-stress-testing",
    "href": "projects/sg-insurance-model/index.html#act-2-behavioral-stress-testing",
    "title": "Singapore Insurance Strategy Dashboard",
    "section": "Act 2: Behavioral Stress Testing",
    "text": "Act 2: Behavioral Stress Testing\nData shows that most retail investors do not reinvest their savings perfectly. I introduced a “Discipline Coefficient”, a slider allowing users to see what happens if they only invest 50%, 70%, or 90% of their premium savings.\n\nKey Insight: Through this feature, I discovered that if an investor’s discipline drops below ~60%, Whole Life plans often outperform BTID due to their “forced savings” nature, despite lower internal rates of return."
  },
  {
    "objectID": "projects/sg-insurance-model/index.html#act-3-stochastic-reality-monte-carlo",
    "href": "projects/sg-insurance-model/index.html#act-3-stochastic-reality-monte-carlo",
    "title": "Singapore Insurance Strategy Dashboard",
    "section": "Act 3: Stochastic Reality (Monte Carlo)",
    "text": "Act 3: Stochastic Reality (Monte Carlo)\nLife isn’t linear. Using Monte Carlo simulations, I modeled:\n- Actuarial Risk: Probabilities of Death and Total Permanent Disability (TPD) using Singapore’s S0408 tables.\n- Sequence of Returns Risk: Instead of a flat 7% return, the model simulates market volatility to show the range of possible outcomes for the BTID portfolio."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_09_Model_Comparison.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_09_Model_Comparison.html",
    "title": "9. Model Comparison",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bs_call_price, merton_jump_call, heston_call_price, bates_call_price, implied_volatility\n\n# 2. Load Data \nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Real Market IVs for the scatter plot\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs) * 100 # Convert to percentage\n\nprint(f\"✅ Market Data Loaded: {len(valid_strikes)} strikes ready for comparison.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Market Data Loaded: 352 strikes ready for comparison.\n\n\n\n\nCode\nprint(\"Loading Calibrated Parameters...\")\n\n# 1. Black-Scholes (From Notebook 04)\nbs_sigma = 0.1245  \n\n# 2. Merton Jump Diffusion (From Notebook 05)\nmerton_params = {\n    'sigma': 0.0850, \n    'lam': 0.9923, \n    'mu_j': -0.0825, \n    'delta': 0.0779\n}\n\n# 3. Heston Stochastic Volatility (From Notebook 06)\nheston_params = {\n    'v0': 0.0115, \n    'kappa': 3.0293, \n    'theta': 0.0684, \n    'xi': 1.1963, \n    'rho': -0.6572\n}\n\n# 4. Bates SVJ Model (The Ultimate Combination)\n# 4. Bates SVJ Model (The Ultimate Combination)\nbates_params = {\n    'v0': 0.0135, \n    'kappa': 3.0290, \n    'theta': 0.0530, \n    'xi': 1.1920, \n    'rho': -0.6481,\n    'lam': 0.9871,\n    'mu_j': 0.0,\n    'delta': 0.0187\n}\n\nprint(\"✅ Parameters successfully loaded into memory.\")\n\n\nLoading Calibrated Parameters...\n✅ Parameters successfully loaded into memory."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_09_Model_Comparison.html#setup-importing-and-defining-the-state",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_09_Model_Comparison.html#setup-importing-and-defining-the-state",
    "title": "9. Model Comparison",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bs_call_price, merton_jump_call, heston_call_price, bates_call_price, implied_volatility\n\n# 2. Load Data \nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Real Market IVs for the scatter plot\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs) * 100 # Convert to percentage\n\nprint(f\"✅ Market Data Loaded: {len(valid_strikes)} strikes ready for comparison.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Market Data Loaded: 352 strikes ready for comparison.\n\n\n\n\nCode\nprint(\"Loading Calibrated Parameters...\")\n\n# 1. Black-Scholes (From Notebook 04)\nbs_sigma = 0.1245  \n\n# 2. Merton Jump Diffusion (From Notebook 05)\nmerton_params = {\n    'sigma': 0.0850, \n    'lam': 0.9923, \n    'mu_j': -0.0825, \n    'delta': 0.0779\n}\n\n# 3. Heston Stochastic Volatility (From Notebook 06)\nheston_params = {\n    'v0': 0.0115, \n    'kappa': 3.0293, \n    'theta': 0.0684, \n    'xi': 1.1963, \n    'rho': -0.6572\n}\n\n# 4. Bates SVJ Model (The Ultimate Combination)\n# 4. Bates SVJ Model (The Ultimate Combination)\nbates_params = {\n    'v0': 0.0135, \n    'kappa': 3.0290, \n    'theta': 0.0530, \n    'xi': 1.1920, \n    'rho': -0.6481,\n    'lam': 0.9871,\n    'mu_j': 0.0,\n    'delta': 0.0187\n}\n\nprint(\"✅ Parameters successfully loaded into memory.\")\n\n\nLoading Calibrated Parameters...\n✅ Parameters successfully loaded into memory."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_09_Model_Comparison.html#calibration-of-the-bates-model",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_09_Model_Comparison.html#calibration-of-the-bates-model",
    "title": "9. Model Comparison",
    "section": "2 Calibration of the Bates Model",
    "text": "2 Calibration of the Bates Model\n\n\nCode\ndef bates_objective(params):\n    v0, kappa, theta, xi, rho, lam, mu_j, delta = params\n    error = 0.0\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for i, K in enumerate(valid_strikes):\n            m_price = bates_call_price(S0, K, T, r, q, v0, kappa, theta, xi, rho, lam, mu_j, delta)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs[i])**2\n                \n    return error / len(valid_strikes)\n\n# Parameter Seeding: [v0, kappa, theta, xi, rho, lam, mu_j, delta]\n# We start exactly where Heston and Merton left off!\nbates_guess = [0.0115, 3.0293, 0.0684, 1.1963, -0.6572, 0.9923, -0.0825, 0.0779]\n\nbates_bounds = [\n    (0.005, 0.15), (0.5, 5.0), (0.005, 0.15), (0.05, 1.5), (-0.95, -0.2), # Heston bounds\n    (0.0, 3.0), (-0.5, 0.0), (0.01, 0.3)                                  # Merton bounds\n]\n\nprint(\"Optimizing Bates Parameters (Seeded Fast Search)...\")\nstart_time = time.time()\nres_bates = minimize(bates_objective, bates_guess, method='L-BFGS-B', bounds=bates_bounds)\n\nprint(f\"✅ Finished in {round(time.time() - start_time, 2)}s\")\nprint(f\"Optimal Parameters: {res_bates.x}\")\nprint(f\"Mean Squared Error: {res_bates.fun:.6f}\")\n\n\nOptimizing Bates Parameters (Seeded Fast Search)...\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[8], line 26\n     24 print(\"Optimizing Bates Parameters (Seeded Fast Search)...\")\n     25 start_time = time.time()\n---&gt; 26 res_bates = minimize(bates_objective, bates_guess, method='L-BFGS-B', bounds=bates_bounds)\n     28 print(f\"✅ Finished in {round(time.time() - start_time, 2)}s\")\n     29 print(f\"Optimal Parameters: {res_bates.x}\")\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_minimize.py:738, in minimize(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\n    735     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n    736                              **options)\n    737 elif meth == 'l-bfgs-b':\n--&gt; 738     res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n    739                            callback=callback, **options)\n    740 elif meth == 'tnc':\n    741     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n    742                         **options)\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:386, in _minimize_lbfgsb(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\n    383     x0 = np.clip(x0, bounds[0], bounds[1])\n    385 # _prepare_scalar_function can use bounds=None to represent no bounds\n--&gt; 386 sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n    387                               bounds=bounds,\n    388                               finite_diff_rel_step=finite_diff_rel_step)\n    390 func_and_grad = sf.fun_and_grad\n    392 nbd = zeros(n, np.int32)\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_optimize.py:291, in _prepare_scalar_function(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\n    287     bounds = (-np.inf, np.inf)\n    289 # ScalarFunction caches. Reuse of fun(x) during grad\n    290 # calculation reduces overall function evaluations.\n--&gt; 291 sf = ScalarFunction(fun, x0, args, grad, hess,\n    292                     finite_diff_rel_step, bounds, epsilon=epsilon)\n    294 return sf\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:232, in ScalarFunction.__init__(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\n    225 # Initial gradient evaluation\n    226 self._wrapped_grad, self._ngev = _wrapper_grad(\n    227     grad,\n    228     fun=self._wrapped_fun,\n    229     args=args,\n    230     finite_diff_options=finite_diff_options\n    231 )\n--&gt; 232 self._update_grad()\n    234 # Hessian evaluation\n    235 if callable(hess):\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:307, in ScalarFunction._update_grad(self)\n    305 if self._orig_grad in FD_METHODS:\n    306     self._update_fun()\n--&gt; 307 self.g = self._wrapped_grad(self.x, f0=self.f)\n    308 self.g_updated = True\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:48, in _wrapper_grad.&lt;locals&gt;.wrapped1(x, f0)\n     46 def wrapped1(x, f0=None):\n     47     ncalls[0] += 1\n---&gt; 48     return approx_derivative(\n     49         fun, x, f0=f0, **finite_diff_options\n     50     )\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:523, in approx_derivative(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\n    520     use_one_sided = False\n    522 if sparsity is None:\n--&gt; 523     return _dense_difference(fun_wrapped, x0, f0, h,\n    524                              use_one_sided, method)\n    525 else:\n    526     if not issparse(sparsity) and len(sparsity) == 2:\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:596, in _dense_difference(fun, x0, f0, h, use_one_sided, method)\n    594     x1[i] += h[i]\n    595     dx = x1[i] - x0[i]  # Recompute dx as exactly representable number.\n--&gt; 596     df = fun(x1) - f0\n    597 elif method == '3-point' and use_one_sided[i]:\n    598     x1[i] += h[i]\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:474, in approx_derivative.&lt;locals&gt;.fun_wrapped(x)\n    471 if xp.isdtype(x.dtype, \"real floating\"):\n    472     x = xp.astype(x, x0.dtype)\n--&gt; 474 f = np.atleast_1d(fun(x, *args, **kwargs))\n    475 if f.ndim &gt; 1:\n    476     raise RuntimeError(\"`fun` return value has \"\n    477                        \"more than 1 dimension.\")\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:21, in _wrapper_fun.&lt;locals&gt;.wrapped(x)\n     17 ncalls[0] += 1\n     18 # Send a copy because the user may overwrite it.\n     19 # Overwriting results in undefined behaviour because\n     20 # fun(self.x) will change self.x, with the two no longer linked.\n---&gt; 21 fx = fun(np.copy(x), *args)\n     22 # Make sure the function returns a true scalar\n     23 if not np.isscalar(fx):\n\nCell In[8], line 8, in bates_objective(params)\n      6 for i, K in enumerate(valid_strikes):\n      7     m_price = bates_call_price(S0, K, T, r, q, v0, kappa, theta, xi, rho, lam, mu_j, delta)\n----&gt; 8     m_iv = implied_volatility(m_price, S0, K, T, r, q)\n     10     if np.isnan(m_iv): error += 5.0\n     11     else: error += (m_iv - target_ivs[i])**2\n\nFile C:\\Github Code\\quant-structuring-workbench\\quant_math_engine.py:25, in implied_volatility(target_price, S, K, T, r, q)\n     22     return bs_call_price(sigma, S, K, T, r, q) - target_price\n     24 try:\n---&gt; 25     return brentq(objective, 1e-4, 5.0) \n     26 except ValueError:\n     27     return np.nan\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_zeros_py.py:798, in brentq(f, a, b, args, xtol, rtol, maxiter, full_output, disp)\n    796     raise ValueError(f\"rtol too small ({rtol:g} &lt; {_rtol:g})\")\n    797 f = _wrap_nan_raise(f)\n--&gt; 798 r = _zeros._brentq(f, a, b, xtol, rtol, maxiter, args, full_output, disp)\n    799 return results_c(full_output, r, \"brentq\")\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_zeros_py.py:94, in _wrap_nan_raise.&lt;locals&gt;.f_raise(x, *args)\n     93 def f_raise(x, *args):\n---&gt; 94     fx = f(x, *args)\n     95     f_raise._function_calls += 1\n     96     if np.isnan(fx):\n\nFile C:\\Github Code\\quant-structuring-workbench\\quant_math_engine.py:22, in implied_volatility.&lt;locals&gt;.objective(sigma)\n     21 def objective(sigma):\n---&gt; 22     return bs_call_price(sigma, S, K, T, r, q) - target_price\n\nFile C:\\Github Code\\quant-structuring-workbench\\quant_math_engine.py:14, in bs_call_price(sigma, S, K, T, r, q)\n     12 d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n     13 d2 = d1 - sigma * np.sqrt(T)\n---&gt; 14 return S * np.exp(-q * T) * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:2135, in rv_continuous.cdf(self, x, *args, **kwds)\n   2133 cond = cond0 & cond1\n   2134 output = zeros(shape(cond), dtyp)\n-&gt; 2135 place(output, (1-cond0)+np.isnan(x), self.badvalue)\n   2136 place(output, cond2, 1.0)\n   2137 if np.any(cond):  # call only if at least 1 entry\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2085, in place(arr, mask, vals)\n   2047 @array_function_dispatch(_place_dispatcher)\n   2048 def place(arr, mask, vals):\n   2049     \"\"\"\n   2050     Change elements of an array based on conditional and input values.\n   2051 \n   (...)\n   2083 \n   2084     \"\"\"\n-&gt; 2085     return _place(arr, mask, vals)\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_09_Model_Comparison.html#visualising-the-model-against-live-data",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_09_Model_Comparison.html#visualising-the-model-against-live-data",
    "title": "9. Model Comparison",
    "section": "3 Visualising the Model against Live Data",
    "text": "3 Visualising the Model against Live Data\n\n\nCode\nprint(\"Generating Bates Volatility Smile...\")\n\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nbates_prices = [bates_call_price(S0, k, T, r, q, *res_bates.x) for k in smooth_strikes]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    bates_iv = [implied_volatility(p, S0, k, T, r, q) for p, k in zip(bates_prices, smooth_strikes)]\n\nvalid_idx = ~np.isnan(bates_iv)\nclean_strikes = np.array(smooth_strikes)[valid_idx]\nclean_iv = np.array(bates_iv)[valid_idx] * 100\n\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\nplt.plot(clean_strikes, clean_iv, color='green', label='Bates (SVJ) Fit', linewidth=2.5)\n\nplt.title(f\"Bates (SVJ) Model Calibration\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nGenerating Bates Volatility Smile...\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"Generating Theoretical Option Prices and Implied Volatilities...\")\n\n# Create a perfectly smooth x-axis for drawing the lines\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 60)\n\n# Arrays to hold the y-axis values\nbs_iv_line = np.full_like(smooth_strikes, bs_sigma) * 100\nmerton_ivs, heston_ivs, bates_ivs = [], [], []\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for K in smooth_strikes:\n        # Merton Math\n        p_merton = merton_jump_call(S0, K, T, r, q, **merton_params)\n        merton_ivs.append(implied_volatility(p_merton, S0, K, T, r, q) * 100)\n        \n        # Heston Math\n        p_heston = heston_call_price(S0, K, T, r, q, **heston_params)\n        heston_ivs.append(implied_volatility(p_heston, S0, K, T, r, q) * 100)\n        \n        # Bates Math\n        p_bates = bates_call_price(S0, K, T, r, q, **bates_params)\n        bates_ivs.append(implied_volatility(p_bates, S0, K, T, r, q) * 100)\n\nprint(\"✅ Math complete. Rendering Master Plot...\")\n\n# --- THE MASTER PLOT ---\nplt.figure(figsize=(13, 7))\n\n# 1. The Real Market Reality\nplt.scatter(valid_strikes, target_ivs, color='black', label='Raw Market IV (WRDS)', marker='x', alpha=0.7, s=40)\n\n# 2. The Theoretical Models\nplt.plot(smooth_strikes, bs_iv_line, color='gray', label='Black-Scholes (Constant Vol)', linewidth=2, linestyle=':')\nplt.plot(smooth_strikes, merton_ivs, color='blue', label='Merton (Jump Diffusion)', linewidth=2.5, linestyle='--')\nplt.plot(smooth_strikes, heston_ivs, color='red', label='Heston (Stochastic Vol)', linewidth=2.5)\nplt.plot(smooth_strikes, bates_ivs, color='green', label='Bates (SVJ - The Holy Grail)', linewidth=4, alpha=0.8)\n\n# Formatting\nplt.axvline(S0, color='black', linestyle='-.', alpha=0.3, label=f'Spot Price (S0 = {S0})')\nplt.title(f\"Quantitative Model Comparison: Fitting the S&P 500 Volatility Skew\\nTarget Expiry: {TARGET_EXDATE} (Maturity: {T:.2f} Years)\", fontsize=16)\nplt.xlabel(\"Strike Price\", fontsize=12)\nplt.ylabel(\"Implied Volatility (%)\", fontsize=12)\nplt.legend(fontsize=12, loc='upper right')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\nGenerating Theoretical Option Prices and Implied Volatilities...\n✅ Math complete. Rendering Master Plot..."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_07_Merton_RealDataCalibration.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_07_Merton_RealDataCalibration.html",
    "title": "7. Merton Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import merton_jump_call, implied_volatility # Change this import for Heston/Bates\n\n# 2. Load the data\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate target IVs for the optimizer\ntarget_ivs = []\nvalid_strikes = []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"✅ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Ready! Targeting 352 liquid strikes."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_07_Merton_RealDataCalibration.html#setup-importing-and-defining-the-state",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_07_Merton_RealDataCalibration.html#setup-importing-and-defining-the-state",
    "title": "7. Merton Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import merton_jump_call, implied_volatility # Change this import for Heston/Bates\n\n# 2. Load the data\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate target IVs for the optimizer\ntarget_ivs = []\nvalid_strikes = []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"✅ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Ready! Targeting 352 liquid strikes."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_07_Merton_RealDataCalibration.html#calibration-of-the-merton-jump-model",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_07_Merton_RealDataCalibration.html#calibration-of-the-merton-jump-model",
    "title": "7. Merton Model Real Data",
    "section": "2 Calibration of the Merton-Jump Model",
    "text": "2 Calibration of the Merton-Jump Model\n\n\nCode\n# --- MERTON OBJECTIVE FUNCTION ---\ndef merton_objective(params):\n    sigma, lam, mu_j, delta = params\n    error = 0.0\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for i, K in enumerate(valid_strikes):\n            # Notice how clean this is because the math is hidden in your engine!\n            m_price = merton_jump_call(S0, K, T, r, q, sigma, lam, mu_j, delta)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs[i])**2\n                \n    return error / len(valid_strikes)\n\n# --- RUN OPTIMIZER ---\nmerton_guess = [0.15, 1.0, -0.15, 0.10] \nmerton_bounds = [(0.05, 0.30), (0.1, 5.0), (-0.5, 0.0), (0.01, 0.30)]\n\nprint(\"Optimizing Merton Parameters...\")\nstart_time = time.time()\nres_merton = minimize(merton_objective, merton_guess, method='L-BFGS-B', bounds=merton_bounds)\n\nprint(f\"✅ Finished in {round(time.time() - start_time, 2)}s\")\nprint(f\"Parameters: {res_merton.x}\")\n\n\nOptimizing Merton Parameters...\n✅ Finished in 62.25s\nParameters: [ 0.08582077  0.99228655 -0.0824808   0.07785328]"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_07_Merton_RealDataCalibration.html#visualising-the-model-against-live-data",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_07_Merton_RealDataCalibration.html#visualising-the-model-against-live-data",
    "title": "7. Merton Model Real Data",
    "section": "3 Visualising the Model against Live Data",
    "text": "3 Visualising the Model against Live Data\n\n\nCode\nprint(\"Generating Volatility Smile...\")\n\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nmerton_prices = [merton_jump_call(S0, k, T, r, q, *res_merton.x) for k in smooth_strikes]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    merton_iv = [implied_volatility(p, S0, k, T, r, q) for p, k in zip(merton_prices, smooth_strikes)]\n\nvalid_idx = ~np.isnan(merton_iv)\nclean_strikes = np.array(smooth_strikes)[valid_idx]\nclean_iv = np.array(merton_iv)[valid_idx] * 100\n\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\nplt.plot(clean_strikes, clean_iv, color='blue', label='Merton Fit', linewidth=2.5)\n\nplt.title(f\"Merton Jump Diffusion Calibration\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nGenerating Volatility Smile..."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_05_BS_RealDataCalibration.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_05_BS_RealDataCalibration.html",
    "title": "5. Black-Scholes Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bs_call_price, implied_volatility\n\n# 2. Load Data \nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Market IVs\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"✅ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Ready! Targeting 352 liquid strikes.\n\n\n\n\nCode\n# --- BLACK-SCHOLES OBJECTIVE FUNCTION ---\ndef bs_objective(params):\n    sigma = params[0]\n    error = 0.0\n    \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for i, K in enumerate(valid_strikes):\n            # Price the option using a CONSTANT volatility\n            m_price = bs_call_price(sigma, S0, K, T, r, q)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs[i])**2\n                \n    return error / len(valid_strikes)\n\n# Guess 15% volatility\nbs_guess = [0.15] \nbs_bounds = [(0.01, 1.0)]\n\nprint(\"Optimizing Constant Black-Scholes Volatility...\")\nres_bs = minimize(bs_objective, bs_guess, method='L-BFGS-B', bounds=bs_bounds)\n\nbest_sigma = res_bs.x[0]\n\nprint(f\"✅ Finished!\")\nprint(f\"Optimal Constant Volatility: {best_sigma * 100:.2f}%\")\nprint(f\"Mean Squared Error: {res_bs.fun:.6f}\")\n\n\nOptimizing Constant Black-Scholes Volatility...\n✅ Finished!\nOptimal Constant Volatility: 13.94%\nMean Squared Error: 0.001505\n\n\n\n\nCode\nprint(\"Generating Black-Scholes Flat Line vs Market Smile...\")\n\n# Black-Scholes assumes IV is exactly the same across every single strike\nclean_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nbs_iv_line = np.full_like(clean_strikes, best_sigma) * 100\n\nplt.figure(figsize=(10, 6))\n\n# Plot Market Data\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\n\n# Plot Black Scholes\nplt.plot(clean_strikes, bs_iv_line, color='blue', label=f'Black-Scholes (Flat {best_sigma*100:.1f}%)', linewidth=2.5)\n\nplt.axvline(S0, color='gray', linestyle=':', label=f'Spot Price (S0 = {S0})')\nplt.title(f\"The Reality of the Market: Why Black-Scholes Fails\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nGenerating Black-Scholes Flat Line vs Market Smile..."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_03_HestonModel.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_03_HestonModel.html",
    "title": "3. Heston Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n2. Merton Jump: Adds “jumps” to the asset price to model market shocks.\nCaptures “Fat Tails” and sudden crashes via Poisson jumps.\n\n3. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the “Smirk” or “Skew” via stochastic vol—essential for pricing OTM puts accurately."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_03_HestonModel.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_03_HestonModel.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "title": "3. Heston Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n2. Merton Jump: Adds “jumps” to the asset price to model market shocks.\nCaptures “Fat Tails” and sudden crashes via Poisson jumps.\n\n3. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the “Smirk” or “Skew” via stochastic vol—essential for pricing OTM puts accurately."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_03_HestonModel.html#objective-capturing-the-leverage-effect-and-volatility-clustering",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_03_HestonModel.html#objective-capturing-the-leverage-effect-and-volatility-clustering",
    "title": "3. Heston Model",
    "section": "2.1 Objective: Capturing the “Leverage Effect” and Volatility Clustering",
    "text": "2.1 Objective: Capturing the “Leverage Effect” and Volatility Clustering\nThe Black-Scholes model assumes volatility is a constant number. However, looking at the VIX, we know that volatility is highly dynamic. It clusters (high vol periods follow high vol periods) and it mean-reverts (eventually calms down).\nSteven Heston (1993) solved this by modeling the asset price and its variance as two correlated, random processes:\n\n2.1.1 The Mathematical Intuition\nThe Heston model is governed by two Stochastic Differential Equations (SDEs):\n\nThe Asset Price Process: \\[\\frac{dS_t}{S_t} = \\mu dt + \\sqrt{v_t} dW_t^S\\]\nThe Variance Process (CIR Process): \\[dv_t = \\kappa(\\theta - v_t)dt + \\xi \\sqrt{v_t} dW_t^v\\]\n\nThe Parameters: * \\(v_0\\): Initial Variance. * \\(\\theta\\) (Theta): Long-term average variance. * \\(\\kappa\\) (Kappa): The rate of mean reversion (how fast vol returns to \\(\\theta\\)). * \\(\\xi\\) (Xi): Volatility of Volatility (determines the convexity of the smile). * \\(\\rho\\) (Rho): The Correlation between the two Brownian motions (\\(dW_t^S\\) and \\(dW_t^v\\)).\nThe Magic of \\(\\rho\\): In equity markets like the S&P 500, \\(\\rho\\) is heavily negative (around -0.7). When the market crashes, volatility explodes. This negative correlation is what mathematically drags the left side of the volatility smile upward, creating the “Skew.”\n\n\n2.1.2 Pricing via Characteristic Functions\nSince there is no simple closed-form solution like Black-Scholes, we use the Fourier Transform method. The price is an integral of the Characteristic Function \\(\\phi\\).\nImportant Note on Stability (Albrecher et al.):\nStandard implementations of the Heston characteristic function often suffer from “branch cut” discontinuities, leading to numerical explosions (sawtooth graphs). We use the “Albrecher” representation to ensure the characteristic function remains continuous and stable."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_03_HestonModel.html#imports-and-setup",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_03_HestonModel.html#imports-and-setup",
    "title": "3. Heston Model",
    "section": "2.2 Imports and Setup",
    "text": "2.2 Imports and Setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom scipy.integrate import quad\nimport seaborn as sns\n\n# Set plotting style\nsns.set_style(\"darkgrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Global Parameters (Toggles)\nS0 = 100.0    # Spot Price\nK_list = np.linspace(80, 120, 50) # Range of Strikes for plotting\nT = 1.0       # Time to Maturity (1 year)\nr = 0.05      # Risk-free rate\nq = 0.0       # Dividend yield\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Heston Simulation Parameters ---\nS0 = 100.0      # Initial Stock Price\nv0 = 0.04       # Initial Variance (20% Volatility)\nkappa = 3.0     # Speed of mean reversion\ntheta = 0.04    # Long-term variance\nxi = 0.6        # Volatility of Volatility\nrho = -0.7      # Negative correlation (Equity market characteristic)\nr = 0.05\nT = 1.0         # 1 Year\nsteps = 252     # Trading days\ndt = T / steps\nn_paths = 3     # Just a few paths to keep the visual clean\n\nnp.random.seed(42)\n\n# 1. Generate Correlated Brownian Motions\n# We use Cholesky decomposition to correlate the random numbers\nZ1 = np.random.standard_normal((steps, n_paths))\nZ2 = np.random.standard_normal((steps, n_paths))\nZ_S = Z1\nZ_v = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n\n# 2. Setup Path Arrays\nS = np.zeros((steps + 1, n_paths))\nv = np.zeros((steps + 1, n_paths))\nS[0] = S0\nv[0] = v0\n\n# 3. Euler-Maruyama Integration \nfor t in range(1, steps + 1):\n    # Variance Process (with Full Truncation to prevent negative variance)\n    v_prev = np.maximum(v[t-1], 0)\n    dv = kappa * (theta - v_prev) * dt + xi * np.sqrt(v_prev) * np.sqrt(dt) * Z_v[t-1]\n    v[t] = np.maximum(v_prev + dv, 0)\n    \n    # Asset Price Process\n    dS = r * S[t-1] * dt + np.sqrt(v_prev) * S[t-1] * np.sqrt(dt) * Z_S[t-1]\n    S[t] = S[t-1] + dS\n\n# --- Plotting the Correlated Paths ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n\n# Plot Asset Price\nax1.plot(S, linewidth=1.5)\nax1.set_title(rf\"Heston Asset Price Paths ($\\rho = {rho}$)\", fontsize=13)\nax1.set_ylabel(\"Asset Price ($S_t$)\")\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Plot Variance\nax2.plot(v, linewidth=1.5)\nax2.axhline(theta, color='black', linestyle='--', label=f'Long-Term Mean ($\\theta={theta}$)')\nax2.set_title(\"Heston Variance Paths\", fontsize=13)\nax2.set_xlabel(\"Trading Days\")\nax2.set_ylabel(\"Variance ($v_t$)\")\nax2.legend()\nax2.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport warnings\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# --- Define Base Parameters ---\nS0_t = 100.0\nT_t = 0.5   # 6 months\nr_t = 0.05\nq_t = 0.0\nstrikes = np.linspace(80, 120, 30)\n\nv0_t, kappa_t, theta_t, xi_t = 0.04, 2.0, 0.04, 0.5\n\n# We will test 3 different correlation environments\ncorrelations = {\n    \"Equities (Negative Skew)\": -0.8,\n    \"FX Markets (Symmetric Smile)\": 0.0,\n    \"Commodities (Positive Skew)\": 0.8\n}\n\nprint(\"Calculating Theoretical Smiles across different markets...\")\n\nplt.figure(figsize=(11, 6))\n\nfor label, rho_test in correlations.items():\n    ivs = []\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for K in strikes:\n            # Get Heston Price\n            price = heston_call_price(S0_t, K, T_t, r_t, q_t, v0_t, kappa_t, theta_t, xi_t, rho_test)\n            # Convert to IV\n            iv = implied_volatility(price, S0_t, K, T_t, r_t, q_t)\n            ivs.append(iv * 100)\n            \n    # Plot the specific market shape\n    plt.plot(strikes, ivs, linewidth=3, label=f\"{label} ($\\\\rho = {rho_test}$)\")\n\n# Formatting\nplt.axvline(S0_t, color='black', linestyle=':', label='Spot Price (ATM)')\nplt.title(\"The Magic of $\\\\rho$: How Heston fits different asset classes\", fontsize=15)\nplt.xlabel(\"Strike Price\", fontsize=12)\nplt.ylabel(\"Implied Volatility (%)\", fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nCalculating Theoretical Smiles across different markets..."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n\n2. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the “Smirk” or “Skew” via stochastic vol—essential for pricing OTM puts accurately.\n3. Merton Jump: Adds “jumps” to the asset price to model market shocks.\nCaptures “Fat Tails” and sudden crashes via Poisson jumps.\n\n\n\nThe Black-Scholes model assumes the stock price \\(S_t\\) follows a Geometric Brownian Motion (GBM):\\[dS_t = \\mu S_t dt + \\sigma S_t dW_t\\]\nWe can deconstruct this engine into two distinct components: * The Deterministic Drift (\\(\\mu S_t dt\\)): This represents the expected, constant growth rate of the asset. If market volatility were zero, the stock would simply grow smoothly at rate \\(\\mu\\), compounding like money in a standard savings account.\n\nThe Stochastic Diffusion (\\(\\sigma S_t dW_t\\)): This injects the randomness. \\(dW_t\\) is a Wiener process (Standard Brownian Motion) representing unpredictable market shocks. By scaling this randomness by the current stock price (\\(S_t\\)), the model ensures that the asset price can never drop below zero, perfectly reflecting the reality of limited liability in equities. Where:\n\n\\(\\mu\\) is the drift.\n\n\\(\\sigma\\) is the constant volatility.\n\n\\(dW_t\\) is a Wiener process (Brownian motion). \n\nCall Option Formula\nThe price of a European Call option \\(C(S, t)\\) is given by:\n\\[C(S, t) = S_0 e^{-qT} N(d_1) - K e^{-rT} N(d_2)\\]  Where \\(N(\\cdot)\\) is the cumulative distribution function of the standard normal distribution, and:\n\\[d_1 = \\frac{\\ln(S_0/K) + (r - q + \\sigma^2/2)T}{\\sigma\\sqrt{T}}\\]\\[d_2 = d_1 - \\sigma\\sqrt{T}\\]\n\n\nVolatility Drag (The Asymmetry of Returns):\nBecause asset returns compound geometrically, downward price movements penalize the overall value more severely than upward movements of the exact same percentage. A 50% drop requires a 100% gain just to recover the initial capital.\nMathematically, this “volatility drag” manifests when we apply Ito’s Lemma to the log-return process, introducing a \\(-\\frac{1}{2}\\sigma^2\\) variance penalty term. The higher the volatility, the stronger this downward drag on the expected geometric return.\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom scipy.integrate import quad\nimport seaborn as sns\n\n# Set plotting style\nsns.set_style(\"darkgrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Global Parameters (Toggles)\nS0 = 100.0    # Spot Price\nK_list = np.linspace(80, 120, 50) # Range of Strikes for plotting\nT = 1.0       # Time to Maturity (1 year)\nr = 0.05      # Risk-free rate\nq = 0.0       # Dividend yield\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\nLoaded 352 strikes. S0: 4783.45, r: 0.0537\n\n\n\n\nCode\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# Load all massive Parquet files into this notebook's RAM\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\n\n\n\nCode\n# 1. Explicitly define your scenario variables so the plotter can see them later\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\n\n# 2. Feed them to the loader to get the exact state\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\n# 3. Extract the variables for the optimizer\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\nprint(f\"✅ Loaded {len(market_strikes)} strikes. S0: {S0}, r: {r*100:.2f}%\")\n\n\n✅ Loaded 352 strikes. S0: 4783.45, r: 5.37%\n\n\n\n\n\nVisualizes the core assumption of the Black-Scholes model: that asset prices follow a log-normal random walk with constant drift and volatility. This Monte Carlo simulation provides a physical intuition for the probability distribution of terminal stock prices.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# --- 1. Simulate Geometric Brownian Motion (GBM) ---\ndef simulate_gbm(S0, mu, sigma, T, dt, num_paths):\n    \"\"\"\n    Simulates asset price paths using Geometric Brownian Motion.\n    \"\"\"\n    N = int(T / dt) # Number of time steps\n    t = np.linspace(0, T, N)\n    \n    # Generate random Wiener process steps\n    W = np.random.standard_normal(size=(num_paths, N)) \n    W = np.cumsum(W, axis=1) * np.sqrt(dt) \n    \n    # Calculate the paths using the analytical solution to the GBM SDE\n    # Notice the volatility drag term: (mu - 0.5 * sigma^2)\n    X = (mu - 0.5 * sigma**2) * t + sigma * W \n    S = S0 * np.exp(X) \n    return t, S\n\n# Parameters for simulation\nS0_sim = 100      # Initial stock price\nmu_sim = 0.08     # Expected return (drift)\nsigma_sim = 0.20  # Volatility\nT_sim = 1.0       # Time horizon (1 year)\ndt_sim = 1/252    # Daily steps\nnum_paths = 15    # Number of simulated paths to plot\n\nt_steps, paths = simulate_gbm(S0_sim, mu_sim, sigma_sim, T_sim, dt_sim, num_paths)\n\nplt.figure(figsize=(10, 6))\nfor i in range(num_paths):\n    plt.plot(t_steps, paths[i, :], lw=1.5, alpha=0.7)\n    \nplt.title('Monte Carlo Simulation of Asset Paths (GBM)')\nplt.xlabel('Time (Years)')\nplt.ylabel('Asset Price ($S_t$)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nImplements the closed-form analytical solutions for European Call options. We visualize Delta against the underlying asset price to demonstrate the dynamic, non-linear nature of option sensitivities, introducing the concept of continuous delta-hedging.\nMapping of the “Big Five” Greeks (\\(\\Delta\\), \\(\\Gamma\\), \\(\\Theta\\), \\(\\nu\\), \\(\\rho\\)). By visualizing these simultaneously, we can analyze the multi-dimensional risk exposure of an option contract as the underlying spot price moves across different moneyness levels.\n\n\nCode\n\ndef d1(S, K, T, r, q, sigma):\n    return (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n\ndef d2(S, K, T, r, q, sigma):\n    return d1(S, K, T, r, q, sigma) - sigma * np.sqrt(T)\n\ndef bs_call_price(S, K, T, r, q, sigma):\n    if sigma &lt;= 0 or T &lt;= 0: return max(S * np.exp(-q * T) - K * np.exp(-r * T), 0)\n    return (S * np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma)) - \n            K * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma)))\n\ndef bs_call_delta(S, K, T, r, q, sigma):\n    \"\"\"Rate of change of option price with respect to underlying asset.\"\"\"\n    return np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma))\n\ndef bs_gamma(S, K, T, r, q, sigma):\n    \"\"\"Rate of change of Delta (convexity). Identical for calls and puts.\"\"\"\n    return (np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma))) / (S * sigma * np.sqrt(T))\n\ndef bs_vega(S, K, T, r, q, sigma):\n    \"\"\"Sensitivity to volatility. Identical for calls and puts.\"\"\"\n    return S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * np.sqrt(T)\n\ndef bs_call_theta(S, K, T, r, q, sigma):\n    \"\"\"Time decay of the option.\"\"\"\n    term1 = -(S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * sigma) / (2 * np.sqrt(T))\n    term2 = r * K * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma))\n    term3 = q * S * np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma))\n    return term1 - term2 + term3\n\n# --- Add Call Rho ---\ndef bs_call_rho(S, K, T, r, q, sigma):\n    \"\"\"Sensitivity to the risk-free interest rate.\"\"\"\n    return K * T * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma))\n\n# --- Plotting the Big Five Greeks ---\nS_range = np.linspace(50, 150, 100)\nK_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed = 100, 1.0, 0.05, 0.0, 0.2\n\n# Calculate Greeks\ndeltas = [bs_call_delta(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\ngammas = [bs_gamma(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nthetas = [bs_call_theta(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nvegas  = [bs_vega(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nrhos   = [bs_call_rho(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\n\n# Create subplots\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\nfig.suptitle('Call Option Greeks vs. Underlying Price ($S_t$)', fontsize=16)\n\ngreeks_data = [\n    ('Delta ($\\Delta$)', deltas, 'purple'), ('Gamma ($\\Gamma$)', gammas, 'blue'), \n    ('Theta ($\\Theta$)', thetas, 'red'), ('Vega ($\\\\nu$)', vegas, 'green'), \n    ('Rho ($\\\\rho$)', rhos, 'orange')\n]\n\nfor i, (title, data, color) in enumerate(greeks_data):\n    ax = axes[i//3, i%3]\n    ax.plot(S_range, data, color=color, lw=2)\n    ax.axvline(x=K_fixed, color='black', linestyle='--', label='Strike (K=100)')\n    ax.set_title(title)\n    ax.set_xlabel('Underlying Price')\n    ax.grid(True, alpha=0.3)\n\naxes[1, 2].axis('off') # Hide the empty 6th subplot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCompletes the basic pricing suite by adding European Put options. We run a computational check to validate our formulas against the Put-Call Parity theorem (\\(C - P = S_0 e^{-qT} - K e^{-rT}\\)), ensuring our engine respects fundamental no-arbitrage constraints.\n\n\nCode\n# --- Put Option Pricing and Greeks ---\ndef bs_put_price(S, K, T, r, q, sigma):\n    if sigma &lt;= 0 or T &lt;= 0: return max(K * np.exp(-r * T) - S * np.exp(-q * T), 0)\n    return (K * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma)) - \n            S * np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma)))\n\ndef bs_put_delta(S, K, T, r, q, sigma):\n    return -np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma))\n\ndef bs_put_theta(S, K, T, r, q, sigma):\n    term1 = -(S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * sigma) / (2 * np.sqrt(T))\n    term2 = r * K * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma))\n    term3 = q * S * np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma))\n    return term1 + term2 - term3\n\ndef bs_put_rho(S, K, T, r, q, sigma):\n    return -K * T * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma))\n\n# --- Put-Call Parity Validation ---\nC = bs_call_price(S0_sim, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed)\nP = bs_put_price(S0_sim, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed)\n\nlhs = C - P\nrhs = S0_sim * np.exp(-q_fixed * T_fixed) - K_fixed * np.exp(-r_fixed * T_fixed)\n\nprint(f\"Put-Call Parity Check:\")\nprint(f\"LHS (Call - Put): {lhs:.4f}\")\nprint(f\"RHS (Discounted S - Discounted K): {rhs:.4f}\")\nprint(f\"Difference: {abs(lhs - rhs):.1e} (Effectively Zero)\")\n\n\nPut-Call Parity Check:\nLHS (Call - Put): 4.8771\nRHS (Discounted S - Discounted K): 4.8771\nDifference: 0.0e+00 (Effectively Zero)\n\n\n\n\n\nOptions pricing is inherently multi-dimensional. The price is not just a function of the underlying asset’s price, but also its strike price and the time remaining until expiration.\nBy calculating the Black-Scholes price across a grid of different strikes and maturities, we can generate a 3D pricing surface. This visualizes how Out-Of-The-Money (OTM) options lose value rapidly as expiration approaches, while In-The-Money (ITM) options converge exactly to their intrinsic payoff.\nShifts from 2D static plots to an interactive 3D surface mapping the Call price against both Strike and Time to Maturity. This allows for a deeper geometric understanding of how time decay and moneyness compound together.\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\nimport scipy.stats as si\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# --- 1. Vectorized Black-Scholes Functions ---\ndef bs_call_price(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    d2 = d1 - vol * np.sqrt(T)\n    return S * np.exp(-q * T) * si.norm.cdf(d1) - K * np.exp(-r * T) * si.norm.cdf(d2)\n\ndef bs_call_delta(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    return np.exp(-q * T) * si.norm.cdf(d1)\n\ndef bs_gamma(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    return (np.exp(-q * T) * si.norm.pdf(d1)) / (S * vol * np.sqrt(T))\n\n# --- 2. Setup Meshgrid ---\nT_surf = np.linspace(0.01, 2.0, 50)\nK_surf = np.linspace(70, 130, 50)\nT_mesh, K_mesh = np.meshgrid(T_surf, K_surf)\n\nS_fixed = 100\nq_fixed = 0.0\n\n# --- 3. Initialize FigureWidget ---\nfig = go.FigureWidget(data=[\n    go.Surface(\n        x=K_mesh, \n        y=T_mesh, \n        z=np.zeros_like(T_mesh), # Placeholder\n        colorscale='viridis',\n        contours={\"z\": {\"show\": True, \"usecolormap\": True, \"project_z\": True}}\n    )\n])\n\n# Fixed layout - no overlapping buttons!\nfig.update_layout(\n    title='Interactive Options Surface',\n    scene=dict(xaxis_title='Strike (K)', yaxis_title='Time (T)', zaxis_title='Value'),\n    width=900, height=700,\n    margin=dict(l=0, r=0, b=0, t=50) \n)\n\n# --- 4. Create UI Controls ---\nvol_slider = widgets.FloatSlider(value=0.2, min=0.05, max=1.0, step=0.05, description='Volatility:')\nrate_slider = widgets.FloatSlider(value=0.05, min=0.0, max=0.2, step=0.01, description='Rate:')\nmetric_dropdown = widgets.Dropdown(options=['Call Price', 'Call Delta', 'Gamma'], value='Gamma', description='Metric:')\n\n# --- 5. Update Logic ---\ndef update_surface(*args):\n    vol = vol_slider.value\n    rate = rate_slider.value\n    metric = metric_dropdown.value\n    \n    if metric == 'Call Price':\n        Z_new = bs_call_price(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n    elif metric == 'Call Delta': \n        Z_new = bs_call_delta(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n    else:\n        Z_new = bs_gamma(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n        \n    with fig.batch_update():\n        fig.data[0].z = Z_new\n\n# Bind controls\nvol_slider.observe(update_surface, 'value')\nrate_slider.observe(update_surface, 'value')\nmetric_dropdown.observe(update_surface, 'value')\n\n# Initial render\nupdate_surface()\n\n# --- 6. Display Clean UI ---\ncontrols = widgets.HBox([vol_slider, rate_slider, metric_dropdown])\ndisplay(widgets.VBox([controls, fig]))\n\n\n\n\n\nWe use a Brent root-finding algorithm to back-calculate the market’s implied volatility from our observed option prices. The resulting plot proves that the Black-Scholes assumption of constant volatility is empirically false—setting up the necessity for the advanced, volatility-skew-aware models in the subsequent notebooks.\n\n\nCode\nfrom scipy.optimize import brentq\n\n# --- Implied Volatility Solver ---\ndef implied_volatility(target_price, S, K, T, r, q, option_type='C'):\n    \"\"\"\n    Backs out the implied volatility using Brent's method.\n    \"\"\"\n    MAX_VOL = 5.0 # 500% volatility cap for solver limits\n    \n    def objective_function(sigma):\n        if option_type == 'C':\n            return bs_call_price(S, K, T, r, q, sigma) - target_price\n        else:\n            return bs_put_price(S, K, T, r, q, sigma) - target_price\n\n    try:\n        # Brent's method requires bounding the root. We search between 1% and 500% vol.\n        iv = brentq(objective_function, 1e-4, MAX_VOL)\n        return iv\n    except ValueError:\n        # Fails if the theoretical price cannot match the market price (e.g., arbitrage violations)\n        return np.nan\n\n# --- Calculate IV for Market Data ---\nmarket_ivs = []\n\nfor K, price in zip(market_strikes, market_prices):\n    iv = implied_volatility(price, S0, K, T, r, q, option_type='C')\n    market_ivs.append(iv)\n\nmarket_ivs = np.array(market_ivs)\n\n# Filter out NaNs if any arbitrage violations existed in the raw data\nvalid_idx = ~np.isnan(market_ivs)\nvalid_strikes = market_strikes[valid_idx]\nvalid_ivs = market_ivs[valid_idx]\n\n# Find the At-The-Money (ATM) volatility to act as our \"Constant BS Assumption\"\natm_idx = np.argmin(np.abs(valid_strikes - S0))\natm_vol = valid_ivs[atm_idx]\n\n# --- Plot the Volatility Skew ---\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, valid_ivs, color='blue', label='Market Implied Volatility', s=15)\nplt.axhline(y=atm_vol, color='red', linestyle='--', label=f'Constant BS Assumption (ATM Vol = {atm_vol:.2%})')\nplt.axvline(x=S0, color='black', linestyle=':', label=f'Spot Price (S0 = {S0:.2f})')\n\nplt.title(f'SPX Implied Volatility Skew/Smile\\nTarget Expiry: {TARGET_EXDATE}', fontsize=14)\nplt.xlabel('Strike Price (K)', fontsize=12)\nplt.ylabel('Implied Volatility ($\\sigma$)', fontsize=12)\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \nplt.legend()\nplt.grid(True, alpha=0.4)\nplt.show()"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n\n2. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the “Smirk” or “Skew” via stochastic vol—essential for pricing OTM puts accurately.\n3. Merton Jump: Adds “jumps” to the asset price to model market shocks.\nCaptures “Fat Tails” and sudden crashes via Poisson jumps."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#black-scholes-model",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#black-scholes-model",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "The Black-Scholes model assumes the stock price \\(S_t\\) follows a Geometric Brownian Motion (GBM):\\[dS_t = \\mu S_t dt + \\sigma S_t dW_t\\]\nWe can deconstruct this engine into two distinct components: * The Deterministic Drift (\\(\\mu S_t dt\\)): This represents the expected, constant growth rate of the asset. If market volatility were zero, the stock would simply grow smoothly at rate \\(\\mu\\), compounding like money in a standard savings account.\n\nThe Stochastic Diffusion (\\(\\sigma S_t dW_t\\)): This injects the randomness. \\(dW_t\\) is a Wiener process (Standard Brownian Motion) representing unpredictable market shocks. By scaling this randomness by the current stock price (\\(S_t\\)), the model ensures that the asset price can never drop below zero, perfectly reflecting the reality of limited liability in equities. Where:\n\n\\(\\mu\\) is the drift.\n\n\\(\\sigma\\) is the constant volatility.\n\n\\(dW_t\\) is a Wiener process (Brownian motion). \n\nCall Option Formula\nThe price of a European Call option \\(C(S, t)\\) is given by:\n\\[C(S, t) = S_0 e^{-qT} N(d_1) - K e^{-rT} N(d_2)\\]  Where \\(N(\\cdot)\\) is the cumulative distribution function of the standard normal distribution, and:\n\\[d_1 = \\frac{\\ln(S_0/K) + (r - q + \\sigma^2/2)T}{\\sigma\\sqrt{T}}\\]\\[d_2 = d_1 - \\sigma\\sqrt{T}\\]\n\n\nVolatility Drag (The Asymmetry of Returns):\nBecause asset returns compound geometrically, downward price movements penalize the overall value more severely than upward movements of the exact same percentage. A 50% drop requires a 100% gain just to recover the initial capital.\nMathematically, this “volatility drag” manifests when we apply Ito’s Lemma to the log-return process, introducing a \\(-\\frac{1}{2}\\sigma^2\\) variance penalty term. The higher the volatility, the stronger this downward drag on the expected geometric return."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#imports-and-setup",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#imports-and-setup",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom scipy.integrate import quad\nimport seaborn as sns\n\n# Set plotting style\nsns.set_style(\"darkgrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Global Parameters (Toggles)\nS0 = 100.0    # Spot Price\nK_list = np.linspace(80, 120, 50) # Range of Strikes for plotting\nT = 1.0       # Time to Maturity (1 year)\nr = 0.05      # Risk-free rate\nq = 0.0       # Dividend yield\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\nLoaded 352 strikes. S0: 4783.45, r: 0.0537\n\n\n\n\nCode\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# Load all massive Parquet files into this notebook's RAM\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\n\n\n\nCode\n# 1. Explicitly define your scenario variables so the plotter can see them later\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\n\n# 2. Feed them to the loader to get the exact state\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\n# 3. Extract the variables for the optimizer\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\nprint(f\"✅ Loaded {len(market_strikes)} strikes. S0: {S0}, r: {r*100:.2f}%\")\n\n\n✅ Loaded 352 strikes. S0: 4783.45, r: 5.37%"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#simulating-black-scholes",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#simulating-black-scholes",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "Visualizes the core assumption of the Black-Scholes model: that asset prices follow a log-normal random walk with constant drift and volatility. This Monte Carlo simulation provides a physical intuition for the probability distribution of terminal stock prices.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# --- 1. Simulate Geometric Brownian Motion (GBM) ---\ndef simulate_gbm(S0, mu, sigma, T, dt, num_paths):\n    \"\"\"\n    Simulates asset price paths using Geometric Brownian Motion.\n    \"\"\"\n    N = int(T / dt) # Number of time steps\n    t = np.linspace(0, T, N)\n    \n    # Generate random Wiener process steps\n    W = np.random.standard_normal(size=(num_paths, N)) \n    W = np.cumsum(W, axis=1) * np.sqrt(dt) \n    \n    # Calculate the paths using the analytical solution to the GBM SDE\n    # Notice the volatility drag term: (mu - 0.5 * sigma^2)\n    X = (mu - 0.5 * sigma**2) * t + sigma * W \n    S = S0 * np.exp(X) \n    return t, S\n\n# Parameters for simulation\nS0_sim = 100      # Initial stock price\nmu_sim = 0.08     # Expected return (drift)\nsigma_sim = 0.20  # Volatility\nT_sim = 1.0       # Time horizon (1 year)\ndt_sim = 1/252    # Daily steps\nnum_paths = 15    # Number of simulated paths to plot\n\nt_steps, paths = simulate_gbm(S0_sim, mu_sim, sigma_sim, T_sim, dt_sim, num_paths)\n\nplt.figure(figsize=(10, 6))\nfor i in range(num_paths):\n    plt.plot(t_steps, paths[i, :], lw=1.5, alpha=0.7)\n    \nplt.title('Monte Carlo Simulation of Asset Paths (GBM)')\nplt.xlabel('Time (Years)')\nplt.ylabel('Asset Price ($S_t$)')\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#black-scholes-call-pricing-the-greeks",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#black-scholes-call-pricing-the-greeks",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "Implements the closed-form analytical solutions for European Call options. We visualize Delta against the underlying asset price to demonstrate the dynamic, non-linear nature of option sensitivities, introducing the concept of continuous delta-hedging.\nMapping of the “Big Five” Greeks (\\(\\Delta\\), \\(\\Gamma\\), \\(\\Theta\\), \\(\\nu\\), \\(\\rho\\)). By visualizing these simultaneously, we can analyze the multi-dimensional risk exposure of an option contract as the underlying spot price moves across different moneyness levels.\n\n\nCode\n\ndef d1(S, K, T, r, q, sigma):\n    return (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n\ndef d2(S, K, T, r, q, sigma):\n    return d1(S, K, T, r, q, sigma) - sigma * np.sqrt(T)\n\ndef bs_call_price(S, K, T, r, q, sigma):\n    if sigma &lt;= 0 or T &lt;= 0: return max(S * np.exp(-q * T) - K * np.exp(-r * T), 0)\n    return (S * np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma)) - \n            K * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma)))\n\ndef bs_call_delta(S, K, T, r, q, sigma):\n    \"\"\"Rate of change of option price with respect to underlying asset.\"\"\"\n    return np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma))\n\ndef bs_gamma(S, K, T, r, q, sigma):\n    \"\"\"Rate of change of Delta (convexity). Identical for calls and puts.\"\"\"\n    return (np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma))) / (S * sigma * np.sqrt(T))\n\ndef bs_vega(S, K, T, r, q, sigma):\n    \"\"\"Sensitivity to volatility. Identical for calls and puts.\"\"\"\n    return S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * np.sqrt(T)\n\ndef bs_call_theta(S, K, T, r, q, sigma):\n    \"\"\"Time decay of the option.\"\"\"\n    term1 = -(S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * sigma) / (2 * np.sqrt(T))\n    term2 = r * K * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma))\n    term3 = q * S * np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma))\n    return term1 - term2 + term3\n\n# --- Add Call Rho ---\ndef bs_call_rho(S, K, T, r, q, sigma):\n    \"\"\"Sensitivity to the risk-free interest rate.\"\"\"\n    return K * T * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma))\n\n# --- Plotting the Big Five Greeks ---\nS_range = np.linspace(50, 150, 100)\nK_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed = 100, 1.0, 0.05, 0.0, 0.2\n\n# Calculate Greeks\ndeltas = [bs_call_delta(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\ngammas = [bs_gamma(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nthetas = [bs_call_theta(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nvegas  = [bs_vega(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nrhos   = [bs_call_rho(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\n\n# Create subplots\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\nfig.suptitle('Call Option Greeks vs. Underlying Price ($S_t$)', fontsize=16)\n\ngreeks_data = [\n    ('Delta ($\\Delta$)', deltas, 'purple'), ('Gamma ($\\Gamma$)', gammas, 'blue'), \n    ('Theta ($\\Theta$)', thetas, 'red'), ('Vega ($\\\\nu$)', vegas, 'green'), \n    ('Rho ($\\\\rho$)', rhos, 'orange')\n]\n\nfor i, (title, data, color) in enumerate(greeks_data):\n    ax = axes[i//3, i%3]\n    ax.plot(S_range, data, color=color, lw=2)\n    ax.axvline(x=K_fixed, color='black', linestyle='--', label='Strike (K=100)')\n    ax.set_title(title)\n    ax.set_xlabel('Underlying Price')\n    ax.grid(True, alpha=0.3)\n\naxes[1, 2].axis('off') # Hide the empty 6th subplot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCompletes the basic pricing suite by adding European Put options. We run a computational check to validate our formulas against the Put-Call Parity theorem (\\(C - P = S_0 e^{-qT} - K e^{-rT}\\)), ensuring our engine respects fundamental no-arbitrage constraints.\n\n\nCode\n# --- Put Option Pricing and Greeks ---\ndef bs_put_price(S, K, T, r, q, sigma):\n    if sigma &lt;= 0 or T &lt;= 0: return max(K * np.exp(-r * T) - S * np.exp(-q * T), 0)\n    return (K * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma)) - \n            S * np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma)))\n\ndef bs_put_delta(S, K, T, r, q, sigma):\n    return -np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma))\n\ndef bs_put_theta(S, K, T, r, q, sigma):\n    term1 = -(S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * sigma) / (2 * np.sqrt(T))\n    term2 = r * K * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma))\n    term3 = q * S * np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma))\n    return term1 + term2 - term3\n\ndef bs_put_rho(S, K, T, r, q, sigma):\n    return -K * T * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma))\n\n# --- Put-Call Parity Validation ---\nC = bs_call_price(S0_sim, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed)\nP = bs_put_price(S0_sim, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed)\n\nlhs = C - P\nrhs = S0_sim * np.exp(-q_fixed * T_fixed) - K_fixed * np.exp(-r_fixed * T_fixed)\n\nprint(f\"Put-Call Parity Check:\")\nprint(f\"LHS (Call - Put): {lhs:.4f}\")\nprint(f\"RHS (Discounted S - Discounted K): {rhs:.4f}\")\nprint(f\"Difference: {abs(lhs - rhs):.1e} (Effectively Zero)\")\n\n\nPut-Call Parity Check:\nLHS (Call - Put): 4.8771\nRHS (Discounted S - Discounted K): 4.8771\nDifference: 0.0e+00 (Effectively Zero)"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#the-options-pricing-surface",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_01_BlackSholes.html#the-options-pricing-surface",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "Options pricing is inherently multi-dimensional. The price is not just a function of the underlying asset’s price, but also its strike price and the time remaining until expiration.\nBy calculating the Black-Scholes price across a grid of different strikes and maturities, we can generate a 3D pricing surface. This visualizes how Out-Of-The-Money (OTM) options lose value rapidly as expiration approaches, while In-The-Money (ITM) options converge exactly to their intrinsic payoff.\nShifts from 2D static plots to an interactive 3D surface mapping the Call price against both Strike and Time to Maturity. This allows for a deeper geometric understanding of how time decay and moneyness compound together.\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\nimport scipy.stats as si\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# --- 1. Vectorized Black-Scholes Functions ---\ndef bs_call_price(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    d2 = d1 - vol * np.sqrt(T)\n    return S * np.exp(-q * T) * si.norm.cdf(d1) - K * np.exp(-r * T) * si.norm.cdf(d2)\n\ndef bs_call_delta(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    return np.exp(-q * T) * si.norm.cdf(d1)\n\ndef bs_gamma(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    return (np.exp(-q * T) * si.norm.pdf(d1)) / (S * vol * np.sqrt(T))\n\n# --- 2. Setup Meshgrid ---\nT_surf = np.linspace(0.01, 2.0, 50)\nK_surf = np.linspace(70, 130, 50)\nT_mesh, K_mesh = np.meshgrid(T_surf, K_surf)\n\nS_fixed = 100\nq_fixed = 0.0\n\n# --- 3. Initialize FigureWidget ---\nfig = go.FigureWidget(data=[\n    go.Surface(\n        x=K_mesh, \n        y=T_mesh, \n        z=np.zeros_like(T_mesh), # Placeholder\n        colorscale='viridis',\n        contours={\"z\": {\"show\": True, \"usecolormap\": True, \"project_z\": True}}\n    )\n])\n\n# Fixed layout - no overlapping buttons!\nfig.update_layout(\n    title='Interactive Options Surface',\n    scene=dict(xaxis_title='Strike (K)', yaxis_title='Time (T)', zaxis_title='Value'),\n    width=900, height=700,\n    margin=dict(l=0, r=0, b=0, t=50) \n)\n\n# --- 4. Create UI Controls ---\nvol_slider = widgets.FloatSlider(value=0.2, min=0.05, max=1.0, step=0.05, description='Volatility:')\nrate_slider = widgets.FloatSlider(value=0.05, min=0.0, max=0.2, step=0.01, description='Rate:')\nmetric_dropdown = widgets.Dropdown(options=['Call Price', 'Call Delta', 'Gamma'], value='Gamma', description='Metric:')\n\n# --- 5. Update Logic ---\ndef update_surface(*args):\n    vol = vol_slider.value\n    rate = rate_slider.value\n    metric = metric_dropdown.value\n    \n    if metric == 'Call Price':\n        Z_new = bs_call_price(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n    elif metric == 'Call Delta': \n        Z_new = bs_call_delta(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n    else:\n        Z_new = bs_gamma(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n        \n    with fig.batch_update():\n        fig.data[0].z = Z_new\n\n# Bind controls\nvol_slider.observe(update_surface, 'value')\nrate_slider.observe(update_surface, 'value')\nmetric_dropdown.observe(update_surface, 'value')\n\n# Initial render\nupdate_surface()\n\n# --- 6. Display Clean UI ---\ncontrols = widgets.HBox([vol_slider, rate_slider, metric_dropdown])\ndisplay(widgets.VBox([controls, fig]))\n\n\n\n\n\nWe use a Brent root-finding algorithm to back-calculate the market’s implied volatility from our observed option prices. The resulting plot proves that the Black-Scholes assumption of constant volatility is empirically false—setting up the necessity for the advanced, volatility-skew-aware models in the subsequent notebooks.\n\n\nCode\nfrom scipy.optimize import brentq\n\n# --- Implied Volatility Solver ---\ndef implied_volatility(target_price, S, K, T, r, q, option_type='C'):\n    \"\"\"\n    Backs out the implied volatility using Brent's method.\n    \"\"\"\n    MAX_VOL = 5.0 # 500% volatility cap for solver limits\n    \n    def objective_function(sigma):\n        if option_type == 'C':\n            return bs_call_price(S, K, T, r, q, sigma) - target_price\n        else:\n            return bs_put_price(S, K, T, r, q, sigma) - target_price\n\n    try:\n        # Brent's method requires bounding the root. We search between 1% and 500% vol.\n        iv = brentq(objective_function, 1e-4, MAX_VOL)\n        return iv\n    except ValueError:\n        # Fails if the theoretical price cannot match the market price (e.g., arbitrage violations)\n        return np.nan\n\n# --- Calculate IV for Market Data ---\nmarket_ivs = []\n\nfor K, price in zip(market_strikes, market_prices):\n    iv = implied_volatility(price, S0, K, T, r, q, option_type='C')\n    market_ivs.append(iv)\n\nmarket_ivs = np.array(market_ivs)\n\n# Filter out NaNs if any arbitrage violations existed in the raw data\nvalid_idx = ~np.isnan(market_ivs)\nvalid_strikes = market_strikes[valid_idx]\nvalid_ivs = market_ivs[valid_idx]\n\n# Find the At-The-Money (ATM) volatility to act as our \"Constant BS Assumption\"\natm_idx = np.argmin(np.abs(valid_strikes - S0))\natm_vol = valid_ivs[atm_idx]\n\n# --- Plot the Volatility Skew ---\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, valid_ivs, color='blue', label='Market Implied Volatility', s=15)\nplt.axhline(y=atm_vol, color='red', linestyle='--', label=f'Constant BS Assumption (ATM Vol = {atm_vol:.2%})')\nplt.axvline(x=S0, color='black', linestyle=':', label=f'Spot Price (S0 = {S0:.2f})')\n\nplt.title(f'SPX Implied Volatility Skew/Smile\\nTarget Expiry: {TARGET_EXDATE}', fontsize=14)\nplt.xlabel('Strike Price (K)', fontsize=12)\nplt.ylabel('Implied Volatility ($\\sigma$)', fontsize=12)\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \nplt.legend()\nplt.grid(True, alpha=0.4)\nplt.show()"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSingapore Insurance Strategy Dashboard\n\n\n\nPython\n\nStreamlit\n\nActuarial Science\n\nData Visualization\n\nMonte Carlo\n\nfeatured\n\n\n\nA Streamlit-powered actuarial dashboard that pits ‘Buy Term, Invest the Difference’ against Whole Life plans, accounting for human behavior and stochastic risk.\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nGap Challenge Solver\n\n\n\nComputer Vision\n\nPython\n\nStreamlit\n\nAlgorithms\n\nfeatured\n\n\n\nEvolution of a Computer Vision pipeline: From brittle Template Matching to robust SVMs, integrated with a Backtracking solver.\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production\n\n\n\nQuantitative Finance\n\nVolatility Modeling\n\nPython\n\nOOP\n\nStreamlit\n\nOptimization\n\nfeatured\n\n\n\nA comprehensive case study bridging Quantitative Research and Software Engineering. From calibrating Heston and Bates models using noisy S&P 500 tick data to deploying an…\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBlack-Sholes Model\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nMerton-Jump Model\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nHeston Model\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nBates Model\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nBlack-Scholes Model Real Data\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nHeston Model Real Data\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nMerton Model Real Data\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nBates Model Real Data\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nModel Comparison\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n\n\n\n\n\n\n\nOut-of-sample Testing for Models\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n:::\n\n:::\n:::\nNo matching items\n:::\n:::"
  },
  {
    "objectID": "notes/posts/Black-Sholes-Derivation/index.html#the-full-mathematical-derivation-of-the-call-formula",
    "href": "notes/posts/Black-Sholes-Derivation/index.html#the-full-mathematical-derivation-of-the-call-formula",
    "title": "Concept: The Black-Scholes Model",
    "section": "The Full Mathematical Derivation of the Call Formula",
    "text": "The Full Mathematical Derivation of the Call Formula\nTo truly understand Black-Scholes, we must walk step-by-step from the assumption of the asset’s random walk to the final pricing formula. We do this by deriving Itô’s Lemma, establishing the Partial Differential Equation (PDE), transforming it into an expected value, and integrating.\n\n\n1. The Stochastic Framework\nSuppose we have a stochastic process \\(X_t\\) that models our asset price, following a generalized Itô drift-diffusion process:\n\\[\ndX_t = \\mu(X_t, t)dt + \\sigma(X_t, t)dW_t\n\\]\nWhere: * \\(\\mu(X_t, t)\\) is the predictable, deterministic drift. * \\(\\sigma(X_t, t)\\) is the volatility (diffusion) scaling factor. * \\(dW_t\\) is the increment of a standard Wiener process (Brownian motion), representing random market shocks.\nWe want to find the differential \\(df\\) of a scalar function \\(f(X_t, t)\\)—such as an option pricing function—that is at least twice-differentiable in \\(x\\) and once-differentiable in \\(t\\).\n\n\n\n2. The Taylor Series Expansion (The Divergence from Ordinary Calculus)\nWe begin by examining the change in our function \\(f\\) over an infinitesimally small time increment \\(\\Delta t\\). We use a two-variable Taylor expansion:\n\\[\n\\Delta f \\approx \\frac{\\partial f}{\\partial t}\\Delta t + \\frac{\\partial f}{\\partial x}\\Delta X + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(\\Delta X)^2 + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial t^2}(\\Delta t)^2 + \\frac{\\partial^2 f}{\\partial x \\partial t}(\\Delta X \\Delta t) + \\dots\n\\]\nThe Core Difference: In standard, deterministic calculus, as \\(\\Delta t \\to 0\\), any terms of order higher than \\(\\Delta t\\) (like \\(\\Delta t^2\\) or \\(\\Delta x^2\\)) shrink to zero so rapidly that they become negligible. We simply drop them.\nHowever, in the stochastic world, the path of \\(X_t\\) is driven by the random walk of \\(W_t\\). Brownian motion is continuous but nowhere differentiable—it is infinitely jagged.\nBecause of this extreme jaggedness, the squared variation \\((\\Delta X)^2\\) does not shrink to zero faster than \\(\\Delta t\\). It is large enough that it must be kept in our equation.\n\n\n\n3. Analyzing \\((\\Delta X)^2\\) and the Rules of Stochastic Calculus\nTo see why the second-order term survives, we substitute our definition of \\(\\Delta X\\) into the squared term:\n\\[\n(\\Delta X)^2 = (\\mu \\Delta t + \\sigma \\Delta W)^2\n\\] \\[\n(\\Delta X)^2 = \\mu^2 (\\Delta t)^2 + 2\\mu\\sigma (\\Delta t \\Delta W) + \\sigma^2 (\\Delta W)^2\n\\]\nNow, we take the limit as \\(\\Delta t \\to 0\\) and apply the formal multiplication rules of stochastic calculus (Itô multiplication table): * \\(dt \\cdot dt = 0\\) (Shrinks too fast) * \\(dt \\cdot dW_t = 0\\) (Shrinks too fast, scales as \\(dt^{3/2}\\)) * \\(dW_t \\cdot dW_t = dt\\) (The critical survival)\n\nDeep Dive: Why does \\((dW_t)^2 = dt\\)?\nThe jump to \\((dW_t)^2 = dt\\) is the most unintuitive part of stochastic calculus. How can a random variable squared equal a deterministic, predictable passage of time? We can break this down statistically, behaviorally, and heuristically.\n1. The Statistical Properties\nHere is the exact step-by-step logic chain to prove this equivalence: * Definition: By definition, an increment of a Wiener process follows a normal distribution centered at zero: \\(\\Delta W \\sim \\mathcal{N}(0, \\Delta t)\\). * Variance Formula: From fundamental statistics, the variance of any random variable is \\(Var[\\Delta W] = E[(\\Delta W)^2] - (E[\\Delta W])^2\\). * Plug in the numbers: Since the mean expected value \\(E[\\Delta W]\\) is exactly \\(0\\), squaring it \\((E[\\Delta W])^2\\) is also \\(0\\). * The Reduction: This leaves us with a perfectly simplified equation: \\(Var[\\Delta W] = E[(\\Delta W)^2]\\). * The Result: Because the variance of a Wiener process over time \\(\\Delta t\\) is defined as \\(\\Delta t\\), it must be mathematically true that the expected value of the squared increment is the time step itself: \\(E[(\\Delta W)^2] = \\Delta t\\).\n2. The Law of Large Numbers (Quadratic Variation)\nOne might naturally ask: “If \\(E[(\\Delta W)^2] = \\Delta t\\), isn’t \\((\\Delta W)^2\\) still a random variable around that expected value?”\nYes, for a single, isolated step, it is. However, Itô’s Lemma deals with continuous time, where the limit as \\(\\Delta t \\to 0\\). If we divide a time interval \\([0, T]\\) into \\(n\\) microscopic pieces, the total sum of these squared increments is called the Quadratic Variation: \\[[W, W]_T = \\lim_{n \\to \\infty} \\sum_{i=1}^{n} (W_{t_i} - W_{t_{i-1}})^2\\]\nAs \\(n\\) becomes infinitely large, we must look at the variance of this sum. For a normal distribution, the fourth moment allows us to calculate the variance of the square: \\[Var[(\\Delta W)^2] = E[(\\Delta W)^4] - (E[(\\Delta W)^2])^2 = 3(\\Delta t)^2 - (\\Delta t)^2 = 2(\\Delta t)^2\\]\nBecause the variance of \\((\\Delta W)^2\\) is proportional to \\((\\Delta t)^2\\), it shrinks to zero vastly faster than the mean (\\(\\Delta t\\)). In the continuous limit, the randomness completely “washes out,” leaving only the deterministic expected value. The sum of squares behaves perfectly like a ticking clock.\n3. The Heuristic Comparison\nTo visualize why this is unique to random walks, compare it to a standard smooth function, \\(g(t) = t\\): * Standard Calculus: \\(\\Delta g = \\Delta t\\). Therefore, \\((\\Delta g)^2 = (\\Delta t)^2\\). As \\(\\Delta t \\to 0\\), this squared term essentially vanishes to zero. * Stochastic Calculus: \\(\\Delta W \\approx \\sqrt{\\Delta t}\\). Therefore, \\((\\Delta W)^2 \\approx \\Delta t\\).\nBecause Brownian motion is inherently “square-root-of-time” risky, its square is of the exact same “size” as time itself.\n\n\n\n\n4. Final Substitution\nNow we return to our original Taylor expansion. Knowing which terms survive the limit as \\(\\Delta t \\to 0\\), we truncate the series, keeping only the terms of order \\(dt\\) or \\(dW_t\\):\n\\[\ndf = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial x}dX_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(dX_t)^2\n\\]\nNext, we substitute our initial definitions: * \\(dX_t = \\mu dt + \\sigma dW_t\\) * \\((dX_t)^2 = \\sigma^2 dt\\)\n\\[\ndf = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial x}(\\mu dt + \\sigma dW_t) + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(\\sigma^2 dt)\n\\]\n\n\n\n5. The Result: Itô’s Lemma\nFinally, we group the deterministic \\(dt\\) terms together and separate the stochastic \\(dW_t\\) term. This yields the foundational equation of stochastic calculus—Itô’s Lemma:\n\\[\ndf = \\left( \\frac{\\partial f}{\\partial t} + \\mu \\frac{\\partial f}{\\partial x} + \\frac{1}{2} \\sigma^2 \\frac{\\partial^2 f}{\\partial x^2} \\right) dt + \\sigma \\frac{\\partial f}{\\partial x} dW_t\n\\]\n(Note for Black-Scholes: In our options pricing model, \\(f\\) is the option price \\(V(S, t)\\), the asset \\(X_t\\) is the stock price \\(S_t\\), the drift \\(\\mu\\) becomes \\(\\mu S_t\\), and the diffusion \\(\\sigma\\) becomes \\(\\sigma S_t\\).)\n\n\n\nStep 2: Constructing the Riskless Portfolio (Delta Hedging)\nNow that we have Itô’s Lemma to describe how the option price \\(V(S, t)\\) changes over time, we need a way to find its fair value. Fischer Black and Myron Scholes realized that if you hold the right combination of the option and the underlying stock, you can completely cancel out the unpredictable market shocks (\\(dW_t\\)).\nWe construct a hypothetical portfolio, denoted as \\(\\Pi\\). In this portfolio, we hold exactly one long position in our option, and we short sell a specific number of shares of the underlying stock. We will call this number of shares \\(\\Delta\\) (not to be confused with a time step \\(\\Delta t\\)).\nThe value of our portfolio at any given time is: \\[\n\\Pi = V - \\Delta S\n\\]\nWe want to observe how the value of this portfolio changes over a microscopic time step \\(dt\\). By applying basic differentiation: \\[\nd\\Pi = dV - \\Delta dS\n\\]\nNow, we substitute the two massive equations we have built so far: 1. \\(dV\\): The change in the option price (from Itô’s Lemma). 2. \\(dS\\): The change in the stock price (from our Geometric Brownian Motion assumption: \\(dS = \\mu S dt + \\sigma S dW_t\\)).\nSubstituting these into our portfolio equation yields: \\[\nd\\Pi = \\left[ \\left( \\frac{\\partial V}{\\partial t} + \\mu S \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt + \\sigma S \\frac{\\partial V}{\\partial S} dW_t \\right] - \\Delta \\left[ \\mu S dt + \\sigma S dW_t \\right]\n\\]\nThis looks messy, but we can organize it by grouping all the deterministic parts (attached to \\(dt\\)) and all the random, volatile parts (attached to \\(dW_t\\)):\n\\[\nd\\Pi = \\left( \\frac{\\partial V}{\\partial t} + \\mu S \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - \\Delta \\mu S \\right) dt + \\left( \\sigma S \\frac{\\partial V}{\\partial S} - \\Delta \\sigma S \\right) dW_t\n\\]\n\nEliminating the Randomness\nThe second set of parentheses contains the \\(dW_t\\) term. This is the sole source of risk in our portfolio. If the market suddenly crashes or spikes, this is the term that dictates our losses or gains.\nThe brilliance of the Black-Scholes model lies in a simple algebraic observation: we can force that entire term to equal zero. We do this by continuously adjusting the number of shares we short (\\(\\Delta\\)).\nIf we deliberately choose: \\[\n\\Delta = \\frac{\\partial V}{\\partial S}\n\\]\nThen the random term becomes: \\[\n\\left( \\sigma S \\frac{\\partial V}{\\partial S} - \\frac{\\partial V}{\\partial S} \\sigma S \\right) dW_t = 0 \\cdot dW_t = 0\n\\]\nBy setting our stock position equal to the partial derivative of the option with respect to the stock (which traders now call the option’s “Delta”), the random shocks completely cancel out. The portfolio becomes completely immune to small movements in the underlying asset.\nWhat we are left with is a portfolio whose change in value is entirely deterministic, driven only by the passage of time: \\[\nd\\Pi = \\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt\n\\]\n\n\n\n\nStep 3: The Principle of No-Arbitrage and the Black-Scholes PDE\nAt the end of Step 2, we successfully engineered a portfolio \\(\\Pi\\) that is completely insulated from the random shocks of the stock market. Because we chose to hold \\(\\Delta = \\frac{\\partial V}{\\partial S}\\) shares of stock short against our long option, the \\(dW_t\\) term was mathematically forced to zero.\nWe were left with a portfolio whose change in value is entirely deterministic, driven only by the smooth passage of time: \\[\nd\\Pi = \\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt\n\\]\nNow, we must ask an economic question: If a portfolio has absolutely zero risk, how much return should it generate?\n\nThe Financial Law of Gravity: No-Arbitrage\nIn an efficient market, there is a fundamental law known as the Principle of No-Arbitrage. An “arbitrage” is a guaranteed, risk-free profit that requires zero net investment.\nIf our completely riskless portfolio \\(\\Pi\\) earned a return higher than a risk-free government bond (yielding the risk-free rate, \\(r\\)), an arbitrageur could borrow infinite amounts of money at rate \\(r\\) from the bank, invest it into our portfolio \\(\\Pi\\), and pocket the difference completely risk-free. Conversely, if \\(\\Pi\\) earned less than \\(r\\), an arbitrageur would short our portfolio and put the cash into the bank, again locking in a risk-free profit.\nBecause market forces instantly exploit and close these loopholes, a fundamental truth emerges: Any portfolio with zero risk must earn exactly the risk-free interest rate.\nMathematically, the growth of our portfolio over the time step \\(dt\\) must equal the continuously compounded risk-free rate \\(r\\) multiplied by the current value of the portfolio: \\[\nd\\Pi = r \\Pi dt\n\\]\n\n\nEquating the Math and the Economics\nWe now have two completely different ways to describe the change in our portfolio (\\(d\\Pi\\)). 1. The Calculus View (from Itô’s Lemma): \\(d\\Pi = \\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt\\) 2. The Economics View (from No-Arbitrage): \\(d\\Pi = r \\Pi dt\\)\nBecause these describe the exact same portfolio, we can set them equal to each other: \\[\n\\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt = r \\Pi dt\n\\]\nRecall from Step 2 that the total value of our portfolio is \\(\\Pi = V - \\Delta S\\), and we defined \\(\\Delta = \\frac{\\partial V}{\\partial S}\\). Let’s substitute that back in: \\[\n\\left( \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} \\right) dt = r \\left( V - \\frac{\\partial V}{\\partial S} S \\right) dt\n\\]\nSince both sides are simply multiplying a rate by the time step \\(dt\\), we can divide both sides by \\(dt\\) to remove it entirely: \\[\n\\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} = rV - rS \\frac{\\partial V}{\\partial S}\n\\]\nFinally, let’s rearrange this equation by moving everything to the left side so that it equals zero.\n\\[\n\\frac{\\partial V}{\\partial t} + rS \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - rV = 0\n\\]\n\n\nThe Grand Reveal: Deconstructing the PDE\nThis is it. This is the Black-Scholes Partial Differential Equation. It is one of the most famous equations in finance, and every single term has a physical, trading interpretation, often mapped directly to the “Greeks”:\n\n\\(\\frac{\\partial V}{\\partial t}\\) (Theta - Time Decay): This is negative for options. As time ticks closer to expiration, the option loses value because there is less time for the stock to make a favorable move.\n\\(rS \\frac{\\partial V}{\\partial S}\\) (Risk-Neutral Drift): This term represents the cost of financing the Delta-hedged position in the stock.\n\\(\\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2}\\) (Gamma - Convexity): This is the engine of the option’s value. Because the second derivative (\\(\\Gamma\\)) is positive, volatility (\\(\\sigma^2\\)) directly adds value to the option. This perfectly offsets the bleed from time decay (\\(\\Theta\\)).\n\\(- rV\\) (Discounting): This simply discounts the final payoff of the option back to present value at the risk-free rate.\n\nThe beauty of the Black-Scholes PDE is that the expected return of the stock (\\(\\mu\\)) has completely vanished! Because we can hedge away the directional risk of the stock, the option’s price depends only on the stock’s volatility (\\(\\sigma\\)), not its expected direction.\n\n\n\n\nInterlude: The Fork in the Road (How do we solve this PDE?)\nAt the end of Step 3, we successfully derived the Black-Scholes PDE: \\[\n\\frac{\\partial V}{\\partial t} + rS \\frac{\\partial V}{\\partial S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} - rV = 0\n\\]\nThis equation dictates exactly how the option price must behave to prevent arbitrage. But an equation is not a price. To get a dollar value, we have to solve this PDE subject to a boundary condition: at maturity (\\(T\\)), a call option is worth exactly \\(\\max(S_T - K, 0)\\).\nFrom here, quantitative finance splits into two distinct paths: Numerical Methods and Analytical Methods.\n\nPath A: The Numerical Approach (Finite Difference Methods)\nInstead of looking for a perfect algebraic formula, Numerical Methods use brute-force computation. We chop time and asset prices into a massive grid (a matrix). We know the exact payoff at expiration (the rightmost edge of the grid), and we mathematically step backward through time, cell by cell, until we reach “today” (the leftmost edge).\nTo do this, we approximate the continuous derivatives from our PDE using discrete “differences” on our grid. Let \\(V_i^j\\) be the option price at stock price node \\(i\\) and time step \\(j\\). * Delta (First Derivative of Space): \\(\\frac{\\partial V}{\\partial S} \\approx \\frac{V_{i+1}^j - V_{i-1}^j}{2\\Delta S}\\) * Gamma (Second Derivative of Space): \\(\\frac{\\partial^2 V}{\\partial S^2} \\approx \\frac{V_{i+1}^j - 2V_i^j + V_{i-1}^j}{(\\Delta S)^2}\\) * Theta (First Derivative of Time): \\(\\frac{\\partial V}{\\partial t} \\approx \\frac{V_i^{j+1} - V_i^j}{\\Delta t}\\)\nBy substituting these grid approximations into the Black-Scholes PDE, we can build a programmatic solver. There are three main ways to construct this grid, trading off between computational speed and mathematical stability.\n\n1. The Explicit Scheme\nThe Explicit method is the simplest. To find the unknown option price at a specific node one step back in time (\\(V_i^j\\)), it relies only on the already-known information from the nodes immediately ahead of it in the future (\\(V_{i-1}^{j+1}, V_i^{j+1}, V_{i+1}^{j+1}\\)).\n\nPros: It is incredibly easy to code. The calculation for each node is a simple linear equation.\nCons: It is conditionally stable. If your time steps (\\(\\Delta t\\)) are too large compared to your price steps (\\(\\Delta S\\)), the mathematical errors compound exponentially. The grid “blows up,” and your computer will output an option price of infinity.\n\n\n\n2. The Implicit Scheme\nThe Implicit method flips the logic. Instead of expressing the current time step as a function of the future, it expresses the future known boundary conditions as a function of the current unknown time step.\nBecause multiple unknown nodes depend on each other simultaneously, you cannot calculate them one by one. You must solve the entire column of the grid at the exact same time using a system of linear equations (specifically, by inverting a tridiagonal matrix).\n\nPros: It is unconditionally stable. No matter how large you make your time steps, the math will never blow up.\nCons: It is computationally expensive. Solving a massive matrix at every single time step requires significantly more processing power.\n\n\n\n3. The Crank-Nicolson Scheme (The Industry Standard)\nIn 1947, John Crank and Phyllis Nicolson realized that if you take an exact 50/50 average of the Explicit and Implicit methods, magical things happen to the error terms.\nThe Crank-Nicolson method centers the time difference exactly halfway between step \\(j\\) and step \\(j+1\\). Like the Implicit method, it requires solving a tridiagonal matrix (often using a lightning-fast technique called the Thomas Algorithm).\n\nThe Verdict: This is the gold standard for PDE options pricing. It is unconditionally stable (like Implicit) but converges on the true price much faster and with significantly higher accuracy than the other two methods.\n\nWhy use Finite Difference Methods? If we just want to price a standard European option, FDM is overkill. But for American Options, which can be exercised early, FDM is mandatory. Because we step backward through time cell by cell, we can stop at every single node and ask the computer: “Is the theoretical option price here lower than the immediate payoff of early exercise? If yes, replace the theoretical price with the exercise value.”\n\n\n\nPath B: The Analytical Approach\nWhile grids are powerful, for European options (which cannot be exercised early), we do not need to step through time mechanically. We can use advanced probability theory to solve the PDE perfectly, instantly, and in one continuous leap.\nThis brings us back to our derivation, and the genius of Richard Feynman and Mark Kac.\n\n\n\n\nStep 4: The Risk-Neutral Expectation (Feynman-Kac)\nSolving this PDE directly requires complex substitutions to turn it into a heat equation. Instead, we use the Feynman-Kac theorem, which states that the solution to this PDE, subject to the European Call boundary condition at maturity \\(V(S, T) = \\max(S_T - K, 0)\\), can be written as a discounted expected value under the risk-neutral measure \\(\\mathbb{Q}\\):\n\\[\nC(S,t) = e^{-r(T-t)} \\mathbb{E}^{\\mathbb{Q}}[\\max(S_T - K, 0)]\n\\]\n\nThe Risk-Neutral Measure (\\(\\mathbb{Q}\\))\nNotice the \\(\\mathbb{Q}\\) above the expectation operator. This indicates that we are calculating the expectation under the Risk-Neutral Measure, not the real-world probability measure (\\(\\mathbb{P}\\)).\nWhy? Remember when we delta-hedged our portfolio in Step 2? The real-world expected return of the stock (\\(\\mu\\)) completely disappeared from our math. Because the option price does not care about the stock’s true expected growth, we can pretend we live in a simplified “risk-neutral” universe. In this universe, all assets—including the risky stock—are expected to grow at exactly the risk-free rate \\(r\\).\nSo, we rewrite our stock’s stochastic differential equation (SDE), replacing \\(\\mu\\) with \\(r\\): \\[\ndS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}_t\n\\]\n\n\nSolving for the Terminal Stock Price (\\(S_T\\))\nTo evaluate our expected payoff, we need an explicit formula for \\(S_T\\). We solve the risk-neutral SDE by applying Itô’s Lemma to the natural log of the stock price, \\(f(S_t) = \\ln(S_t)\\).\nUsing Itô’s Lemma on \\(\\ln(S_t)\\) yields: \\[\nd(\\ln S_t) = \\left( r - \\frac{1}{2}\\sigma^2 \\right) dt + \\sigma dW^{\\mathbb{Q}}_t\n\\]\n(Notice the \\(-\\frac{1}{2}\\sigma^2\\) term appearing! This is the exact mathematical origin of the volatility drag we discussed in Section 1. The variance mathematically pulls down the expected geometric growth rate.)\nBecause the drift and volatility terms are now constants, we can cleanly integrate both sides from today (\\(t\\)) to expiration (\\(T\\)). Let \\(\\tau = T - t\\) (the time to maturity). \\[\n\\ln(S_T) - \\ln(S_t) = \\left( r - \\frac{1}{2}\\sigma^2 \\right)\\tau + \\sigma (W_T - W_t)\n\\]\nBy taking the exponential of both sides, we get the explicit, log-normal distribution for the stock price at maturity: \\[\nS_T = S_t \\exp\\left( \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma \\sqrt{\\tau} Z \\right)\n\\]\nWhere \\(Z \\sim \\mathcal{N}(0,1)\\) is a standard normal random variable representing the total accumulated randomness between \\(t\\) and \\(T\\).\n\n\n\n\nStep 5: Splitting and Solving the Integrals\nWe can now rewrite our expectation as an integral over the standard normal Probability Density Function (PDF), which is \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}\\):\n\\[\nC(S,t) = e^{-r\\tau} \\int_{-\\infty}^{\\infty} \\max(S_T(z) - K, 0) \\phi(z) dz\n\\]\n\nFinding the Boundary Condition (Dropping the Max Function)\nThe option only has a payoff when \\(S_T &gt; K\\). We need to find the critical value of our normal variable \\(Z\\) where the stock price exactly equals the strike price (\\(S_T = K\\)):\n\\[\nS_t \\exp\\left( \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma \\sqrt{\\tau} Z \\right) &gt; K\n\\] Take the natural log of both sides: \\[\n\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma \\sqrt{\\tau} Z &gt; \\ln\\left(\\frac{K}{S_t}\\right)\n\\] Isolate \\(Z\\): \\[\nZ &gt; \\frac{\\ln(K/S_t) - (r - \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}}\n\\]\nLet’s define this entire right-side boundary threshold as \\(-d_2\\). Thus, the option is “in-the-money” and pays off whenever \\(Z &gt; -d_2\\). Because we know the lower bound, we can drop the \\(\\max()\\) function and restrict our integral bounds:\n\\[\nC(S,t) = e^{-r\\tau} \\int_{-d_2}^{\\infty} (S_T(z) - K) \\phi(z) dz\n\\]\n\n\nSplitting the Integral\nWe expand the terms and split this into two separate integrals: \\[\nC(S,t) = \\underbrace{e^{-r\\tau} \\int_{-d_2}^{\\infty} S_T(z) \\phi(z) dz}_{\\text{Part A (Stock)}} \\quad - \\quad \\underbrace{K e^{-r\\tau} \\int_{-d_2}^{\\infty} \\phi(z) dz}_{\\text{Part B (Strike)}}\n\\]\n\n\nSolving Part B (The Strike Portion)\nPart B is incredibly straightforward. It is simply the discounted strike price multiplied by the probability that \\(Z &gt; -d_2\\). Because the standard normal distribution is perfectly symmetric, the probability of being greater than \\(-d_2\\) is identical to the probability of being less than \\(d_2\\). Let \\(N(\\cdot)\\) denote the standard normal Cumulative Distribution Function (CDF). \\[\nP(Z &gt; -d_2) = P(Z &lt; d_2) = N(d_2)\n\\]\nSo, Part B simplifies beautifully to: \\[\nK e^{-r\\tau} N(d_2)\n\\]\n\n\nSolving Part A (The Stock Portion)\nPart A is trickier because \\(S_T(z)\\) also contains an exponential function. Let’s substitute \\(S_T(z)\\) and \\(\\phi(z)\\) into the integral: \\[\ne^{-r\\tau} \\int_{-d_2}^{\\infty} S_t e^{(r - \\sigma^2/2)\\tau + \\sigma \\sqrt{\\tau} z} \\left( \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\right) dz\n\\]\nWe pull the constants out of the integral and combine the \\(e\\) exponents: \\[\nS_t e^{-r\\tau} e^{r\\tau - \\sigma^2\\tau/2} \\int_{-d_2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}z^2 + \\sigma \\sqrt{\\tau} z} dz\n\\]\nThe \\(e^{-r\\tau}\\) and \\(e^{r\\tau}\\) cancel out outside the integral. Inside the integral, we must complete the square in the exponent to turn it back into a recognizable normal distribution: \\[\n-\\frac{1}{2}z^2 + \\sigma \\sqrt{\\tau} z = -\\frac{1}{2}(z^2 - 2\\sigma \\sqrt{\\tau} z)\n\\] \\[\n(z^2 - 2\\sigma \\sqrt{\\tau} z) = (z - \\sigma \\sqrt{\\tau})^2 - \\sigma^2 \\tau\n\\]\nSubstituting this completed square back into the exponent: \\[\nS_t e^{-\\sigma^2\\tau/2} \\int_{-d_2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}(z - \\sigma \\sqrt{\\tau})^2 + \\sigma^2\\tau/2} dz\n\\]\nThe \\(e^{-\\sigma^2\\tau/2}\\) on the outside and \\(e^{\\sigma^2\\tau/2}\\) on the inside cancel perfectly! We are left with: \\[\nS_t \\int_{-d_2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}(z - \\sigma \\sqrt{\\tau})^2} dz\n\\]\nThis integral is just a standard normal distribution that has been shifted by a mean of \\(\\sigma \\sqrt{\\tau}\\). Let’s make a final substitution: let \\(y = z - \\sigma \\sqrt{\\tau}\\), so \\(dy = dz\\). When \\(z = -d_2\\), our new lower bound becomes \\(y = -d_2 - \\sigma \\sqrt{\\tau}\\).\nBy mathematical definition, we call this new boundary \\(-d_1\\). Therefore: \\[\nd_1 = d_2 + \\sigma \\sqrt{\\tau}\n\\]\nThe integral becomes: \\[\nS_t \\int_{-d_1}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2} dy\n\\]\nJust like in Part B, this is simply \\(S_t \\times P(Y &gt; -d_1)\\). Due to symmetry, this equals \\(S_t N(d_1)\\).\n\n\n\n\nStep 6: The Final Black-Scholes Formula\nBy recombining our solved integrals (Part A - Part B), we finally arrive at the glorious, closed-form Black-Scholes formula for a European Call option:\n\\[\nC(S, t) = S_t N(d_1) - K e^{-r\\tau} N(d_2)\n\\]\nWhere: \\[\nd_1 = \\frac{\\ln(S_t/K) + (r + \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}}\n\\] \\[\nd_2 = d_1 - \\sigma \\sqrt{\\tau} = \\frac{\\ln(S_t/K) + (r - \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}}\n\\]\n\nFinancial Intuition of the Formula\nThe math is beautiful, but what does it actually mean for a trader? * \\(N(d_2)\\) is the risk-neutral probability that the option will expire “in-the-money” (that \\(S_T &gt; K\\)). Therefore, \\(K e^{-r\\tau} N(d_2)\\) is simply the expected cost of having to pay the strike price at expiration, discounted to today. * \\(N(d_1)\\) is the option’s Delta (\\(\\Delta\\)). It represents the probability-weighted expected value of the stock, conditional on the option expiring in the money. * Therefore, the formula simply says: Call Price = (Expected Benefit of receiving the stock) - (Expected Cost of paying the strike)."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "The content on this website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n\n\nCreative Commons License Logo"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Jordan's Projects",
    "section": "",
    "text": "hello"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I’m [Your Name]. This is where you can write a more detailed biography…"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Contact \n\n\n\n&lt;label for=\"full-name\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Full Name&lt;/label&gt;\n&lt;input type=\"text\" id=\"full-name\" name=\"name\" required style=\"width: 100%; padding: 0.5em;\"&gt;\n\n\n&lt;label for=\"email-address\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Email Address&lt;/label&gt;\n&lt;input type=\"email\" id=\"email-address\" name=\"_replyto\" required style=\"width: 100%; padding: 0.5em;\"&gt;\n\n\n&lt;label for=\"message\" style=\"display: block; margin-bottom: 0.5em;\"&gt;Message&lt;/label&gt;\n&lt;textarea id=\"message\" name=\"message\" rows=\"6\" required style=\"width: 100%; padding: 0.5em;\"&gt;&lt;/textarea&gt;\n\n\nSend Message"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jordan Chong",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Email\n  \n  \n    \n     CV\n  \n\n  \n  \nStudied at Singapore Management University (SMU), Bachelor of Business Administration (Finance) with Second Major in Data Science.\nPreviously worked in Debt Capital Markets at DBS Bank and Advanced Manufacturing Development at EDB.\nCurrently, I am focused on building robust pricing models, researching market microstructure, and applying ML algorithms to financial datasets.\n\n\n\n\n\nFeatured Projects\n\nView all projects →\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingapore Insurance Strategy Dashboard\n\n\n\nPython\n\nStreamlit\n\nActuarial Science\n\nData Visualization\n\nMonte Carlo\n\nfeatured\n\n\n\nA Streamlit-powered actuarial dashboard that pits ‘Buy Term, Invest the Difference’ against Whole Life plans, accounting for human behavior and stochastic risk.\n\n\n\n\n\n\n\n\n\n\n\n\nGap Challenge Solver\n\n\n\nComputer Vision\n\nPython\n\nStreamlit\n\nAlgorithms\n\nfeatured\n\n\n\nEvolution of a Computer Vision pipeline: From brittle Template Matching to robust SVMs, integrated with a Backtracking solver.\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production\n\n\n\nQuantitative Finance\n\nVolatility Modeling\n\nPython\n\nOOP\n\nStreamlit\n\nOptimization\n\nfeatured\n\n\n\nA comprehensive case study bridging Quantitative Research and Software Engineering. From calibrating Heston and Bates models using noisy S&P 500 tick data to deploying an…\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Technical Notes",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nConcept: The Black-Scholes Model\n\n\n\nStochastic Calculus\n\nOptions Pricing\n\nQuantitative Finance\n\n\n\nA complete mathematical derivation of the Black-Scholes call option formula, from Geometric Brownian Motion to the final PDE and Feynman-Kac solution.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html",
    "href": "projects/gap-challenge-solver/index.html",
    "title": "Gap Challenge Solver",
    "section": "",
    "text": "Source idea credited to Dianyi Yang: kv9898/gap_challenge_solver\nYou can try the exercises yourself here:\n- Gap Challenge\n- Gap Challenge Practice"
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#the-challenge",
    "href": "projects/gap-challenge-solver/index.html#the-challenge",
    "title": "Gap Challenge Solver",
    "section": "The Challenge",
    "text": "The Challenge\nThe “Gap” puzzle is a logic game where players must fill a missing cell in a grid, ensuring no symbol repeats in any row or column. My goal was to build a tool that could solve this instantly from a simple screenshot.\n\n\n\n\n\n\n4x4 Grid\n\n\n\n\n\n\n\n5x5 Grid\n\n\n\n\n\n\nGap Challenge - Each shape can only appear in a row or column once. The player must use logic by elimination to find the shape that belongs in the question mark.\n\n\n\nWhile the logic puzzle itself is a standard Constraint Satisfaction Problem, digitizing the grid from a raw image proved to be the real engineering challenge. This post documents the three major iterations (“Acts”) of my Computer Vision pipeline and the lessons learned from each:\n\nAct 1: Naive Template Matching\n\nAct 2: Geometric Determination of Shapes\n\nAct 3: Image Classification with SVM\n\n\nMajor Problem Faced with Image Recognition\nA major issue was the different forms that the shapes could take based on different Gap Challenge applications. Some shapes were hollow, some backgrounds were black and some bounding grids were just lines.\n\n\n\n\n\n\nNormal Version\n\n\n\n\n\n\n\nLine Version\n\n\n\n\n\n\n\nDark-mode Version\n\n\n\n\n\n\nNote the difference in shapes and grid colors. The hard part was building a robust solution that would be able to handle all these different versions."
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#act-i-the-naive-approach-template-matching",
    "href": "projects/gap-challenge-solver/index.html#act-i-the-naive-approach-template-matching",
    "title": "Gap Challenge Solver",
    "section": "Act I: The Naive Approach (Template Matching)",
    "text": "Act I: The Naive Approach (Template Matching)\nMy first instinct was to use Template Matching. I cropped reference images of each shape (Star, Circle, Square) and used cv2.matchTemplate to slide them over the screenshot, looking for pixel-perfect matches. I tried to account for the different size of the screenshots as well, scaling the reference shapes to match the screenshots.\n\n\n\nReference Images used for template matching\n\n\n\n\n\nHeatmap matching of the reference images. Green boxes show a &gt;90% match score against the reference image.\n\n\n\nThe Cause of Failure\nThis method worked well on the base example but failed when tested on other forms of the puzzle. Although I implemented multi-scale matching to handle zoom levels, the approach hit a hard ceiling when the puzzle’s visual style changed.\nWhen the game switched to “Dark Mode” or used “Hollow” shapes, my solid-colored templates failed immediately. To fix this, I would have needed to crop and save a new template for every possible theme variation.\n\n\n\n\n\n\nFailure Case 1: Hollow Shapes\n\n\n\n\n\n\n\nFailure Case 2: Dark Mode\n\n\n\n\n\n\nLesson: Hard-coding pixel comparisons doesn’t work if the inputs vary significantly. I needed a more robust method and decided to try and use the geometric differences in shapes to differentiate them."
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#act-ii-the-geometric-pivot-heuristics",
    "href": "projects/gap-challenge-solver/index.html#act-ii-the-geometric-pivot-heuristics",
    "title": "Gap Challenge Solver",
    "section": "Act II: The Geometric Pivot (Heuristics)",
    "text": "Act II: The Geometric Pivot (Heuristics)\nFor the second iteration, I moved to Feature Extraction. Instead of matching images, I used cv2.findContours to extract the shapes and analyze their geometric properties.\nI built a Decision Tree based on metrics like:\n- Vertices: cv2.approxPolyDP (e.g., 3 vertices = Triangle).\n- Solidity: Area divided by Convex Hull Area (distinguishes solid Squares from hollow Crosses).\n- Circularity: \\(4\\pi \\times \\frac{Area}{Perimeter^2}\\) (distinguishes Circles from Triangles).\n\nThe “Live Scanner” Debugger\nTo refine my thresholds, I built a visualization tool that calculated these metrics for every cell so that I could debug the errors efficiently.\n\n\n\nThe Logic Dashboard. Watching the metrics update in real-time allowed me to tune the decision tree.\n\n\n\n\nThe Bottleneck\nWhile robust against scale, this method suffered from “Whack-a-Mole” logic, where more issues would pop-up after fixing one and I realised I was trying to fix symptoms instead of fixing the root cause of the recognition issues:\n- The “Hollow” Edge Case: A hollow square has low solidity, confusing the algorithm.\n- The “Question Mark” Problem: The question mark consists of two separate blobs (hook + dot), breaking the single-contour logic.\n\n\n\n\n\n\nFailure Case: Hollow Shapes\n\n\n\n\n\n\n\nFailure Case: Dark Mode\n\n\n\n\n\nThe “Whack-a-Mole” logic resulted in decision trees like:\n# Attempts to handle edge cases manually\nif vertices &gt; 8 and circularity &lt; 0.6 and num_blobs == 1:\n    return \"Star\"\nelif num_blobs == 2:\n    return \"Question\"\nThis resulted in more and more rules that I wasn’t even sure of and I decided a needed a system to learn these rules itself."
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#act-iii-the-robust-solution-support-vector-machines",
    "href": "projects/gap-challenge-solver/index.html#act-iii-the-robust-solution-support-vector-machines",
    "title": "Gap Challenge Solver",
    "section": "Act III: The Robust Solution (Support Vector Machines)",
    "text": "Act III: The Robust Solution (Support Vector Machines)\nIn the final phase, I treated this as a classification problem. I chose a Support Vector Machine (SVM) because they perform exceptionally well on high-dimensional binary data (pixel maps) with smaller datasets.\n\n1. The Data Pipeline (Solving Distribution Shift)\nA mistake that I made at the start was training on “perfect” data but testing on “messy” data.\nI manually took multiple screenshots of the grids, with minor variations, to create a large train dataset for the SVM.\nHowever, I realised that the SVM was not performing well, because the cropping method used for the test data was different from the perfectly cropped screenshots i was taking for the train data.\nTo fix this, I used the test cropping algorithm to build an automated pipeline (collect_feedback_data.py) that extracted thousands of training examples directly from screenshots.\nThis ensured my training data included all the real-world noise (grid lines, anti-aliasing) that the model would see in production. I utilized Otsu’s Binarization here to automatically find the optimal threshold for separating shapes from the background, solving the “Dark Mode” issue.\n\n\n\nAutomatic cropping and extraction of train data and grids for the SVM to train on.\n\n\n\n\n2. The “Universal Translator” (Preprocessing)\nTo make the model immune to lighting changes and user cropping errors, I engineered a standardization pipeline: 1. Resize to 64x64.\n2. Corner Difference Strategy: Calculate the average background color from the image corners and subtract it. This isolates the shape regardless of whether the theme is Dark or Light.\n3. Centering: Find the bounding box of the shape and center it on a black canvas.\n\n\n3. Fine-Tuning the Brain (Grid Search)\nAn SVM is only as good as its hyperparameters. Default settings often lead to overfitting, where the model memorizes the training data but fails on new inputs.\nTo solve this, I implemented a Grid Search to exhaustively test combinations of C (Regularization) and Gamma (Kernel Coefficient). This allowed me to mathematically find the configuration that ignored minor pixel noise while maintaining a distinct decision boundary between similar shapes (like the Circle and the hollow Ring).\n\n\n4. X-Ray Vision (Verification)\nMachine Learning models can be “black boxes.” To trust the system, I built a side-by-side debugger comparing the Human View (RGB) against the Machine View (Preprocessed). This allowed me to catch preprocessing errors—like the dot of the ‘?’ being filtered out as noise—before retraining the model.\n\n\n\nSVM Debugger. Left is the game input; Right is what the model actually sees."
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#the-solver-logic-backtracking",
    "href": "projects/gap-challenge-solver/index.html#the-solver-logic-backtracking",
    "title": "Gap Challenge Solver",
    "section": "The Solver Logic (Backtracking)",
    "text": "The Solver Logic (Backtracking)\nWith a reliable Vision pipeline returning a 2D matrix (e.g., [['Star', 'Empty'], ['Circle', 'Square']]), the problem reduced to a standard algorithm.\nI implemented a Recursive Backtracking solver. It works by:\n1. Scanning for an empty cell.\n2. Guessing a shape from the available options.\n3. Validating row/column constraints.\n4. Recursively attempting to solve the rest of the grid.\n5. Backtracking if a guess leads to a contradiction.\ndef solve_with_backtracking(board, universe):\n    # Base Case: No empty spots left\n    if not find_empty(board): return True \n    \n    row, col = find_empty(board)\n    \n    for shape in universe:\n        if is_valid_move(board, shape, row, col):\n            board[row][col] = shape\n            # Recursive Step\n            if solve_with_backtracking(board, universe): return True\n            # Backtrack\n            board[row][col] = 'blank'\n            \n    return False"
  },
  {
    "objectID": "projects/gap-challenge-solver/index.html#conclusion-tech-stack",
    "href": "projects/gap-challenge-solver/index.html#conclusion-tech-stack",
    "title": "Gap Challenge Solver",
    "section": "Conclusion & Tech Stack",
    "text": "Conclusion & Tech Stack\nThis project was a lesson in selecting the right tool for the job. While deep learning (CNNs) could have solved this, it would have been overkill for a 5-class problem. An SVM combined with robust Data Engineering provided a lightweight, instant, and highly accurate solution (99% accuracy on test set).\n\nLanguage: Python 3.10\nVision: OpenCV (Canny, Otsu’s Binarization, Contours)\nML: Scikit-Learn (SVM, GridSearch)\nFrontend: Streamlit"
  },
  {
    "objectID": "projects/quant-structuring-workbench/index.html#the-full-stack-quant-philosophy",
    "href": "projects/quant-structuring-workbench/index.html#the-full-stack-quant-philosophy",
    "title": "End-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production",
    "section": "The “Full-Stack Quant” Philosophy",
    "text": "The “Full-Stack Quant” Philosophy\nIn the financial industry, there is often a strict divide between Quantitative Researchers (who derive the math and calibrate models in isolated Jupyter notebooks) and Quantitative Developers (who translate that math into scalable, object-oriented code for the trading desk).\nThis project was built to bridge that exact gap. It is divided into two distinct phases, documenting the iterative journey from raw mathematical theory to a live, interactive production environment.\n\n🚀 Launch Live Structuring App"
  },
  {
    "objectID": "projects/quant-structuring-workbench/index.html#phase-i-the-quantitative-research-lab-mathematics-calibration",
    "href": "projects/quant-structuring-workbench/index.html#phase-i-the-quantitative-research-lab-mathematics-calibration",
    "title": "End-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production",
    "section": "Phase I: The Quantitative Research Lab (Mathematics & Calibration)",
    "text": "Phase I: The Quantitative Research Lab (Mathematics & Calibration)\nBefore an options pricing engine can be trusted by a structuring desk, it must be validated against historical market data. The goal of this phase was to calibrate three advanced models—Merton Jump-Diffusion, Heston Stochastic Volatility, and the Bates (SVJ) Model—to raw S&P 500 options data from the Wharton Research Data Services (WRDS).\n\nIteration 1: Navigating the Data Pipeline\nReal market data is inherently noisy and structurally idiosyncratic. Building the MarketDataLoader required several critical formatting iterations: 1. The WRDS Strike Quirk: WRDS stores strike prices multiplied by 1,000. Initial pricing runs returned catastrophic NaNs because a $4,780 strike was being read as $4,780,000. A normalization layer (chain['strike_price'].values / 1000.0) had to be injected. 2. Trimming the Fat: An option chain contains thousands of highly illiquid, deep Out-of-the-Money strikes with zero bids. Feeding these into an optimizer guarantees failure. I implemented a dynamic filtering mechanism (strike_bound_pct=0.15) to isolate only the highly liquid strikes within a 15% radius of the Spot price, ensuring the optimizer only trained on pure market signal.\n\n\nIteration 2: The Heston Model and The Gibbs Phenomenon\nThe most mathematically demanding phase of the project was implementing the Heston model’s Characteristic Function. Unlike Black-Scholes, Heston requires complex Fourier inversion to evaluate the pricing integral.\n\nThe Bug: During the initial calibration, the plotted Implied Volatility curve exhibited violent, unnatural sine-wave artifacts on the extreme wings of the smile.\nThe Diagnosis: This was a classic manifestation of the Gibbs Phenomenon . The scipy.integrate.quad function was truncating the integration of the probability wave too early (at an upper limit of 100). Because short-dated options (\\(T = 0.10\\)) have very slowly decaying characteristic functions, chopping the math off artificially created literal ripples in the pricing space.\nThe Fix: I re-engineered the integration block, pushing limit_max to 2000 and tightening the absolute and relative error tolerances (epsabs=1e-4, epsrel=1e-4). The artifacts vanished, resulting in a flawless, continuous pricing curve.\n\n\n\nIteration 3: Overcoming the 20-Minute Bottleneck\nWith the math corrected, I deployed a Hybrid Optimizer: a Global Differential Evolution algorithm to scout the 5-dimensional parameter space, followed by a Local L-BFGS-B sniper to pinpoint the exact local minimum.\n\nThe Bottleneck: The initial run took over 21 minutes to calibrate. Profiling the math engine revealed the “Silent Killer” of quant optimization: nested root-finding loops. To calculate the Mean Squared Error (MSE), the objective function had to calculate a Heston price, and then run a Black-Scholes root-finder (Brent’s Method) to reverse-engineer the IV for all 350 strikes, thousands of times.\nThe 10x Speed Fix: Recognizing that a strike at 4780 provides the exact same structural information as a strike at 4785, I implemented a data Downsampling Algorithm (target_strikes[::sample_step]). By forcing the optimizer to train on 35 representative strikes instead of 350, calibration time plummeted from 21 minutes to under 2 minutes with zero loss in curve fidelity.\n\n\n\nIteration 4: The Bates Model & Parameter Redundancy\nThe “Final Boss” of the research phase was the Bates (SVJ) model, which merges Heston’s stochastic volatility with Merton’s Poisson jumps. * Parameter Seeding: To optimize this 8-parameter monster, I seeded the algorithm with the winning parameters from the individual Heston and Merton runs, allowing it to calibrate in under 60 seconds. * The Over-parameterization Trap: The optimizer revealed a fascinating quantitative reality. It forced the average jump size (\\(\\mu_J\\)) to exactly \\(0.0\\). Because Heston’s correlation parameter (\\(\\rho = -0.65\\)) was already dragging the left side of the skew upward sufficiently, the optimizer deemed the Merton jumps mathematically redundant for this specific 30-day slice. This served as a masterclass in the dangers of over-parameterization in quantitative modeling.\n\n\n\n\n\n\nNotePart I: The Theoretical Frameworks (The “Textbook”)\n\n\n\nBefore touching real data, I built Monte Carlo simulators to visually prove the mathematical intuition behind each model.\n\nModel 1: Black-Scholes & The Flaw of Constant Volatility\n\nModel 2: Merton Jump-Diffusion & Market Shocks\n\nModel 3: The Heston Model & The Leverage Effect (\\(\\rho\\))\n\nModel 4: The Bates Model (SVJ) - The Holy Grail\n\n\n\n\n\n\n\n\n\n\nTipPart II: Real Market Calibration (The “Lab”)\n\n\n\nIngesting raw WRDS data for SPX options, I built objective functions to reverse-engineer Implied Volatilities and minimize pricing errors.\n\nCalibrating Black-Scholes (Proving the Disconnect)\n\nCalibrating Merton (Fitting the Short-Term Skew)\n\nCalibrating Heston (Hybrid Global-to-Local Search)\n\nCalibrating Bates (Parameter Seeding)\n\n\n\n\n\n\n\n\n\n\nImportantPart III: Synthesis & Predictive Power\n\n\n\nCurve-fitting today’s data is easy; predicting tomorrow’s is the true test of a quant.\n\nThe Master Comparison: Heston vs. Merton vs. Bates\n\nTime-Series Out-of-Sample (OOS) Testing"
  },
  {
    "objectID": "projects/quant-structuring-workbench/index.html#phase-ii-the-interactive-structuring-desk-deployment",
    "href": "projects/quant-structuring-workbench/index.html#phase-ii-the-interactive-structuring-desk-deployment",
    "title": "End-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production",
    "section": "Phase II: The Interactive Structuring Desk (Deployment)",
    "text": "Phase II: The Interactive Structuring Desk (Deployment)\nA robust math engine is useless if a trader cannot interact with it. Phase II focused on abstracting the math into a scalable software architecture and deploying it via Streamlit.\n\nIteration 1: The Object-Oriented Refactor\nJupyter notebooks run linearly, which makes them terrible for building dynamic portfolios. To solve this, I entirely refactored the pricing engine using Object-Oriented Programming (OOP). * Abstract Base Classes: I designed a core Instrument class enforcing strict polymorphic protocols (price(), payoff()). * Encapsulation: Subclasses like ZeroCouponBond and EuropeanOption inherited this base, encapsulating their specific Black-Scholes or intrinsic math. This allows the application to aggregate entirely different asset classes natively without complex if/else routing.\n\n\nIteration 2: Session State and Dynamic Construction\nA structuring desk needs to build multi-leg positions (e.g., Iron Condors, Strangles). * Using Streamlit’s st.session_state, I engineered a persistent memory ledger. Users can continuously inject new Instrument objects into their portfolio. * The backend loops through the ledger in real-time, dynamically aggregating the individual payoff profiles into a net portfolio matrix, plotting the exact PnL horizon instantly.\n\n\nIteration 3: 3D Volatility Surface Visualization\nTraders rely on visual heuristics to identify mispricings across the term structure. * I integrated plotly.graph_objects to render a high-performance WebGL 3D Volatility Surface. * Fuzzy Matching: To allow traders to inspect specific maturity slices, I implemented a 2D Smile extractor. Because floating-point math often causes mismatch errors (e.g., \\(T=0.1000001\\)), I utilized np.isclose to fuzzy-match the slider input to the underlying DataFrame, ensuring the 2D chart rendered perfectly every time."
  },
  {
    "objectID": "projects/quant-structuring-workbench/index.html#conclusion",
    "href": "projects/quant-structuring-workbench/index.html#conclusion",
    "title": "End-to-End Quantitative Options Pricing: From WRDS Research to Interactive Production",
    "section": "Conclusion",
    "text": "Conclusion\nThis workbench stands as a complete, end-to-end demonstration of the quantitative lifecycle. It proves the ability to not only write complex Fourier transformations and navigate local optimization minima but to immediately pivot into software engineering, encapsulating that research into a live, Object-Oriented application ready for a global markets floor."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_02_MertonJumpDiffusion.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_02_MertonJumpDiffusion.html",
    "title": "2. Merton-Jump Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n\n2. Merton Jump: Adds “jumps” to the asset price to model market shocks.\nCaptures “Fat Tails” and sudden crashes via Poisson jumps.\n\n3. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the “Smirk” or “Skew” via stochastic vol—essential for pricing OTM puts accurately."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_02_MertonJumpDiffusion.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_02_MertonJumpDiffusion.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "title": "2. Merton-Jump Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n\n2. Merton Jump: Adds “jumps” to the asset price to model market shocks.\nCaptures “Fat Tails” and sudden crashes via Poisson jumps.\n\n3. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the “Smirk” or “Skew” via stochastic vol—essential for pricing OTM puts accurately."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_02_MertonJumpDiffusion.html#objective-capturing-market-shocks-and-fat-tails",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_02_MertonJumpDiffusion.html#objective-capturing-market-shocks-and-fat-tails",
    "title": "2. Merton-Jump Model",
    "section": "2.1 Objective: Capturing Market Shocks and “Fat Tails”",
    "text": "2.1 Objective: Capturing Market Shocks and “Fat Tails”\nThe Black-Scholes model assumes that stock prices follow a continuous, smooth path (Geometric Brownian Motion). However, in reality, markets experience sudden, discontinuous shocks—earnings surprises, macroeconomic news, or overnight crashes.\nRobert Merton (1976) extended the Black-Scholes model by adding a Poisson Jump Process to the continuous diffusion.\n\n2.1.1 The Mathematical Intuition\nThe asset price dynamics under MJD are described by: \\[\\frac{dS_t}{S_t} = (\\mu - \\lambda k) dt + \\sigma dW_t + (J_t - 1) dN_t\\]\nWhere: 1. The Continuous Part (\\(\\sigma dW_t\\)): The standard Black-Scholes daily noise (Brownian motion). 2. The Jump Counter (\\(dN_t\\)): A Poisson process. Most days it is 0. Occasionally, it “fires” and equals 1. The frequency of jumps is governed by \\(\\lambda\\) (expected jumps per year). 3. The Jump Size (\\(J_t\\)): When a jump occurs, how big is it? Merton assumes the jump sizes are log-normally distributed, with a mean jump size (\\(\\mu_j\\)) and a jump variance (\\(\\delta^2\\)).\nBy adding these random jumps, the Merton model naturally creates the “Fat Tails” and “Left Skew” seen in real-world S&P 500 return distributions. Let’s visualize this difference."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_02_MertonJumpDiffusion.html#imports-and-setup",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_02_MertonJumpDiffusion.html#imports-and-setup",
    "title": "2. Merton-Jump Model",
    "section": "2.2 Imports and Setup",
    "text": "2.2 Imports and Setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom scipy.integrate import quad\nimport seaborn as sns\n\n# Set plotting style\nsns.set_style(\"darkgrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Global Parameters (Toggles)\nS0 = 100.0    # Spot Price\nK_list = np.linspace(80, 120, 50) # Range of Strikes for plotting\nT = 1.0       # Time to Maturity (1 year)\nr = 0.05      # Risk-free rate\nq = 0.0       # Dividend yield\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# --- Simulation Parameters ---\nS0 = 100       # Initial Stock Price\nT = 1.0        # Time to maturity (1 Year)\nr = 0.05       # Risk-free rate\nsigma = 0.15   # Continuous Volatility (15%)\nsteps = 252    # Trading days in a year\nn_paths = 5    # Number of paths to visualize\ndt = T / steps\n\n# --- Merton Jump Parameters ---\nlam = 3.0      # Expect 3 jumps per year\nmu_j = -0.15   # Average jump size is a 15% drop (Negative skew)\ndelta = 0.10   # Volatility of the jump size\n\n# Random seeds for reproducibility\nnp.random.seed(42)\n\n# 1. Generate Random Variables\nZ = np.random.standard_normal((steps, n_paths)) # Brownian Motion\nN = np.random.poisson(lam * dt, (steps, n_paths)) # Poisson Jump Counter\nJ = np.random.normal(mu_j, delta, (steps, n_paths)) # Log-Normal Jump Sizes\n\n# 2. Calculate Returns\n# Black-Scholes Returns\nret_bs = (r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z\n\n# Merton Returns (Must subtract the compensator to prevent arbitrage)\nk = np.exp(mu_j + 0.5 * delta**2) - 1\nret_merton = (r - lam * k - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z + N * J\n\n# 3. Build Price Paths\nS_bs = np.zeros((steps + 1, n_paths))\nS_merton = np.zeros((steps + 1, n_paths))\nS_bs[0], S_merton[0] = S0, S0\n\nfor t in range(1, steps + 1):\n    S_bs[t] = S_bs[t-1] * np.exp(ret_bs[t-1])\n    S_merton[t] = S_merton[t-1] * np.exp(ret_merton[t-1])\n\n# --- Plot the Paths ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\n# Plot Black-Scholes\naxes[0].plot(S_bs, alpha=0.8, linewidth=1.5)\naxes[0].set_title(\"Black-Scholes (Continuous Diffusion)\", fontsize=13)\naxes[0].set_ylabel(\"Asset Price\")\naxes[0].set_xlabel(\"Trading Days\")\naxes[0].grid(True, linestyle='--', alpha=0.5)\n\n# Plot Merton\naxes[1].plot(S_merton, alpha=0.8, linewidth=1.5)\naxes[1].set_title(f\"Merton Jump Diffusion ($\\lambda={lam}$, $\\mu_J={mu_j}$)\", fontsize=13)\naxes[1].set_xlabel(\"Trading Days\")\naxes[1].grid(True, linestyle='--', alpha=0.5)\n\nplt.suptitle(\"Price Path Comparison: The Impact of Poisson Jumps\", fontsize=16, y=1.05)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# --- Simulate 10,000 paths to analyze the Distribution ---\nn_sims = 10000\n\nZ_dist = np.random.standard_normal((steps, n_sims))\nN_dist = np.random.poisson(lam * dt, (steps, n_sims))\nJ_dist = np.random.normal(mu_j, delta, (steps, n_sims))\n\nret_bs_dist = (r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z_dist\nret_merton_dist = (r - lam * k - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z_dist + N_dist * J_dist\n\n# Sum the daily log returns to get the 1-Year Total Return\ntotal_ret_bs = np.sum(ret_bs_dist, axis=0)\ntotal_ret_merton = np.sum(ret_merton_dist, axis=0)\n\n# --- Plot the Return Distributions ---\nplt.figure(figsize=(10, 6))\n\n# Plot Histograms\nplt.hist(total_ret_bs, bins=100, alpha=0.6, density=True, color='blue', label='Black-Scholes (Normal)')\nplt.hist(total_ret_merton, bins=100, alpha=0.5, density=True, color='red', label='Merton (Jump Diffusion)')\n\nplt.title(\"1-Year Log Return Distribution: Fat Tails & Skewness\", fontsize=14)\nplt.xlabel(\"Total Log Return\")\nplt.ylabel(\"Frequency / Probability Density\")\nplt.axvline(0, color='black', linestyle='--', alpha=0.5)\n\n# Highlight the \"Crash\" zone (Left Tail)\nplt.axvspan(-1.0, -0.4, color='red', alpha=0.1, label='The \"Fat Tail\" (Crash Zone)')\n\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.4)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Import our math engine tools\nfrom quant_math_engine import merton_jump_call, implied_volatility\n\n# --- 1. Define Theoretical Market Parameters ---\nS0 = 100.0\nT = 0.25      # 3 months to expiration (Jumps are highly visible in short-term options)\nr = 0.05\nq = 0.0\n\n# Define a range of strikes from Deep OTM Puts (80) to Deep OTM Calls (120)\ntheoretical_strikes = np.linspace(80, 120, 40)\n\n# --- 2. Define Baseline vs Jump Parameters ---\n# Black-Scholes assumes a flat, constant 15% volatility\nsigma_baseline = 0.15 \n\n# Merton assumes the same 15% baseline, PLUS the risk of an overnight crash\nlam = 1.0     # 1 jump per year\nmu_j = -0.20  # The average jump is a 20% market crash\ndelta = 0.15  # Volatility of the jump size\n\nprint(\"Calculating Theoretical Merton Prices and Implied Volatilities...\")\nprint(\"Note: Reversing prices back into IV requires heavy root-finding!\")\n\n# --- 3. Calculate Prices and Convert to IV ---\nmerton_ivs = []\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for K in theoretical_strikes:\n        # Step A: Get the theoretical Merton Price\n        m_price = merton_jump_call(S0, K, T, r, q, sigma_baseline, lam, mu_j, delta)\n        \n        # Step B: Force that price through a Black-Scholes root-finder to extract the IV\n        m_iv = implied_volatility(m_price, S0, K, T, r, q)\n        merton_ivs.append(m_iv)\n\nmerton_ivs = np.array(merton_ivs) * 100 # Convert to percentage\n\n# --- 4. Plot the Theoretical Disconnect ---\nplt.figure(figsize=(10, 6))\n\n# Plot the flat Black-Scholes assumption\nplt.axhline(sigma_baseline * 100, color='blue', linestyle='--', linewidth=2, \n            label=f'Black-Scholes (Constant {sigma_baseline*100:.0f}%)')\n\n# Plot the resulting Merton Volatility Smile\nplt.plot(theoretical_strikes, merton_ivs, color='red', linewidth=3, \n         label='Merton Jump Diffusion (Theoretical Skew)')\n\n# Formatting\nplt.axvline(S0, color='black', linestyle=':', label=f'Spot Price (S0 = {S0})')\nplt.title(\"Why Merton Invented Jumps: The Theoretical Volatility Skew\", fontsize=14)\nplt.xlabel(\"Strike Price\", fontsize=12)\nplt.ylabel(\"Implied Volatility (%)\", fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\nCalculating Theoretical Merton Prices and Implied Volatilities...\nNote: Reversing prices back into IV requires heavy root-finding!"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_04_BatesModel.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_04_BatesModel.html",
    "title": "4. Bates Model",
    "section": "",
    "text": "We have seen that Merton perfectly captures the short-term skew (by adding sudden jumps/crashes), but struggles with long-term options. We have seen that Heston perfectly captures the long-term skew (using mean-reverting stochastic volatility), but struggles to bend the curve enough for short-term crashes without using absurd parameters.\nDavid Bates (1996) combined them into the SVJ Model (Stochastic Volatility with Jumps).\n\n\nThe Bates model simply takes the two Heston SDEs and adds the Merton Poisson jump counter directly to the asset price process:\n\nThe Asset Price Process (Heston + Merton): \\[\\frac{dS_t}{S_t} = (\\mu - \\lambda k) dt + \\sqrt{v_t} dW_t^S + (J_t - 1) dN_t\\]\nThe Variance Process (Standard Heston): \\[dv_t = \\kappa(\\theta - v_t)dt + \\xi \\sqrt{v_t} dW_t^v\\]\n\nBecause the Heston diffusion process and the Merton jump process are mathematically independent, Bates realized you could simply multiply their Characteristic Functions together. This created a single, unified model that can price the entire Volatility Surface (short-term and long-term) flawlessly.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Bates Simulation Parameters ---\nS0 = 100.0\nv0 = 0.04       # Initial Variance (20% Vol)\nkappa = 2.0     # Speed of mean reversion\ntheta = 0.04    # Long-term variance\nxi = 0.5        # Volatility of Volatility\nrho = -0.7      # Negative correlation (Leverage effect)\nr = 0.05\nT = 1.0         # 1 Year\nsteps = 252     # Trading days\ndt = T / steps\nn_paths = 3     \n\n# Jump Parameters (Merton)\nlam = 2.0       # 2 jumps per year\nmu_j = -0.15    # Average jump is a 15% drop\ndelta = 0.10    # Volatility of the jump size\n\nnp.random.seed(42)\n\n# 1. Generate Correlated Brownian Motions (Heston part)\nZ1 = np.random.standard_normal((steps, n_paths))\nZ2 = np.random.standard_normal((steps, n_paths))\nZ_S = Z1\nZ_v = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n\n# 2. Generate Jumps (Merton part)\nN = np.random.poisson(lam * dt, (steps, n_paths))\nJ = np.random.normal(mu_j, delta, (steps, n_paths))\nk = np.exp(mu_j + 0.5 * delta**2) - 1\n\n# 3. Setup Path Arrays\nS = np.zeros((steps + 1, n_paths))\nv = np.zeros((steps + 1, n_paths))\nS[0] = S0\nv[0] = v0\n\n# 4. Euler-Maruyama Integration \nfor t in range(1, steps + 1):\n    # Variance Process (Heston)\n    v_prev = np.maximum(v[t-1], 0)\n    dv = kappa * (theta - v_prev) * dt + xi * np.sqrt(v_prev) * np.sqrt(dt) * Z_v[t-1]\n    v[t] = np.maximum(v_prev + dv, 0)\n    \n    # Asset Price Process (Heston Diffusion + Merton Jumps)\n    continuous_return = (r - lam * k) * dt + np.sqrt(v_prev) * np.sqrt(dt) * Z_S[t-1]\n    jump_return = N[t-1] * J[t-1]\n    \n    S[t] = S[t-1] * np.exp(continuous_return + jump_return)\n\n# --- Plotting the SVJ Paths ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n\n# Plot Asset Price\nax1.plot(S, linewidth=1.5)\nax1.set_title(rf\"Bates (SVJ) Asset Price Paths ($\\lambda={lam}$ jumps/yr)\", fontsize=13) \nax1.set_ylabel(\"Asset Price ($S_t$)\")\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Plot Variance\nax2.plot(v, linewidth=1.5)\nax2.axhline(theta, color='black', linestyle='--', label=rf'Long-Term Mean ($\\theta={theta}$)') \nax2.set_title(rf\"Stochastic Variance Paths ($\\rho={rho}$)\", fontsize=13)\nax2.set_xlabel(\"Trading Days\")\nax2.set_ylabel(\"Variance ($v_t$)\")\nax2.legend()\nax2.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_04_BatesModel.html#objective-the-holy-grail-of-classical-option-pricing",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_04_BatesModel.html#objective-the-holy-grail-of-classical-option-pricing",
    "title": "4. Bates Model",
    "section": "",
    "text": "We have seen that Merton perfectly captures the short-term skew (by adding sudden jumps/crashes), but struggles with long-term options. We have seen that Heston perfectly captures the long-term skew (using mean-reverting stochastic volatility), but struggles to bend the curve enough for short-term crashes without using absurd parameters.\nDavid Bates (1996) combined them into the SVJ Model (Stochastic Volatility with Jumps).\n\n\nThe Bates model simply takes the two Heston SDEs and adds the Merton Poisson jump counter directly to the asset price process:\n\nThe Asset Price Process (Heston + Merton): \\[\\frac{dS_t}{S_t} = (\\mu - \\lambda k) dt + \\sqrt{v_t} dW_t^S + (J_t - 1) dN_t\\]\nThe Variance Process (Standard Heston): \\[dv_t = \\kappa(\\theta - v_t)dt + \\xi \\sqrt{v_t} dW_t^v\\]\n\nBecause the Heston diffusion process and the Merton jump process are mathematically independent, Bates realized you could simply multiply their Characteristic Functions together. This created a single, unified model that can price the entire Volatility Surface (short-term and long-term) flawlessly.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Bates Simulation Parameters ---\nS0 = 100.0\nv0 = 0.04       # Initial Variance (20% Vol)\nkappa = 2.0     # Speed of mean reversion\ntheta = 0.04    # Long-term variance\nxi = 0.5        # Volatility of Volatility\nrho = -0.7      # Negative correlation (Leverage effect)\nr = 0.05\nT = 1.0         # 1 Year\nsteps = 252     # Trading days\ndt = T / steps\nn_paths = 3     \n\n# Jump Parameters (Merton)\nlam = 2.0       # 2 jumps per year\nmu_j = -0.15    # Average jump is a 15% drop\ndelta = 0.10    # Volatility of the jump size\n\nnp.random.seed(42)\n\n# 1. Generate Correlated Brownian Motions (Heston part)\nZ1 = np.random.standard_normal((steps, n_paths))\nZ2 = np.random.standard_normal((steps, n_paths))\nZ_S = Z1\nZ_v = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n\n# 2. Generate Jumps (Merton part)\nN = np.random.poisson(lam * dt, (steps, n_paths))\nJ = np.random.normal(mu_j, delta, (steps, n_paths))\nk = np.exp(mu_j + 0.5 * delta**2) - 1\n\n# 3. Setup Path Arrays\nS = np.zeros((steps + 1, n_paths))\nv = np.zeros((steps + 1, n_paths))\nS[0] = S0\nv[0] = v0\n\n# 4. Euler-Maruyama Integration \nfor t in range(1, steps + 1):\n    # Variance Process (Heston)\n    v_prev = np.maximum(v[t-1], 0)\n    dv = kappa * (theta - v_prev) * dt + xi * np.sqrt(v_prev) * np.sqrt(dt) * Z_v[t-1]\n    v[t] = np.maximum(v_prev + dv, 0)\n    \n    # Asset Price Process (Heston Diffusion + Merton Jumps)\n    continuous_return = (r - lam * k) * dt + np.sqrt(v_prev) * np.sqrt(dt) * Z_S[t-1]\n    jump_return = N[t-1] * J[t-1]\n    \n    S[t] = S[t-1] * np.exp(continuous_return + jump_return)\n\n# --- Plotting the SVJ Paths ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n\n# Plot Asset Price\nax1.plot(S, linewidth=1.5)\nax1.set_title(rf\"Bates (SVJ) Asset Price Paths ($\\lambda={lam}$ jumps/yr)\", fontsize=13) \nax1.set_ylabel(\"Asset Price ($S_t$)\")\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Plot Variance\nax2.plot(v, linewidth=1.5)\nax2.axhline(theta, color='black', linestyle='--', label=rf'Long-Term Mean ($\\theta={theta}$)') \nax2.set_title(rf\"Stochastic Variance Paths ($\\rho={rho}$)\", fontsize=13)\nax2.set_xlabel(\"Trading Days\")\nax2.set_ylabel(\"Variance ($v_t$)\")\nax2.legend()\nax2.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_06_Heston_RealDataCalibration.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_06_Heston_RealDataCalibration.html",
    "title": "6. Heston Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import differential_evolution, minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# 2. Load Data (Only reads Parquet once)\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Market IVs\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"✅ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Ready! Targeting 352 liquid strikes."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_06_Heston_RealDataCalibration.html#setup-importing-and-defining-the-state",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_06_Heston_RealDataCalibration.html#setup-importing-and-defining-the-state",
    "title": "6. Heston Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import differential_evolution, minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# 2. Load Data (Only reads Parquet once)\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Market IVs\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"✅ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Ready! Targeting 352 liquid strikes."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_06_Heston_RealDataCalibration.html#calibration-of-the-heston-model",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_06_Heston_RealDataCalibration.html#calibration-of-the-heston-model",
    "title": "6. Heston Model Real Data",
    "section": "2 Calibration of the Heston Model",
    "text": "2 Calibration of the Heston Model\n\n\nCode\nsample_step = max(1, len(valid_strikes) // 40) \n\ntarget_strikes_sampled = valid_strikes[::sample_step]\ntarget_ivs_sampled = target_ivs[::sample_step]\n\nprint(f\"Downsampled from {len(valid_strikes)} to {len(target_strikes_sampled)} strikes for 10x faster optimization.\")\n\n# --- UPDATE YOUR OBJECTIVE FUNCTION TO USE THE SAMPLED ARRAYS ---\ndef heston_objective(params):\n    v0, kappa, theta, xi, rho = params\n    error = 0.0\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # Loop through the SMALL array now\n        for i, K in enumerate(target_strikes_sampled):\n            m_price = heston_call_price(S0, K, T, r, q, v0, kappa, theta, xi, rho)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs_sampled[i])**2\n                \n    return error / len(target_strikes_sampled)\n\nheston_bounds = [(0.01, 0.15), (0.5, 5.0), (0.01, 0.15), (0.05, 1.5), (-0.95, -0.4)]\n\nprint(\"Optimizing Heston Parameters (Hybrid Search)...\")\nstart_time = time.time()\n\n# Stage 1: Global Scout\nres_global = differential_evolution(heston_objective, heston_bounds, popsize=8, maxiter=15, seed=42)\n\n# Stage 2: Local Sniper\nres_heston = minimize(heston_objective, res_global.x, method='L-BFGS-B', bounds=heston_bounds)\n\nprint(f\"✅ Finished in {round(time.time() - start_time, 2)}s\")\nprint(f\"Optimal Parameters: {res_heston.x}\")\nprint(f\"Mean Squared Error: {res_heston.fun:.6f}\")\n\n\nDownsampled from 352 to 44 strikes for 10x faster optimization.\nOptimizing Heston Parameters (Hybrid Search)...\n✅ Finished in 144.73s\nOptimal Parameters: [ 0.0146198   4.48319131  0.03773232  1.22309101 -0.63401892]\nMean Squared Error: 0.000040"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_06_Heston_RealDataCalibration.html#visualising-the-model-against-live-data",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_06_Heston_RealDataCalibration.html#visualising-the-model-against-live-data",
    "title": "6. Heston Model Real Data",
    "section": "3 Visualising the Model against Live Data",
    "text": "3 Visualising the Model against Live Data\n\n\nCode\nprint(\"Generating Heston Volatility Smile...\")\n\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nheston_prices = [heston_call_price(S0, k, T, r, q, *res_heston.x) for k in smooth_strikes]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    heston_iv = [implied_volatility(p, S0, k, T, r, q) for p, k in zip(heston_prices, smooth_strikes)]\n\nvalid_idx = ~np.isnan(heston_iv)\nclean_strikes = np.array(smooth_strikes)[valid_idx]\nclean_iv = np.array(heston_iv)[valid_idx] * 100\n\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\nplt.plot(clean_strikes, clean_iv, color='red', label='Heston Fit', linewidth=2.5)\n\nplt.title(f\"Heston Stochastic Volatility Calibration\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nGenerating Heston Volatility Smile..."
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_08_Bates_RealDataCalibration.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_08_Bates_RealDataCalibration.html",
    "title": "8. Bates Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bates_call_price, implied_volatility\n\n# 2. Load Data (Only reads Parquet once)\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Market IVs\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"✅ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Ready! Targeting 352 liquid strikes.\n\n\n\n\nCode\n# --- THE SPEED FIX: Downsample the data ---\nsample_step = max(1, len(valid_strikes) // 40) \ntarget_strikes_sampled = valid_strikes[::sample_step]\ntarget_ivs_sampled = target_ivs[::sample_step]\n\nprint(f\"Downsampled from {len(valid_strikes)} to {len(target_strikes_sampled)} strikes for 10x faster optimization.\")\n\n# --- BATES OBJECTIVE FUNCTION ---\ndef bates_objective(params):\n    v0, kappa, theta, xi, rho, lam, mu_j, delta = params\n    error = 0.0\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # Loop through the SMALL array to save massive amounts of time\n        for i, K in enumerate(target_strikes_sampled):\n            m_price = bates_call_price(S0, K, T, r, q, v0, kappa, theta, xi, rho, lam, mu_j, delta)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs_sampled[i])**2\n                \n    return error / len(target_strikes_sampled)\n\n# Parameter Seeding: [v0, kappa, theta, xi, rho, lam, mu_j, delta]\n# We start with your winning parameters from the Heston and Merton runs!\nbates_guess = [0.0115, 3.0293, 0.0684, 1.1963, -0.6572, 0.9923, -0.0825, 0.0779]\n\nbates_bounds = [\n    (0.005, 0.15), (0.5, 5.0), (0.005, 0.15), (0.05, 1.5), (-0.95, -0.2), # Heston components\n    (0.0, 3.0), (-0.5, 0.0), (0.01, 0.3)                                  # Merton components\n]\n\nprint(\"Optimizing Bates Parameters (Seeded Fast Search)...\")\nstart_time = time.time()\nres_bates = minimize(bates_objective, bates_guess, method='L-BFGS-B', bounds=bates_bounds)\n\nprint(f\"✅ Finished in {round(time.time() - start_time, 2)}s\")\nprint(f\"Optimal Parameters: {res_bates.x}\")\nprint(f\"Mean Squared Error: {res_bates.fun:.6f}\")\n\n\nDownsampled from 352 to 44 strikes for 10x faster optimization.\nOptimizing Bates Parameters (Seeded Fast Search)...\n✅ Finished in 138.9s\nOptimal Parameters: [ 0.005       3.03227026  0.10706195  1.49104011 -0.67036382  1.0118486\n  0.          0.0220192 ]\nMean Squared Error: 0.000034\n\n\n\n\nCode\nprint(\"Generating Bates Volatility Smile...\")\n\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nbates_prices = [bates_call_price(S0, k, T, r, q, *res_bates.x) for k in smooth_strikes]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    bates_iv = [implied_volatility(p, S0, k, T, r, q) for p, k in zip(bates_prices, smooth_strikes)]\n\nvalid_idx = ~np.isnan(bates_iv)\nclean_strikes = np.array(smooth_strikes)[valid_idx]\nclean_iv = np.array(bates_iv)[valid_idx] * 100\n\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\nplt.plot(clean_strikes, clean_iv, color='green', label='Bates (SVJ) Fit', linewidth=2.5)\n\nplt.title(f\"Bates (SVJ) Model Calibration\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n# ==========================================\n# SAVE THE FINAL PARAMETERS FOR NOTEBOOK 09\n# ==========================================\nprint(\"\\n\" + \"=\"*50)\nprint(\"🏆 FINAL BATES PARAMETERS DICTIONARY (Copy this!)\")\nprint(\"=\"*50)\nprint(\"bates_params = {\")\nprint(f\"    'v0': {res_bates.x[0]:.4f},\")\nprint(f\"    'kappa': {res_bates.x[1]:.4f},\")\nprint(f\"    'theta': {res_bates.x[2]:.4f},\")\nprint(f\"    'xi': {res_bates.x[3]:.4f},\")\nprint(f\"    'rho': {res_bates.x[4]:.4f},\")\nprint(f\"    'lam': {res_bates.x[5]:.4f},\")\nprint(f\"    'mu_j': {res_bates.x[6]:.4f},\")\nprint(f\"    'delta': {res_bates.x[7]:.4f}\")\nprint(\"}\")\n\n\nGenerating Bates Volatility Smile...\n\n\n\n\n\n\n\n\n\n\n==================================================\n🏆 FINAL BATES PARAMETERS DICTIONARY (Copy this!)\n==================================================\nbates_params = {\n    'v0': 0.0050,\n    'kappa': 3.0323,\n    'theta': 0.1071,\n    'xi': 1.4910,\n    'rho': -0.6704,\n    'lam': 1.0118,\n    'mu_j': 0.0000,\n    'delta': 0.0220\n}"
  },
  {
    "objectID": "projects/quant-structuring-workbench/notebooks/quarto_10_OutOfSampleTesting.html",
    "href": "projects/quant-structuring-workbench/notebooks/quarto_10_OutOfSampleTesting.html",
    "title": "10. Out-of-sample Testing for Models",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bates_call_price, implied_volatility\n\n# 2. Load Data into RAM\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nprint(\"✅ Base Data Loaded. Ready for Out-Of-Sample Testing.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n✅ Data Loaded Successfully.\n✅ Base Data Loaded. Ready for Out-Of-Sample Testing.\n\n\n\n\nCode\n# --- 1. Define Dates ---\nIN_SAMPLE_DATE = '2024-01-10'    # The day we trained the model\nOOS_DATE = '2024-01-11'          # The \"Future\" day we are testing (1 day later)\nTARGET_EXDATE = '2024-02-16'     # The same expiration contract\n\nprint(f\"Stepping forward in time from {IN_SAMPLE_DATE} to {OOS_DATE}...\")\n\n# --- 2. Load the Locked-In Parameters (Trained on Jan 10) ---\n# We keep these exactly as they were calibrated yesterday!\nlocked_bates_params = {\n    'v0': 0.0135, \n    'kappa': 3.0290, \n    'theta': 0.0530, \n    'xi': 1.1920, \n    'rho': -0.6481,\n    'lam': 0.9871,\n    'mu_j': 0.0,\n    'delta': 0.0187\n}\n\n# --- 3. Pull the \"Future\" Market State (Jan 11) ---\noos_state = loader.get_market_state(OOS_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\noos_S0, oos_T, oos_r, oos_q = oos_state['S0'], oos_state['T'], oos_state['r'], oos_state['q']\noos_market_strikes, oos_market_prices = oos_state['strikes'], oos_state['prices']\n\n# Calculate the actual Jan 11 Market IVs\noos_target_ivs, oos_valid_strikes = [], []\nfor i, K in enumerate(oos_market_strikes):\n    iv = implied_volatility(oos_market_prices[i], oos_S0, K, oos_T, oos_r, oos_q)\n    if not np.isnan(iv):\n        oos_target_ivs.append(iv)\n        oos_valid_strikes.append(K)\n\noos_valid_strikes = np.array(oos_valid_strikes)\noos_target_ivs = np.array(oos_target_ivs)\n\nprint(f\"✅ Future State Acquired: Spot Price moved to {oos_S0} | Time remaining: {oos_T:.4f} years\")\n\n\nStepping forward in time from 2024-01-10 to 2024-01-11...\n✅ Future State Acquired: Spot Price moved to 4780.24 | Time remaining: 0.0986 years\n\n\n\n\nCode\nprint(\"Generating Out-Of-Sample Predictions using locked parameters...\")\n\nsmooth_strikes = np.linspace(min(oos_valid_strikes), max(oos_valid_strikes), 50)\noos_predicted_prices = []\noos_predicted_ivs = []\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    # Generate the smooth prediction line for the chart\n    for K in smooth_strikes:\n        # Note: We use oos_S0 and oos_T, but locked_bates_params!\n        p_pred = bates_call_price(oos_S0, K, oos_T, oos_r, oos_q, **locked_bates_params)\n        iv_pred = implied_volatility(p_pred, oos_S0, K, oos_T, oos_r, oos_q)\n        oos_predicted_ivs.append(iv_pred * 100)\n    \n    # Generate the specific predictions to calculate the OOS Error\n    oos_error = 0.0\n    for i, K in enumerate(oos_valid_strikes):\n        p_exact = bates_call_price(oos_S0, K, oos_T, oos_r, oos_q, **locked_bates_params)\n        iv_exact = implied_volatility(p_exact, oos_S0, K, oos_T, oos_r, oos_q)\n        \n        if not np.isnan(iv_exact):\n            oos_error += (iv_exact - oos_target_ivs[i])**2\n\noos_mse = oos_error / len(oos_valid_strikes)\nprint(f\"✅ Predictions complete. Out-Of-Sample MSE: {oos_mse:.6f}\")\n\n\nGenerating Out-Of-Sample Predictions using locked parameters...\n✅ Predictions complete. Out-Of-Sample MSE: 0.000043\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\n\n# Plot the NEW Market Data for Day 2\nplt.scatter(oos_valid_strikes, oos_target_ivs * 100, color='black', label=f'Actual Market IV on {OOS_DATE}', marker='x', s=50)\n\n# Plot the Model's PREDICTION based on Day 1's training\nplt.plot(smooth_strikes, oos_predicted_ivs, color='green', linewidth=3, label=f'Bates SVJ Prediction (Trained on {IN_SAMPLE_DATE})')\n\n# Formatting\nplt.axvline(oos_S0, color='gray', linestyle=':', label=f'New Spot Price (S0 = {oos_S0})')\nplt.title(f\"Out-of-Sample Testing: Bates Model Predictive Power\\nPredicting {OOS_DATE} prices using {IN_SAMPLE_DATE} parameters\", fontsize=14)\nplt.xlabel(\"Strike Price\", fontsize=12)\nplt.ylabel(\"Implied Volatility (%)\", fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "readings/East of Eden/template.html#summary",
    "href": "readings/East of Eden/template.html#summary",
    "title": "East of Eden",
    "section": "Summary",
    "text": "Summary\nA multi-generational saga retelling the biblical story of Cain and Abel…"
  },
  {
    "objectID": "readings/East of Eden/template.html#key-takeaways",
    "href": "readings/East of Eden/template.html#key-takeaways",
    "title": "East of Eden",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nTimshel: The Hebrew word meaning “Thou Mayest.” It signifies that humans have the choice to overcome sin; we are not condemned by our nature.\nInheritance: We are not bound by the sins of our fathers."
  },
  {
    "objectID": "readings/East of Eden/template.html#favorite-quotes",
    "href": "readings/East of Eden/template.html#favorite-quotes",
    "title": "East of Eden",
    "section": "Favorite Quotes",
    "text": "Favorite Quotes\n\n“And now that you don’t have to be perfect, you can be good.”"
  },
  {
    "objectID": "readings/East of Eden/template.html#reflection",
    "href": "readings/East of Eden/template.html#reflection",
    "title": "East of Eden",
    "section": "Reflection",
    "text": "Reflection\nThis book shifted my perspective on agency. In a deterministic world (or in a quantitative model), we assume inputs dictate outputs. Steinbeck argues for the “error term”—the human capacity to choose against the grain.c"
  }
]