[
  {
    "objectID": "quarto_10_OutOfSampleTesting.html",
    "href": "quarto_10_OutOfSampleTesting.html",
    "title": "10. Out-of-sample Testing for Models",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bates_call_price, implied_volatility\n\n# 2. Load Data into RAM\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nprint(\"‚úÖ Base Data Loaded. Ready for Out-Of-Sample Testing.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Base Data Loaded. Ready for Out-Of-Sample Testing.\n\n\n\n\nCode\n# --- 1. Define Dates ---\nIN_SAMPLE_DATE = '2024-01-10'    # The day we trained the model\nOOS_DATE = '2024-01-11'          # The \"Future\" day we are testing (1 day later)\nTARGET_EXDATE = '2024-02-16'     # The same expiration contract\n\nprint(f\"Stepping forward in time from {IN_SAMPLE_DATE} to {OOS_DATE}...\")\n\n# --- 2. Load the Locked-In Parameters (Trained on Jan 10) ---\n# We keep these exactly as they were calibrated yesterday!\nlocked_bates_params = {\n    'v0': 0.0135, \n    'kappa': 3.0290, \n    'theta': 0.0530, \n    'xi': 1.1920, \n    'rho': -0.6481,\n    'lam': 0.9871,\n    'mu_j': 0.0,\n    'delta': 0.0187\n}\n\n# --- 3. Pull the \"Future\" Market State (Jan 11) ---\noos_state = loader.get_market_state(OOS_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\noos_S0, oos_T, oos_r, oos_q = oos_state['S0'], oos_state['T'], oos_state['r'], oos_state['q']\noos_market_strikes, oos_market_prices = oos_state['strikes'], oos_state['prices']\n\n# Calculate the actual Jan 11 Market IVs\noos_target_ivs, oos_valid_strikes = [], []\nfor i, K in enumerate(oos_market_strikes):\n    iv = implied_volatility(oos_market_prices[i], oos_S0, K, oos_T, oos_r, oos_q)\n    if not np.isnan(iv):\n        oos_target_ivs.append(iv)\n        oos_valid_strikes.append(K)\n\noos_valid_strikes = np.array(oos_valid_strikes)\noos_target_ivs = np.array(oos_target_ivs)\n\nprint(f\"‚úÖ Future State Acquired: Spot Price moved to {oos_S0} | Time remaining: {oos_T:.4f} years\")\n\n\nStepping forward in time from 2024-01-10 to 2024-01-11...\n‚úÖ Future State Acquired: Spot Price moved to 4780.24 | Time remaining: 0.0986 years\n\n\n\n\nCode\nprint(\"Generating Out-Of-Sample Predictions using locked parameters...\")\n\nsmooth_strikes = np.linspace(min(oos_valid_strikes), max(oos_valid_strikes), 50)\noos_predicted_prices = []\noos_predicted_ivs = []\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    # Generate the smooth prediction line for the chart\n    for K in smooth_strikes:\n        # Note: We use oos_S0 and oos_T, but locked_bates_params!\n        p_pred = bates_call_price(oos_S0, K, oos_T, oos_r, oos_q, **locked_bates_params)\n        iv_pred = implied_volatility(p_pred, oos_S0, K, oos_T, oos_r, oos_q)\n        oos_predicted_ivs.append(iv_pred * 100)\n    \n    # Generate the specific predictions to calculate the OOS Error\n    oos_error = 0.0\n    for i, K in enumerate(oos_valid_strikes):\n        p_exact = bates_call_price(oos_S0, K, oos_T, oos_r, oos_q, **locked_bates_params)\n        iv_exact = implied_volatility(p_exact, oos_S0, K, oos_T, oos_r, oos_q)\n        \n        if not np.isnan(iv_exact):\n            oos_error += (iv_exact - oos_target_ivs[i])**2\n\noos_mse = oos_error / len(oos_valid_strikes)\nprint(f\"‚úÖ Predictions complete. Out-Of-Sample MSE: {oos_mse:.6f}\")\n\n\nGenerating Out-Of-Sample Predictions using locked parameters...\n‚úÖ Predictions complete. Out-Of-Sample MSE: 0.000043\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\n\n# Plot the NEW Market Data for Day 2\nplt.scatter(oos_valid_strikes, oos_target_ivs * 100, color='black', label=f'Actual Market IV on {OOS_DATE}', marker='x', s=50)\n\n# Plot the Model's PREDICTION based on Day 1's training\nplt.plot(smooth_strikes, oos_predicted_ivs, color='green', linewidth=3, label=f'Bates SVJ Prediction (Trained on {IN_SAMPLE_DATE})')\n\n# Formatting\nplt.axvline(oos_S0, color='gray', linestyle=':', label=f'New Spot Price (S0 = {oos_S0})')\nplt.title(f\"Out-of-Sample Testing: Bates Model Predictive Power\\nPredicting {OOS_DATE} prices using {IN_SAMPLE_DATE} parameters\", fontsize=14)\nplt.xlabel(\"Strike Price\", fontsize=12)\nplt.ylabel(\"Implied Volatility (%)\", fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "quarto_08_Bates_RealDataCalibration.html",
    "href": "quarto_08_Bates_RealDataCalibration.html",
    "title": "8. Bates Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bates_call_price, implied_volatility\n\n# 2. Load Data (Only reads Parquet once)\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Market IVs\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"‚úÖ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Ready! Targeting 352 liquid strikes.\n\n\n\n\nCode\n# --- THE SPEED FIX: Downsample the data ---\nsample_step = max(1, len(valid_strikes) // 40) \ntarget_strikes_sampled = valid_strikes[::sample_step]\ntarget_ivs_sampled = target_ivs[::sample_step]\n\nprint(f\"Downsampled from {len(valid_strikes)} to {len(target_strikes_sampled)} strikes for 10x faster optimization.\")\n\n# --- BATES OBJECTIVE FUNCTION ---\ndef bates_objective(params):\n    v0, kappa, theta, xi, rho, lam, mu_j, delta = params\n    error = 0.0\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # Loop through the SMALL array to save massive amounts of time\n        for i, K in enumerate(target_strikes_sampled):\n            m_price = bates_call_price(S0, K, T, r, q, v0, kappa, theta, xi, rho, lam, mu_j, delta)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs_sampled[i])**2\n                \n    return error / len(target_strikes_sampled)\n\n# Parameter Seeding: [v0, kappa, theta, xi, rho, lam, mu_j, delta]\n# We start with your winning parameters from the Heston and Merton runs!\nbates_guess = [0.0115, 3.0293, 0.0684, 1.1963, -0.6572, 0.9923, -0.0825, 0.0779]\n\nbates_bounds = [\n    (0.005, 0.15), (0.5, 5.0), (0.005, 0.15), (0.05, 1.5), (-0.95, -0.2), # Heston components\n    (0.0, 3.0), (-0.5, 0.0), (0.01, 0.3)                                  # Merton components\n]\n\nprint(\"Optimizing Bates Parameters (Seeded Fast Search)...\")\nstart_time = time.time()\nres_bates = minimize(bates_objective, bates_guess, method='L-BFGS-B', bounds=bates_bounds)\n\nprint(f\"‚úÖ Finished in {round(time.time() - start_time, 2)}s\")\nprint(f\"Optimal Parameters: {res_bates.x}\")\nprint(f\"Mean Squared Error: {res_bates.fun:.6f}\")\n\n\nDownsampled from 352 to 44 strikes for 10x faster optimization.\nOptimizing Bates Parameters (Seeded Fast Search)...\n‚úÖ Finished in 138.9s\nOptimal Parameters: [ 0.005       3.03227026  0.10706195  1.49104011 -0.67036382  1.0118486\n  0.          0.0220192 ]\nMean Squared Error: 0.000034\n\n\n\n\nCode\nprint(\"Generating Bates Volatility Smile...\")\n\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nbates_prices = [bates_call_price(S0, k, T, r, q, *res_bates.x) for k in smooth_strikes]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    bates_iv = [implied_volatility(p, S0, k, T, r, q) for p, k in zip(bates_prices, smooth_strikes)]\n\nvalid_idx = ~np.isnan(bates_iv)\nclean_strikes = np.array(smooth_strikes)[valid_idx]\nclean_iv = np.array(bates_iv)[valid_idx] * 100\n\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\nplt.plot(clean_strikes, clean_iv, color='green', label='Bates (SVJ) Fit', linewidth=2.5)\n\nplt.title(f\"Bates (SVJ) Model Calibration\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n# ==========================================\n# SAVE THE FINAL PARAMETERS FOR NOTEBOOK 09\n# ==========================================\nprint(\"\\n\" + \"=\"*50)\nprint(\"üèÜ FINAL BATES PARAMETERS DICTIONARY (Copy this!)\")\nprint(\"=\"*50)\nprint(\"bates_params = {\")\nprint(f\"    'v0': {res_bates.x[0]:.4f},\")\nprint(f\"    'kappa': {res_bates.x[1]:.4f},\")\nprint(f\"    'theta': {res_bates.x[2]:.4f},\")\nprint(f\"    'xi': {res_bates.x[3]:.4f},\")\nprint(f\"    'rho': {res_bates.x[4]:.4f},\")\nprint(f\"    'lam': {res_bates.x[5]:.4f},\")\nprint(f\"    'mu_j': {res_bates.x[6]:.4f},\")\nprint(f\"    'delta': {res_bates.x[7]:.4f}\")\nprint(\"}\")\n\n\nGenerating Bates Volatility Smile...\n\n\n\n\n\n\n\n\n\n\n==================================================\nüèÜ FINAL BATES PARAMETERS DICTIONARY (Copy this!)\n==================================================\nbates_params = {\n    'v0': 0.0050,\n    'kappa': 3.0323,\n    'theta': 0.1071,\n    'xi': 1.4910,\n    'rho': -0.6704,\n    'lam': 1.0118,\n    'mu_j': 0.0000,\n    'delta': 0.0220\n}"
  },
  {
    "objectID": "quarto_06_Heston_RealDataCalibration.html",
    "href": "quarto_06_Heston_RealDataCalibration.html",
    "title": "6. Heston Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import differential_evolution, minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# 2. Load Data (Only reads Parquet once)\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Market IVs\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"‚úÖ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Ready! Targeting 352 liquid strikes."
  },
  {
    "objectID": "quarto_06_Heston_RealDataCalibration.html#setup-importing-and-defining-the-state",
    "href": "quarto_06_Heston_RealDataCalibration.html#setup-importing-and-defining-the-state",
    "title": "6. Heston Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import differential_evolution, minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# 2. Load Data (Only reads Parquet once)\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Market IVs\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"‚úÖ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Ready! Targeting 352 liquid strikes."
  },
  {
    "objectID": "quarto_06_Heston_RealDataCalibration.html#calibration-of-the-heston-model",
    "href": "quarto_06_Heston_RealDataCalibration.html#calibration-of-the-heston-model",
    "title": "6. Heston Model Real Data",
    "section": "2 Calibration of the Heston Model",
    "text": "2 Calibration of the Heston Model\n\n\nCode\nsample_step = max(1, len(valid_strikes) // 40) \n\ntarget_strikes_sampled = valid_strikes[::sample_step]\ntarget_ivs_sampled = target_ivs[::sample_step]\n\nprint(f\"Downsampled from {len(valid_strikes)} to {len(target_strikes_sampled)} strikes for 10x faster optimization.\")\n\n# --- UPDATE YOUR OBJECTIVE FUNCTION TO USE THE SAMPLED ARRAYS ---\ndef heston_objective(params):\n    v0, kappa, theta, xi, rho = params\n    error = 0.0\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # Loop through the SMALL array now\n        for i, K in enumerate(target_strikes_sampled):\n            m_price = heston_call_price(S0, K, T, r, q, v0, kappa, theta, xi, rho)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs_sampled[i])**2\n                \n    return error / len(target_strikes_sampled)\n\nheston_bounds = [(0.01, 0.15), (0.5, 5.0), (0.01, 0.15), (0.05, 1.5), (-0.95, -0.4)]\n\nprint(\"Optimizing Heston Parameters (Hybrid Search)...\")\nstart_time = time.time()\n\n# Stage 1: Global Scout\nres_global = differential_evolution(heston_objective, heston_bounds, popsize=8, maxiter=15, seed=42)\n\n# Stage 2: Local Sniper\nres_heston = minimize(heston_objective, res_global.x, method='L-BFGS-B', bounds=heston_bounds)\n\nprint(f\"‚úÖ Finished in {round(time.time() - start_time, 2)}s\")\nprint(f\"Optimal Parameters: {res_heston.x}\")\nprint(f\"Mean Squared Error: {res_heston.fun:.6f}\")\n\n\nDownsampled from 352 to 44 strikes for 10x faster optimization.\nOptimizing Heston Parameters (Hybrid Search)...\n‚úÖ Finished in 144.73s\nOptimal Parameters: [ 0.0146198   4.48319131  0.03773232  1.22309101 -0.63401892]\nMean Squared Error: 0.000040"
  },
  {
    "objectID": "quarto_06_Heston_RealDataCalibration.html#visualising-the-model-against-live-data",
    "href": "quarto_06_Heston_RealDataCalibration.html#visualising-the-model-against-live-data",
    "title": "6. Heston Model Real Data",
    "section": "3 Visualising the Model against Live Data",
    "text": "3 Visualising the Model against Live Data\n\n\nCode\nprint(\"Generating Heston Volatility Smile...\")\n\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nheston_prices = [heston_call_price(S0, k, T, r, q, *res_heston.x) for k in smooth_strikes]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    heston_iv = [implied_volatility(p, S0, k, T, r, q) for p, k in zip(heston_prices, smooth_strikes)]\n\nvalid_idx = ~np.isnan(heston_iv)\nclean_strikes = np.array(smooth_strikes)[valid_idx]\nclean_iv = np.array(heston_iv)[valid_idx] * 100\n\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\nplt.plot(clean_strikes, clean_iv, color='red', label='Heston Fit', linewidth=2.5)\n\nplt.title(f\"Heston Stochastic Volatility Calibration\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nGenerating Heston Volatility Smile..."
  },
  {
    "objectID": "quarto_04_BatesModel.html",
    "href": "quarto_04_BatesModel.html",
    "title": "4. Bates Model",
    "section": "",
    "text": "We have seen that Merton perfectly captures the short-term skew (by adding sudden jumps/crashes), but struggles with long-term options. We have seen that Heston perfectly captures the long-term skew (using mean-reverting stochastic volatility), but struggles to bend the curve enough for short-term crashes without using absurd parameters.\nDavid Bates (1996) combined them into the SVJ Model (Stochastic Volatility with Jumps).\n\n\nThe Bates model simply takes the two Heston SDEs and adds the Merton Poisson jump counter directly to the asset price process:\n\nThe Asset Price Process (Heston + Merton): \\[\\frac{dS_t}{S_t} = (\\mu - \\lambda k) dt + \\sqrt{v_t} dW_t^S + (J_t - 1) dN_t\\]\nThe Variance Process (Standard Heston): \\[dv_t = \\kappa(\\theta - v_t)dt + \\xi \\sqrt{v_t} dW_t^v\\]\n\nBecause the Heston diffusion process and the Merton jump process are mathematically independent, Bates realized you could simply multiply their Characteristic Functions together. This created a single, unified model that can price the entire Volatility Surface (short-term and long-term) flawlessly.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Bates Simulation Parameters ---\nS0 = 100.0\nv0 = 0.04       # Initial Variance (20% Vol)\nkappa = 2.0     # Speed of mean reversion\ntheta = 0.04    # Long-term variance\nxi = 0.5        # Volatility of Volatility\nrho = -0.7      # Negative correlation (Leverage effect)\nr = 0.05\nT = 1.0         # 1 Year\nsteps = 252     # Trading days\ndt = T / steps\nn_paths = 3     \n\n# Jump Parameters (Merton)\nlam = 2.0       # 2 jumps per year\nmu_j = -0.15    # Average jump is a 15% drop\ndelta = 0.10    # Volatility of the jump size\n\nnp.random.seed(42)\n\n# 1. Generate Correlated Brownian Motions (Heston part)\nZ1 = np.random.standard_normal((steps, n_paths))\nZ2 = np.random.standard_normal((steps, n_paths))\nZ_S = Z1\nZ_v = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n\n# 2. Generate Jumps (Merton part)\nN = np.random.poisson(lam * dt, (steps, n_paths))\nJ = np.random.normal(mu_j, delta, (steps, n_paths))\nk = np.exp(mu_j + 0.5 * delta**2) - 1\n\n# 3. Setup Path Arrays\nS = np.zeros((steps + 1, n_paths))\nv = np.zeros((steps + 1, n_paths))\nS[0] = S0\nv[0] = v0\n\n# 4. Euler-Maruyama Integration \nfor t in range(1, steps + 1):\n    # Variance Process (Heston)\n    v_prev = np.maximum(v[t-1], 0)\n    dv = kappa * (theta - v_prev) * dt + xi * np.sqrt(v_prev) * np.sqrt(dt) * Z_v[t-1]\n    v[t] = np.maximum(v_prev + dv, 0)\n    \n    # Asset Price Process (Heston Diffusion + Merton Jumps)\n    continuous_return = (r - lam * k) * dt + np.sqrt(v_prev) * np.sqrt(dt) * Z_S[t-1]\n    jump_return = N[t-1] * J[t-1]\n    \n    S[t] = S[t-1] * np.exp(continuous_return + jump_return)\n\n# --- Plotting the SVJ Paths ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n\n# Plot Asset Price\nax1.plot(S, linewidth=1.5)\nax1.set_title(rf\"Bates (SVJ) Asset Price Paths ($\\lambda={lam}$ jumps/yr)\", fontsize=13) \nax1.set_ylabel(\"Asset Price ($S_t$)\")\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Plot Variance\nax2.plot(v, linewidth=1.5)\nax2.axhline(theta, color='black', linestyle='--', label=rf'Long-Term Mean ($\\theta={theta}$)') \nax2.set_title(rf\"Stochastic Variance Paths ($\\rho={rho}$)\", fontsize=13)\nax2.set_xlabel(\"Trading Days\")\nax2.set_ylabel(\"Variance ($v_t$)\")\nax2.legend()\nax2.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "quarto_04_BatesModel.html#objective-the-holy-grail-of-classical-option-pricing",
    "href": "quarto_04_BatesModel.html#objective-the-holy-grail-of-classical-option-pricing",
    "title": "4. Bates Model",
    "section": "",
    "text": "We have seen that Merton perfectly captures the short-term skew (by adding sudden jumps/crashes), but struggles with long-term options. We have seen that Heston perfectly captures the long-term skew (using mean-reverting stochastic volatility), but struggles to bend the curve enough for short-term crashes without using absurd parameters.\nDavid Bates (1996) combined them into the SVJ Model (Stochastic Volatility with Jumps).\n\n\nThe Bates model simply takes the two Heston SDEs and adds the Merton Poisson jump counter directly to the asset price process:\n\nThe Asset Price Process (Heston + Merton): \\[\\frac{dS_t}{S_t} = (\\mu - \\lambda k) dt + \\sqrt{v_t} dW_t^S + (J_t - 1) dN_t\\]\nThe Variance Process (Standard Heston): \\[dv_t = \\kappa(\\theta - v_t)dt + \\xi \\sqrt{v_t} dW_t^v\\]\n\nBecause the Heston diffusion process and the Merton jump process are mathematically independent, Bates realized you could simply multiply their Characteristic Functions together. This created a single, unified model that can price the entire Volatility Surface (short-term and long-term) flawlessly.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Bates Simulation Parameters ---\nS0 = 100.0\nv0 = 0.04       # Initial Variance (20% Vol)\nkappa = 2.0     # Speed of mean reversion\ntheta = 0.04    # Long-term variance\nxi = 0.5        # Volatility of Volatility\nrho = -0.7      # Negative correlation (Leverage effect)\nr = 0.05\nT = 1.0         # 1 Year\nsteps = 252     # Trading days\ndt = T / steps\nn_paths = 3     \n\n# Jump Parameters (Merton)\nlam = 2.0       # 2 jumps per year\nmu_j = -0.15    # Average jump is a 15% drop\ndelta = 0.10    # Volatility of the jump size\n\nnp.random.seed(42)\n\n# 1. Generate Correlated Brownian Motions (Heston part)\nZ1 = np.random.standard_normal((steps, n_paths))\nZ2 = np.random.standard_normal((steps, n_paths))\nZ_S = Z1\nZ_v = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n\n# 2. Generate Jumps (Merton part)\nN = np.random.poisson(lam * dt, (steps, n_paths))\nJ = np.random.normal(mu_j, delta, (steps, n_paths))\nk = np.exp(mu_j + 0.5 * delta**2) - 1\n\n# 3. Setup Path Arrays\nS = np.zeros((steps + 1, n_paths))\nv = np.zeros((steps + 1, n_paths))\nS[0] = S0\nv[0] = v0\n\n# 4. Euler-Maruyama Integration \nfor t in range(1, steps + 1):\n    # Variance Process (Heston)\n    v_prev = np.maximum(v[t-1], 0)\n    dv = kappa * (theta - v_prev) * dt + xi * np.sqrt(v_prev) * np.sqrt(dt) * Z_v[t-1]\n    v[t] = np.maximum(v_prev + dv, 0)\n    \n    # Asset Price Process (Heston Diffusion + Merton Jumps)\n    continuous_return = (r - lam * k) * dt + np.sqrt(v_prev) * np.sqrt(dt) * Z_S[t-1]\n    jump_return = N[t-1] * J[t-1]\n    \n    S[t] = S[t-1] * np.exp(continuous_return + jump_return)\n\n# --- Plotting the SVJ Paths ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n\n# Plot Asset Price\nax1.plot(S, linewidth=1.5)\nax1.set_title(rf\"Bates (SVJ) Asset Price Paths ($\\lambda={lam}$ jumps/yr)\", fontsize=13) \nax1.set_ylabel(\"Asset Price ($S_t$)\")\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Plot Variance\nax2.plot(v, linewidth=1.5)\nax2.axhline(theta, color='black', linestyle='--', label=rf'Long-Term Mean ($\\theta={theta}$)') \nax2.set_title(rf\"Stochastic Variance Paths ($\\rho={rho}$)\", fontsize=13)\nax2.set_xlabel(\"Trading Days\")\nax2.set_ylabel(\"Variance ($v_t$)\")\nax2.legend()\nax2.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "quarto_02_MertonJumpDiffusion.html",
    "href": "quarto_02_MertonJumpDiffusion.html",
    "title": "2. Merton-Jump Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n\n2. Merton Jump: Adds ‚Äújumps‚Äù to the asset price to model market shocks.\nCaptures ‚ÄúFat Tails‚Äù and sudden crashes via Poisson jumps.\n\n3. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the ‚ÄúSmirk‚Äù or ‚ÄúSkew‚Äù via stochastic vol‚Äîessential for pricing OTM puts accurately."
  },
  {
    "objectID": "quarto_02_MertonJumpDiffusion.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "href": "quarto_02_MertonJumpDiffusion.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "title": "2. Merton-Jump Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n\n2. Merton Jump: Adds ‚Äújumps‚Äù to the asset price to model market shocks.\nCaptures ‚ÄúFat Tails‚Äù and sudden crashes via Poisson jumps.\n\n3. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the ‚ÄúSmirk‚Äù or ‚ÄúSkew‚Äù via stochastic vol‚Äîessential for pricing OTM puts accurately."
  },
  {
    "objectID": "quarto_02_MertonJumpDiffusion.html#objective-capturing-market-shocks-and-fat-tails",
    "href": "quarto_02_MertonJumpDiffusion.html#objective-capturing-market-shocks-and-fat-tails",
    "title": "2. Merton-Jump Model",
    "section": "2.1 Objective: Capturing Market Shocks and ‚ÄúFat Tails‚Äù",
    "text": "2.1 Objective: Capturing Market Shocks and ‚ÄúFat Tails‚Äù\nThe Black-Scholes model assumes that stock prices follow a continuous, smooth path (Geometric Brownian Motion). However, in reality, markets experience sudden, discontinuous shocks‚Äîearnings surprises, macroeconomic news, or overnight crashes.\nRobert Merton (1976) extended the Black-Scholes model by adding a Poisson Jump Process to the continuous diffusion.\n\n2.1.1 The Mathematical Intuition\nThe asset price dynamics under MJD are described by: \\[\\frac{dS_t}{S_t} = (\\mu - \\lambda k) dt + \\sigma dW_t + (J_t - 1) dN_t\\]\nWhere: 1. The Continuous Part (\\(\\sigma dW_t\\)): The standard Black-Scholes daily noise (Brownian motion). 2. The Jump Counter (\\(dN_t\\)): A Poisson process. Most days it is 0. Occasionally, it ‚Äúfires‚Äù and equals 1. The frequency of jumps is governed by \\(\\lambda\\) (expected jumps per year). 3. The Jump Size (\\(J_t\\)): When a jump occurs, how big is it? Merton assumes the jump sizes are log-normally distributed, with a mean jump size (\\(\\mu_j\\)) and a jump variance (\\(\\delta^2\\)).\nBy adding these random jumps, the Merton model naturally creates the ‚ÄúFat Tails‚Äù and ‚ÄúLeft Skew‚Äù seen in real-world S&P 500 return distributions. Let‚Äôs visualize this difference."
  },
  {
    "objectID": "quarto_02_MertonJumpDiffusion.html#imports-and-setup",
    "href": "quarto_02_MertonJumpDiffusion.html#imports-and-setup",
    "title": "2. Merton-Jump Model",
    "section": "2.2 Imports and Setup",
    "text": "2.2 Imports and Setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom scipy.integrate import quad\nimport seaborn as sns\n\n# Set plotting style\nsns.set_style(\"darkgrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Global Parameters (Toggles)\nS0 = 100.0    # Spot Price\nK_list = np.linspace(80, 120, 50) # Range of Strikes for plotting\nT = 1.0       # Time to Maturity (1 year)\nr = 0.05      # Risk-free rate\nq = 0.0       # Dividend yield\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# --- Simulation Parameters ---\nS0 = 100       # Initial Stock Price\nT = 1.0        # Time to maturity (1 Year)\nr = 0.05       # Risk-free rate\nsigma = 0.15   # Continuous Volatility (15%)\nsteps = 252    # Trading days in a year\nn_paths = 5    # Number of paths to visualize\ndt = T / steps\n\n# --- Merton Jump Parameters ---\nlam = 3.0      # Expect 3 jumps per year\nmu_j = -0.15   # Average jump size is a 15% drop (Negative skew)\ndelta = 0.10   # Volatility of the jump size\n\n# Random seeds for reproducibility\nnp.random.seed(42)\n\n# 1. Generate Random Variables\nZ = np.random.standard_normal((steps, n_paths)) # Brownian Motion\nN = np.random.poisson(lam * dt, (steps, n_paths)) # Poisson Jump Counter\nJ = np.random.normal(mu_j, delta, (steps, n_paths)) # Log-Normal Jump Sizes\n\n# 2. Calculate Returns\n# Black-Scholes Returns\nret_bs = (r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z\n\n# Merton Returns (Must subtract the compensator to prevent arbitrage)\nk = np.exp(mu_j + 0.5 * delta**2) - 1\nret_merton = (r - lam * k - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z + N * J\n\n# 3. Build Price Paths\nS_bs = np.zeros((steps + 1, n_paths))\nS_merton = np.zeros((steps + 1, n_paths))\nS_bs[0], S_merton[0] = S0, S0\n\nfor t in range(1, steps + 1):\n    S_bs[t] = S_bs[t-1] * np.exp(ret_bs[t-1])\n    S_merton[t] = S_merton[t-1] * np.exp(ret_merton[t-1])\n\n# --- Plot the Paths ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\n# Plot Black-Scholes\naxes[0].plot(S_bs, alpha=0.8, linewidth=1.5)\naxes[0].set_title(\"Black-Scholes (Continuous Diffusion)\", fontsize=13)\naxes[0].set_ylabel(\"Asset Price\")\naxes[0].set_xlabel(\"Trading Days\")\naxes[0].grid(True, linestyle='--', alpha=0.5)\n\n# Plot Merton\naxes[1].plot(S_merton, alpha=0.8, linewidth=1.5)\naxes[1].set_title(f\"Merton Jump Diffusion ($\\lambda={lam}$, $\\mu_J={mu_j}$)\", fontsize=13)\naxes[1].set_xlabel(\"Trading Days\")\naxes[1].grid(True, linestyle='--', alpha=0.5)\n\nplt.suptitle(\"Price Path Comparison: The Impact of Poisson Jumps\", fontsize=16, y=1.05)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# --- Simulate 10,000 paths to analyze the Distribution ---\nn_sims = 10000\n\nZ_dist = np.random.standard_normal((steps, n_sims))\nN_dist = np.random.poisson(lam * dt, (steps, n_sims))\nJ_dist = np.random.normal(mu_j, delta, (steps, n_sims))\n\nret_bs_dist = (r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z_dist\nret_merton_dist = (r - lam * k - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z_dist + N_dist * J_dist\n\n# Sum the daily log returns to get the 1-Year Total Return\ntotal_ret_bs = np.sum(ret_bs_dist, axis=0)\ntotal_ret_merton = np.sum(ret_merton_dist, axis=0)\n\n# --- Plot the Return Distributions ---\nplt.figure(figsize=(10, 6))\n\n# Plot Histograms\nplt.hist(total_ret_bs, bins=100, alpha=0.6, density=True, color='blue', label='Black-Scholes (Normal)')\nplt.hist(total_ret_merton, bins=100, alpha=0.5, density=True, color='red', label='Merton (Jump Diffusion)')\n\nplt.title(\"1-Year Log Return Distribution: Fat Tails & Skewness\", fontsize=14)\nplt.xlabel(\"Total Log Return\")\nplt.ylabel(\"Frequency / Probability Density\")\nplt.axvline(0, color='black', linestyle='--', alpha=0.5)\n\n# Highlight the \"Crash\" zone (Left Tail)\nplt.axvspan(-1.0, -0.4, color='red', alpha=0.1, label='The \"Fat Tail\" (Crash Zone)')\n\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.4)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Import our math engine tools\nfrom quant_math_engine import merton_jump_call, implied_volatility\n\n# --- 1. Define Theoretical Market Parameters ---\nS0 = 100.0\nT = 0.25      # 3 months to expiration (Jumps are highly visible in short-term options)\nr = 0.05\nq = 0.0\n\n# Define a range of strikes from Deep OTM Puts (80) to Deep OTM Calls (120)\ntheoretical_strikes = np.linspace(80, 120, 40)\n\n# --- 2. Define Baseline vs Jump Parameters ---\n# Black-Scholes assumes a flat, constant 15% volatility\nsigma_baseline = 0.15 \n\n# Merton assumes the same 15% baseline, PLUS the risk of an overnight crash\nlam = 1.0     # 1 jump per year\nmu_j = -0.20  # The average jump is a 20% market crash\ndelta = 0.15  # Volatility of the jump size\n\nprint(\"Calculating Theoretical Merton Prices and Implied Volatilities...\")\nprint(\"Note: Reversing prices back into IV requires heavy root-finding!\")\n\n# --- 3. Calculate Prices and Convert to IV ---\nmerton_ivs = []\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for K in theoretical_strikes:\n        # Step A: Get the theoretical Merton Price\n        m_price = merton_jump_call(S0, K, T, r, q, sigma_baseline, lam, mu_j, delta)\n        \n        # Step B: Force that price through a Black-Scholes root-finder to extract the IV\n        m_iv = implied_volatility(m_price, S0, K, T, r, q)\n        merton_ivs.append(m_iv)\n\nmerton_ivs = np.array(merton_ivs) * 100 # Convert to percentage\n\n# --- 4. Plot the Theoretical Disconnect ---\nplt.figure(figsize=(10, 6))\n\n# Plot the flat Black-Scholes assumption\nplt.axhline(sigma_baseline * 100, color='blue', linestyle='--', linewidth=2, \n            label=f'Black-Scholes (Constant {sigma_baseline*100:.0f}%)')\n\n# Plot the resulting Merton Volatility Smile\nplt.plot(theoretical_strikes, merton_ivs, color='red', linewidth=3, \n         label='Merton Jump Diffusion (Theoretical Skew)')\n\n# Formatting\nplt.axvline(S0, color='black', linestyle=':', label=f'Spot Price (S0 = {S0})')\nplt.title(\"Why Merton Invented Jumps: The Theoretical Volatility Skew\", fontsize=14)\nplt.xlabel(\"Strike Price\", fontsize=12)\nplt.ylabel(\"Implied Volatility (%)\", fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\nCalculating Theoretical Merton Prices and Implied Volatilities...\nNote: Reversing prices back into IV requires heavy root-finding!"
  },
  {
    "objectID": "quarto_01_BlackSholes.html",
    "href": "quarto_01_BlackSholes.html",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n\n2. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the ‚ÄúSmirk‚Äù or ‚ÄúSkew‚Äù via stochastic vol‚Äîessential for pricing OTM puts accurately.\n3. Merton Jump: Adds ‚Äújumps‚Äù to the asset price to model market shocks.\nCaptures ‚ÄúFat Tails‚Äù and sudden crashes via Poisson jumps.\n\n\n\nThe Black-Scholes model assumes the stock price \\(S_t\\) follows a Geometric Brownian Motion (GBM):\\[dS_t = \\mu S_t dt + \\sigma S_t dW_t\\]\nWe can deconstruct this engine into two distinct components: * The Deterministic Drift (\\(\\mu S_t dt\\)): This represents the expected, constant growth rate of the asset. If market volatility were zero, the stock would simply grow smoothly at rate \\(\\mu\\), compounding like money in a standard savings account.\n\nThe Stochastic Diffusion (\\(\\sigma S_t dW_t\\)): This injects the randomness. \\(dW_t\\) is a Wiener process (Standard Brownian Motion) representing unpredictable market shocks. By scaling this randomness by the current stock price (\\(S_t\\)), the model ensures that the asset price can never drop below zero, perfectly reflecting the reality of limited liability in equities. Where:\n\n\\(\\mu\\) is the drift.\n\n\\(\\sigma\\) is the constant volatility.\n\n\\(dW_t\\) is a Wiener process (Brownian motion). \n\nCall Option Formula\nThe price of a European Call option \\(C(S, t)\\) is given by:\n\\[C(S, t) = S_0 e^{-qT} N(d_1) - K e^{-rT} N(d_2)\\]  Where \\(N(\\cdot)\\) is the cumulative distribution function of the standard normal distribution, and:\n\\[d_1 = \\frac{\\ln(S_0/K) + (r - q + \\sigma^2/2)T}{\\sigma\\sqrt{T}}\\]\\[d_2 = d_1 - \\sigma\\sqrt{T}\\]\n\n\nVolatility Drag (The Asymmetry of Returns):\nBecause asset returns compound geometrically, downward price movements penalize the overall value more severely than upward movements of the exact same percentage. A 50% drop requires a 100% gain just to recover the initial capital.\nMathematically, this ‚Äúvolatility drag‚Äù manifests when we apply Ito‚Äôs Lemma to the log-return process, introducing a \\(-\\frac{1}{2}\\sigma^2\\) variance penalty term. The higher the volatility, the stronger this downward drag on the expected geometric return.\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom scipy.integrate import quad\nimport seaborn as sns\n\n# Set plotting style\nsns.set_style(\"darkgrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Global Parameters (Toggles)\nS0 = 100.0    # Spot Price\nK_list = np.linspace(80, 120, 50) # Range of Strikes for plotting\nT = 1.0       # Time to Maturity (1 year)\nr = 0.05      # Risk-free rate\nq = 0.0       # Dividend yield\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\nLoaded 352 strikes. S0: 4783.45, r: 0.0537\n\n\n\n\nCode\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# Load all massive Parquet files into this notebook's RAM\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\n\n\n\nCode\n# 1. Explicitly define your scenario variables so the plotter can see them later\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\n\n# 2. Feed them to the loader to get the exact state\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\n# 3. Extract the variables for the optimizer\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\nprint(f\"‚úÖ Loaded {len(market_strikes)} strikes. S0: {S0}, r: {r*100:.2f}%\")\n\n\n‚úÖ Loaded 352 strikes. S0: 4783.45, r: 5.37%\n\n\n\n\n\nVisualizes the core assumption of the Black-Scholes model: that asset prices follow a log-normal random walk with constant drift and volatility. This Monte Carlo simulation provides a physical intuition for the probability distribution of terminal stock prices.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# --- 1. Simulate Geometric Brownian Motion (GBM) ---\ndef simulate_gbm(S0, mu, sigma, T, dt, num_paths):\n    \"\"\"\n    Simulates asset price paths using Geometric Brownian Motion.\n    \"\"\"\n    N = int(T / dt) # Number of time steps\n    t = np.linspace(0, T, N)\n    \n    # Generate random Wiener process steps\n    W = np.random.standard_normal(size=(num_paths, N)) \n    W = np.cumsum(W, axis=1) * np.sqrt(dt) \n    \n    # Calculate the paths using the analytical solution to the GBM SDE\n    # Notice the volatility drag term: (mu - 0.5 * sigma^2)\n    X = (mu - 0.5 * sigma**2) * t + sigma * W \n    S = S0 * np.exp(X) \n    return t, S\n\n# Parameters for simulation\nS0_sim = 100      # Initial stock price\nmu_sim = 0.08     # Expected return (drift)\nsigma_sim = 0.20  # Volatility\nT_sim = 1.0       # Time horizon (1 year)\ndt_sim = 1/252    # Daily steps\nnum_paths = 15    # Number of simulated paths to plot\n\nt_steps, paths = simulate_gbm(S0_sim, mu_sim, sigma_sim, T_sim, dt_sim, num_paths)\n\nplt.figure(figsize=(10, 6))\nfor i in range(num_paths):\n    plt.plot(t_steps, paths[i, :], lw=1.5, alpha=0.7)\n    \nplt.title('Monte Carlo Simulation of Asset Paths (GBM)')\nplt.xlabel('Time (Years)')\nplt.ylabel('Asset Price ($S_t$)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nImplements the closed-form analytical solutions for European Call options. We visualize Delta against the underlying asset price to demonstrate the dynamic, non-linear nature of option sensitivities, introducing the concept of continuous delta-hedging.\nMapping of the ‚ÄúBig Five‚Äù Greeks (\\(\\Delta\\), \\(\\Gamma\\), \\(\\Theta\\), \\(\\nu\\), \\(\\rho\\)). By visualizing these simultaneously, we can analyze the multi-dimensional risk exposure of an option contract as the underlying spot price moves across different moneyness levels.\n\n\nCode\n\ndef d1(S, K, T, r, q, sigma):\n    return (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n\ndef d2(S, K, T, r, q, sigma):\n    return d1(S, K, T, r, q, sigma) - sigma * np.sqrt(T)\n\ndef bs_call_price(S, K, T, r, q, sigma):\n    if sigma &lt;= 0 or T &lt;= 0: return max(S * np.exp(-q * T) - K * np.exp(-r * T), 0)\n    return (S * np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma)) - \n            K * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma)))\n\ndef bs_call_delta(S, K, T, r, q, sigma):\n    \"\"\"Rate of change of option price with respect to underlying asset.\"\"\"\n    return np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma))\n\ndef bs_gamma(S, K, T, r, q, sigma):\n    \"\"\"Rate of change of Delta (convexity). Identical for calls and puts.\"\"\"\n    return (np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma))) / (S * sigma * np.sqrt(T))\n\ndef bs_vega(S, K, T, r, q, sigma):\n    \"\"\"Sensitivity to volatility. Identical for calls and puts.\"\"\"\n    return S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * np.sqrt(T)\n\ndef bs_call_theta(S, K, T, r, q, sigma):\n    \"\"\"Time decay of the option.\"\"\"\n    term1 = -(S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * sigma) / (2 * np.sqrt(T))\n    term2 = r * K * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma))\n    term3 = q * S * np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma))\n    return term1 - term2 + term3\n\n# --- Add Call Rho ---\ndef bs_call_rho(S, K, T, r, q, sigma):\n    \"\"\"Sensitivity to the risk-free interest rate.\"\"\"\n    return K * T * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma))\n\n# --- Plotting the Big Five Greeks ---\nS_range = np.linspace(50, 150, 100)\nK_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed = 100, 1.0, 0.05, 0.0, 0.2\n\n# Calculate Greeks\ndeltas = [bs_call_delta(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\ngammas = [bs_gamma(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nthetas = [bs_call_theta(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nvegas  = [bs_vega(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nrhos   = [bs_call_rho(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\n\n# Create subplots\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\nfig.suptitle('Call Option Greeks vs. Underlying Price ($S_t$)', fontsize=16)\n\ngreeks_data = [\n    ('Delta ($\\Delta$)', deltas, 'purple'), ('Gamma ($\\Gamma$)', gammas, 'blue'), \n    ('Theta ($\\Theta$)', thetas, 'red'), ('Vega ($\\\\nu$)', vegas, 'green'), \n    ('Rho ($\\\\rho$)', rhos, 'orange')\n]\n\nfor i, (title, data, color) in enumerate(greeks_data):\n    ax = axes[i//3, i%3]\n    ax.plot(S_range, data, color=color, lw=2)\n    ax.axvline(x=K_fixed, color='black', linestyle='--', label='Strike (K=100)')\n    ax.set_title(title)\n    ax.set_xlabel('Underlying Price')\n    ax.grid(True, alpha=0.3)\n\naxes[1, 2].axis('off') # Hide the empty 6th subplot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCompletes the basic pricing suite by adding European Put options. We run a computational check to validate our formulas against the Put-Call Parity theorem (\\(C - P = S_0 e^{-qT} - K e^{-rT}\\)), ensuring our engine respects fundamental no-arbitrage constraints.\n\n\nCode\n# --- Put Option Pricing and Greeks ---\ndef bs_put_price(S, K, T, r, q, sigma):\n    if sigma &lt;= 0 or T &lt;= 0: return max(K * np.exp(-r * T) - S * np.exp(-q * T), 0)\n    return (K * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma)) - \n            S * np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma)))\n\ndef bs_put_delta(S, K, T, r, q, sigma):\n    return -np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma))\n\ndef bs_put_theta(S, K, T, r, q, sigma):\n    term1 = -(S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * sigma) / (2 * np.sqrt(T))\n    term2 = r * K * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma))\n    term3 = q * S * np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma))\n    return term1 + term2 - term3\n\ndef bs_put_rho(S, K, T, r, q, sigma):\n    return -K * T * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma))\n\n# --- Put-Call Parity Validation ---\nC = bs_call_price(S0_sim, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed)\nP = bs_put_price(S0_sim, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed)\n\nlhs = C - P\nrhs = S0_sim * np.exp(-q_fixed * T_fixed) - K_fixed * np.exp(-r_fixed * T_fixed)\n\nprint(f\"Put-Call Parity Check:\")\nprint(f\"LHS (Call - Put): {lhs:.4f}\")\nprint(f\"RHS (Discounted S - Discounted K): {rhs:.4f}\")\nprint(f\"Difference: {abs(lhs - rhs):.1e} (Effectively Zero)\")\n\n\nPut-Call Parity Check:\nLHS (Call - Put): 4.8771\nRHS (Discounted S - Discounted K): 4.8771\nDifference: 0.0e+00 (Effectively Zero)\n\n\n\n\n\nOptions pricing is inherently multi-dimensional. The price is not just a function of the underlying asset‚Äôs price, but also its strike price and the time remaining until expiration.\nBy calculating the Black-Scholes price across a grid of different strikes and maturities, we can generate a 3D pricing surface. This visualizes how Out-Of-The-Money (OTM) options lose value rapidly as expiration approaches, while In-The-Money (ITM) options converge exactly to their intrinsic payoff.\nShifts from 2D static plots to an interactive 3D surface mapping the Call price against both Strike and Time to Maturity. This allows for a deeper geometric understanding of how time decay and moneyness compound together.\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\nimport scipy.stats as si\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# --- 1. Vectorized Black-Scholes Functions ---\ndef bs_call_price(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    d2 = d1 - vol * np.sqrt(T)\n    return S * np.exp(-q * T) * si.norm.cdf(d1) - K * np.exp(-r * T) * si.norm.cdf(d2)\n\ndef bs_call_delta(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    return np.exp(-q * T) * si.norm.cdf(d1)\n\ndef bs_gamma(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    return (np.exp(-q * T) * si.norm.pdf(d1)) / (S * vol * np.sqrt(T))\n\n# --- 2. Setup Meshgrid ---\nT_surf = np.linspace(0.01, 2.0, 50)\nK_surf = np.linspace(70, 130, 50)\nT_mesh, K_mesh = np.meshgrid(T_surf, K_surf)\n\nS_fixed = 100\nq_fixed = 0.0\n\n# --- 3. Initialize FigureWidget ---\nfig = go.FigureWidget(data=[\n    go.Surface(\n        x=K_mesh, \n        y=T_mesh, \n        z=np.zeros_like(T_mesh), # Placeholder\n        colorscale='viridis',\n        contours={\"z\": {\"show\": True, \"usecolormap\": True, \"project_z\": True}}\n    )\n])\n\n# Fixed layout - no overlapping buttons!\nfig.update_layout(\n    title='Interactive Options Surface',\n    scene=dict(xaxis_title='Strike (K)', yaxis_title='Time (T)', zaxis_title='Value'),\n    width=900, height=700,\n    margin=dict(l=0, r=0, b=0, t=50) \n)\n\n# --- 4. Create UI Controls ---\nvol_slider = widgets.FloatSlider(value=0.2, min=0.05, max=1.0, step=0.05, description='Volatility:')\nrate_slider = widgets.FloatSlider(value=0.05, min=0.0, max=0.2, step=0.01, description='Rate:')\nmetric_dropdown = widgets.Dropdown(options=['Call Price', 'Call Delta', 'Gamma'], value='Gamma', description='Metric:')\n\n# --- 5. Update Logic ---\ndef update_surface(*args):\n    vol = vol_slider.value\n    rate = rate_slider.value\n    metric = metric_dropdown.value\n    \n    if metric == 'Call Price':\n        Z_new = bs_call_price(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n    elif metric == 'Call Delta': \n        Z_new = bs_call_delta(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n    else:\n        Z_new = bs_gamma(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n        \n    with fig.batch_update():\n        fig.data[0].z = Z_new\n\n# Bind controls\nvol_slider.observe(update_surface, 'value')\nrate_slider.observe(update_surface, 'value')\nmetric_dropdown.observe(update_surface, 'value')\n\n# Initial render\nupdate_surface()\n\n# --- 6. Display Clean UI ---\ncontrols = widgets.HBox([vol_slider, rate_slider, metric_dropdown])\ndisplay(widgets.VBox([controls, fig]))\n\n\n\n\n\nWe use a Brent root-finding algorithm to back-calculate the market‚Äôs implied volatility from our observed option prices. The resulting plot proves that the Black-Scholes assumption of constant volatility is empirically false‚Äîsetting up the necessity for the advanced, volatility-skew-aware models in the subsequent notebooks.\n\n\nCode\nfrom scipy.optimize import brentq\n\n# --- Implied Volatility Solver ---\ndef implied_volatility(target_price, S, K, T, r, q, option_type='C'):\n    \"\"\"\n    Backs out the implied volatility using Brent's method.\n    \"\"\"\n    MAX_VOL = 5.0 # 500% volatility cap for solver limits\n    \n    def objective_function(sigma):\n        if option_type == 'C':\n            return bs_call_price(S, K, T, r, q, sigma) - target_price\n        else:\n            return bs_put_price(S, K, T, r, q, sigma) - target_price\n\n    try:\n        # Brent's method requires bounding the root. We search between 1% and 500% vol.\n        iv = brentq(objective_function, 1e-4, MAX_VOL)\n        return iv\n    except ValueError:\n        # Fails if the theoretical price cannot match the market price (e.g., arbitrage violations)\n        return np.nan\n\n# --- Calculate IV for Market Data ---\nmarket_ivs = []\n\nfor K, price in zip(market_strikes, market_prices):\n    iv = implied_volatility(price, S0, K, T, r, q, option_type='C')\n    market_ivs.append(iv)\n\nmarket_ivs = np.array(market_ivs)\n\n# Filter out NaNs if any arbitrage violations existed in the raw data\nvalid_idx = ~np.isnan(market_ivs)\nvalid_strikes = market_strikes[valid_idx]\nvalid_ivs = market_ivs[valid_idx]\n\n# Find the At-The-Money (ATM) volatility to act as our \"Constant BS Assumption\"\natm_idx = np.argmin(np.abs(valid_strikes - S0))\natm_vol = valid_ivs[atm_idx]\n\n# --- Plot the Volatility Skew ---\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, valid_ivs, color='blue', label='Market Implied Volatility', s=15)\nplt.axhline(y=atm_vol, color='red', linestyle='--', label=f'Constant BS Assumption (ATM Vol = {atm_vol:.2%})')\nplt.axvline(x=S0, color='black', linestyle=':', label=f'Spot Price (S0 = {S0:.2f})')\n\nplt.title(f'SPX Implied Volatility Skew/Smile\\nTarget Expiry: {TARGET_EXDATE}', fontsize=14)\nplt.xlabel('Strike Price (K)', fontsize=12)\nplt.ylabel('Implied Volatility ($\\sigma$)', fontsize=12)\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \nplt.legend()\nplt.grid(True, alpha=0.4)\nplt.show()"
  },
  {
    "objectID": "quarto_01_BlackSholes.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "href": "quarto_01_BlackSholes.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n\n2. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the ‚ÄúSmirk‚Äù or ‚ÄúSkew‚Äù via stochastic vol‚Äîessential for pricing OTM puts accurately.\n3. Merton Jump: Adds ‚Äújumps‚Äù to the asset price to model market shocks.\nCaptures ‚ÄúFat Tails‚Äù and sudden crashes via Poisson jumps."
  },
  {
    "objectID": "quarto_01_BlackSholes.html#black-scholes-model",
    "href": "quarto_01_BlackSholes.html#black-scholes-model",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "The Black-Scholes model assumes the stock price \\(S_t\\) follows a Geometric Brownian Motion (GBM):\\[dS_t = \\mu S_t dt + \\sigma S_t dW_t\\]\nWe can deconstruct this engine into two distinct components: * The Deterministic Drift (\\(\\mu S_t dt\\)): This represents the expected, constant growth rate of the asset. If market volatility were zero, the stock would simply grow smoothly at rate \\(\\mu\\), compounding like money in a standard savings account.\n\nThe Stochastic Diffusion (\\(\\sigma S_t dW_t\\)): This injects the randomness. \\(dW_t\\) is a Wiener process (Standard Brownian Motion) representing unpredictable market shocks. By scaling this randomness by the current stock price (\\(S_t\\)), the model ensures that the asset price can never drop below zero, perfectly reflecting the reality of limited liability in equities. Where:\n\n\\(\\mu\\) is the drift.\n\n\\(\\sigma\\) is the constant volatility.\n\n\\(dW_t\\) is a Wiener process (Brownian motion). \n\nCall Option Formula\nThe price of a European Call option \\(C(S, t)\\) is given by:\n\\[C(S, t) = S_0 e^{-qT} N(d_1) - K e^{-rT} N(d_2)\\]  Where \\(N(\\cdot)\\) is the cumulative distribution function of the standard normal distribution, and:\n\\[d_1 = \\frac{\\ln(S_0/K) + (r - q + \\sigma^2/2)T}{\\sigma\\sqrt{T}}\\]\\[d_2 = d_1 - \\sigma\\sqrt{T}\\]\n\n\nVolatility Drag (The Asymmetry of Returns):\nBecause asset returns compound geometrically, downward price movements penalize the overall value more severely than upward movements of the exact same percentage. A 50% drop requires a 100% gain just to recover the initial capital.\nMathematically, this ‚Äúvolatility drag‚Äù manifests when we apply Ito‚Äôs Lemma to the log-return process, introducing a \\(-\\frac{1}{2}\\sigma^2\\) variance penalty term. The higher the volatility, the stronger this downward drag on the expected geometric return."
  },
  {
    "objectID": "quarto_01_BlackSholes.html#imports-and-setup",
    "href": "quarto_01_BlackSholes.html#imports-and-setup",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom scipy.integrate import quad\nimport seaborn as sns\n\n# Set plotting style\nsns.set_style(\"darkgrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Global Parameters (Toggles)\nS0 = 100.0    # Spot Price\nK_list = np.linspace(80, 120, 50) # Range of Strikes for plotting\nT = 1.0       # Time to Maturity (1 year)\nr = 0.05      # Risk-free rate\nq = 0.0       # Dividend yield\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\nLoaded 352 strikes. S0: 4783.45, r: 0.0537\n\n\n\n\nCode\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# Load all massive Parquet files into this notebook's RAM\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\n\n\n\nCode\n# 1. Explicitly define your scenario variables so the plotter can see them later\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\n\n# 2. Feed them to the loader to get the exact state\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\n# 3. Extract the variables for the optimizer\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\nprint(f\"‚úÖ Loaded {len(market_strikes)} strikes. S0: {S0}, r: {r*100:.2f}%\")\n\n\n‚úÖ Loaded 352 strikes. S0: 4783.45, r: 5.37%"
  },
  {
    "objectID": "quarto_01_BlackSholes.html#simulating-black-scholes",
    "href": "quarto_01_BlackSholes.html#simulating-black-scholes",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "Visualizes the core assumption of the Black-Scholes model: that asset prices follow a log-normal random walk with constant drift and volatility. This Monte Carlo simulation provides a physical intuition for the probability distribution of terminal stock prices.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# --- 1. Simulate Geometric Brownian Motion (GBM) ---\ndef simulate_gbm(S0, mu, sigma, T, dt, num_paths):\n    \"\"\"\n    Simulates asset price paths using Geometric Brownian Motion.\n    \"\"\"\n    N = int(T / dt) # Number of time steps\n    t = np.linspace(0, T, N)\n    \n    # Generate random Wiener process steps\n    W = np.random.standard_normal(size=(num_paths, N)) \n    W = np.cumsum(W, axis=1) * np.sqrt(dt) \n    \n    # Calculate the paths using the analytical solution to the GBM SDE\n    # Notice the volatility drag term: (mu - 0.5 * sigma^2)\n    X = (mu - 0.5 * sigma**2) * t + sigma * W \n    S = S0 * np.exp(X) \n    return t, S\n\n# Parameters for simulation\nS0_sim = 100      # Initial stock price\nmu_sim = 0.08     # Expected return (drift)\nsigma_sim = 0.20  # Volatility\nT_sim = 1.0       # Time horizon (1 year)\ndt_sim = 1/252    # Daily steps\nnum_paths = 15    # Number of simulated paths to plot\n\nt_steps, paths = simulate_gbm(S0_sim, mu_sim, sigma_sim, T_sim, dt_sim, num_paths)\n\nplt.figure(figsize=(10, 6))\nfor i in range(num_paths):\n    plt.plot(t_steps, paths[i, :], lw=1.5, alpha=0.7)\n    \nplt.title('Monte Carlo Simulation of Asset Paths (GBM)')\nplt.xlabel('Time (Years)')\nplt.ylabel('Asset Price ($S_t$)')\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "quarto_01_BlackSholes.html#black-scholes-call-pricing-the-greeks",
    "href": "quarto_01_BlackSholes.html#black-scholes-call-pricing-the-greeks",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "Implements the closed-form analytical solutions for European Call options. We visualize Delta against the underlying asset price to demonstrate the dynamic, non-linear nature of option sensitivities, introducing the concept of continuous delta-hedging.\nMapping of the ‚ÄúBig Five‚Äù Greeks (\\(\\Delta\\), \\(\\Gamma\\), \\(\\Theta\\), \\(\\nu\\), \\(\\rho\\)). By visualizing these simultaneously, we can analyze the multi-dimensional risk exposure of an option contract as the underlying spot price moves across different moneyness levels.\n\n\nCode\n\ndef d1(S, K, T, r, q, sigma):\n    return (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n\ndef d2(S, K, T, r, q, sigma):\n    return d1(S, K, T, r, q, sigma) - sigma * np.sqrt(T)\n\ndef bs_call_price(S, K, T, r, q, sigma):\n    if sigma &lt;= 0 or T &lt;= 0: return max(S * np.exp(-q * T) - K * np.exp(-r * T), 0)\n    return (S * np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma)) - \n            K * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma)))\n\ndef bs_call_delta(S, K, T, r, q, sigma):\n    \"\"\"Rate of change of option price with respect to underlying asset.\"\"\"\n    return np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma))\n\ndef bs_gamma(S, K, T, r, q, sigma):\n    \"\"\"Rate of change of Delta (convexity). Identical for calls and puts.\"\"\"\n    return (np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma))) / (S * sigma * np.sqrt(T))\n\ndef bs_vega(S, K, T, r, q, sigma):\n    \"\"\"Sensitivity to volatility. Identical for calls and puts.\"\"\"\n    return S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * np.sqrt(T)\n\ndef bs_call_theta(S, K, T, r, q, sigma):\n    \"\"\"Time decay of the option.\"\"\"\n    term1 = -(S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * sigma) / (2 * np.sqrt(T))\n    term2 = r * K * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma))\n    term3 = q * S * np.exp(-q * T) * si.norm.cdf(d1(S, K, T, r, q, sigma))\n    return term1 - term2 + term3\n\n# --- Add Call Rho ---\ndef bs_call_rho(S, K, T, r, q, sigma):\n    \"\"\"Sensitivity to the risk-free interest rate.\"\"\"\n    return K * T * np.exp(-r * T) * si.norm.cdf(d2(S, K, T, r, q, sigma))\n\n# --- Plotting the Big Five Greeks ---\nS_range = np.linspace(50, 150, 100)\nK_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed = 100, 1.0, 0.05, 0.0, 0.2\n\n# Calculate Greeks\ndeltas = [bs_call_delta(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\ngammas = [bs_gamma(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nthetas = [bs_call_theta(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nvegas  = [bs_vega(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\nrhos   = [bs_call_rho(S, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed) for S in S_range]\n\n# Create subplots\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\nfig.suptitle('Call Option Greeks vs. Underlying Price ($S_t$)', fontsize=16)\n\ngreeks_data = [\n    ('Delta ($\\Delta$)', deltas, 'purple'), ('Gamma ($\\Gamma$)', gammas, 'blue'), \n    ('Theta ($\\Theta$)', thetas, 'red'), ('Vega ($\\\\nu$)', vegas, 'green'), \n    ('Rho ($\\\\rho$)', rhos, 'orange')\n]\n\nfor i, (title, data, color) in enumerate(greeks_data):\n    ax = axes[i//3, i%3]\n    ax.plot(S_range, data, color=color, lw=2)\n    ax.axvline(x=K_fixed, color='black', linestyle='--', label='Strike (K=100)')\n    ax.set_title(title)\n    ax.set_xlabel('Underlying Price')\n    ax.grid(True, alpha=0.3)\n\naxes[1, 2].axis('off') # Hide the empty 6th subplot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCompletes the basic pricing suite by adding European Put options. We run a computational check to validate our formulas against the Put-Call Parity theorem (\\(C - P = S_0 e^{-qT} - K e^{-rT}\\)), ensuring our engine respects fundamental no-arbitrage constraints.\n\n\nCode\n# --- Put Option Pricing and Greeks ---\ndef bs_put_price(S, K, T, r, q, sigma):\n    if sigma &lt;= 0 or T &lt;= 0: return max(K * np.exp(-r * T) - S * np.exp(-q * T), 0)\n    return (K * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma)) - \n            S * np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma)))\n\ndef bs_put_delta(S, K, T, r, q, sigma):\n    return -np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma))\n\ndef bs_put_theta(S, K, T, r, q, sigma):\n    term1 = -(S * np.exp(-q * T) * si.norm.pdf(d1(S, K, T, r, q, sigma)) * sigma) / (2 * np.sqrt(T))\n    term2 = r * K * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma))\n    term3 = q * S * np.exp(-q * T) * si.norm.cdf(-d1(S, K, T, r, q, sigma))\n    return term1 + term2 - term3\n\ndef bs_put_rho(S, K, T, r, q, sigma):\n    return -K * T * np.exp(-r * T) * si.norm.cdf(-d2(S, K, T, r, q, sigma))\n\n# --- Put-Call Parity Validation ---\nC = bs_call_price(S0_sim, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed)\nP = bs_put_price(S0_sim, K_fixed, T_fixed, r_fixed, q_fixed, sigma_fixed)\n\nlhs = C - P\nrhs = S0_sim * np.exp(-q_fixed * T_fixed) - K_fixed * np.exp(-r_fixed * T_fixed)\n\nprint(f\"Put-Call Parity Check:\")\nprint(f\"LHS (Call - Put): {lhs:.4f}\")\nprint(f\"RHS (Discounted S - Discounted K): {rhs:.4f}\")\nprint(f\"Difference: {abs(lhs - rhs):.1e} (Effectively Zero)\")\n\n\nPut-Call Parity Check:\nLHS (Call - Put): 4.8771\nRHS (Discounted S - Discounted K): 4.8771\nDifference: 0.0e+00 (Effectively Zero)"
  },
  {
    "objectID": "quarto_01_BlackSholes.html#the-options-pricing-surface",
    "href": "quarto_01_BlackSholes.html#the-options-pricing-surface",
    "title": "1. Black-Sholes Model",
    "section": "",
    "text": "Options pricing is inherently multi-dimensional. The price is not just a function of the underlying asset‚Äôs price, but also its strike price and the time remaining until expiration.\nBy calculating the Black-Scholes price across a grid of different strikes and maturities, we can generate a 3D pricing surface. This visualizes how Out-Of-The-Money (OTM) options lose value rapidly as expiration approaches, while In-The-Money (ITM) options converge exactly to their intrinsic payoff.\nShifts from 2D static plots to an interactive 3D surface mapping the Call price against both Strike and Time to Maturity. This allows for a deeper geometric understanding of how time decay and moneyness compound together.\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\nimport scipy.stats as si\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# --- 1. Vectorized Black-Scholes Functions ---\ndef bs_call_price(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    d2 = d1 - vol * np.sqrt(T)\n    return S * np.exp(-q * T) * si.norm.cdf(d1) - K * np.exp(-r * T) * si.norm.cdf(d2)\n\ndef bs_call_delta(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    return np.exp(-q * T) * si.norm.cdf(d1)\n\ndef bs_gamma(S, K, T, r, q, vol):\n    d1 = (np.log(S / K) + (r - q + 0.5 * vol**2) * T) / (vol * np.sqrt(T))\n    return (np.exp(-q * T) * si.norm.pdf(d1)) / (S * vol * np.sqrt(T))\n\n# --- 2. Setup Meshgrid ---\nT_surf = np.linspace(0.01, 2.0, 50)\nK_surf = np.linspace(70, 130, 50)\nT_mesh, K_mesh = np.meshgrid(T_surf, K_surf)\n\nS_fixed = 100\nq_fixed = 0.0\n\n# --- 3. Initialize FigureWidget ---\nfig = go.FigureWidget(data=[\n    go.Surface(\n        x=K_mesh, \n        y=T_mesh, \n        z=np.zeros_like(T_mesh), # Placeholder\n        colorscale='viridis',\n        contours={\"z\": {\"show\": True, \"usecolormap\": True, \"project_z\": True}}\n    )\n])\n\n# Fixed layout - no overlapping buttons!\nfig.update_layout(\n    title='Interactive Options Surface',\n    scene=dict(xaxis_title='Strike (K)', yaxis_title='Time (T)', zaxis_title='Value'),\n    width=900, height=700,\n    margin=dict(l=0, r=0, b=0, t=50) \n)\n\n# --- 4. Create UI Controls ---\nvol_slider = widgets.FloatSlider(value=0.2, min=0.05, max=1.0, step=0.05, description='Volatility:')\nrate_slider = widgets.FloatSlider(value=0.05, min=0.0, max=0.2, step=0.01, description='Rate:')\nmetric_dropdown = widgets.Dropdown(options=['Call Price', 'Call Delta', 'Gamma'], value='Gamma', description='Metric:')\n\n# --- 5. Update Logic ---\ndef update_surface(*args):\n    vol = vol_slider.value\n    rate = rate_slider.value\n    metric = metric_dropdown.value\n    \n    if metric == 'Call Price':\n        Z_new = bs_call_price(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n    elif metric == 'Call Delta': \n        Z_new = bs_call_delta(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n    else:\n        Z_new = bs_gamma(S_fixed, K_mesh, T_mesh, rate, q_fixed, vol)\n        \n    with fig.batch_update():\n        fig.data[0].z = Z_new\n\n# Bind controls\nvol_slider.observe(update_surface, 'value')\nrate_slider.observe(update_surface, 'value')\nmetric_dropdown.observe(update_surface, 'value')\n\n# Initial render\nupdate_surface()\n\n# --- 6. Display Clean UI ---\ncontrols = widgets.HBox([vol_slider, rate_slider, metric_dropdown])\ndisplay(widgets.VBox([controls, fig]))\n\n\n\n\n\nWe use a Brent root-finding algorithm to back-calculate the market‚Äôs implied volatility from our observed option prices. The resulting plot proves that the Black-Scholes assumption of constant volatility is empirically false‚Äîsetting up the necessity for the advanced, volatility-skew-aware models in the subsequent notebooks.\n\n\nCode\nfrom scipy.optimize import brentq\n\n# --- Implied Volatility Solver ---\ndef implied_volatility(target_price, S, K, T, r, q, option_type='C'):\n    \"\"\"\n    Backs out the implied volatility using Brent's method.\n    \"\"\"\n    MAX_VOL = 5.0 # 500% volatility cap for solver limits\n    \n    def objective_function(sigma):\n        if option_type == 'C':\n            return bs_call_price(S, K, T, r, q, sigma) - target_price\n        else:\n            return bs_put_price(S, K, T, r, q, sigma) - target_price\n\n    try:\n        # Brent's method requires bounding the root. We search between 1% and 500% vol.\n        iv = brentq(objective_function, 1e-4, MAX_VOL)\n        return iv\n    except ValueError:\n        # Fails if the theoretical price cannot match the market price (e.g., arbitrage violations)\n        return np.nan\n\n# --- Calculate IV for Market Data ---\nmarket_ivs = []\n\nfor K, price in zip(market_strikes, market_prices):\n    iv = implied_volatility(price, S0, K, T, r, q, option_type='C')\n    market_ivs.append(iv)\n\nmarket_ivs = np.array(market_ivs)\n\n# Filter out NaNs if any arbitrage violations existed in the raw data\nvalid_idx = ~np.isnan(market_ivs)\nvalid_strikes = market_strikes[valid_idx]\nvalid_ivs = market_ivs[valid_idx]\n\n# Find the At-The-Money (ATM) volatility to act as our \"Constant BS Assumption\"\natm_idx = np.argmin(np.abs(valid_strikes - S0))\natm_vol = valid_ivs[atm_idx]\n\n# --- Plot the Volatility Skew ---\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, valid_ivs, color='blue', label='Market Implied Volatility', s=15)\nplt.axhline(y=atm_vol, color='red', linestyle='--', label=f'Constant BS Assumption (ATM Vol = {atm_vol:.2%})')\nplt.axvline(x=S0, color='black', linestyle=':', label=f'Spot Price (S0 = {S0:.2f})')\n\nplt.title(f'SPX Implied Volatility Skew/Smile\\nTarget Expiry: {TARGET_EXDATE}', fontsize=14)\nplt.xlabel('Strike Price (K)', fontsize=12)\nplt.ylabel('Implied Volatility ($\\sigma$)', fontsize=12)\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \nplt.legend()\nplt.grid(True, alpha=0.4)\nplt.show()"
  },
  {
    "objectID": "quarto_03_HestonModel.html",
    "href": "quarto_03_HestonModel.html",
    "title": "3. Heston Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n2. Merton Jump: Adds ‚Äújumps‚Äù to the asset price to model market shocks.\nCaptures ‚ÄúFat Tails‚Äù and sudden crashes via Poisson jumps.\n\n3. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the ‚ÄúSmirk‚Äù or ‚ÄúSkew‚Äù via stochastic vol‚Äîessential for pricing OTM puts accurately."
  },
  {
    "objectID": "quarto_03_HestonModel.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "href": "quarto_03_HestonModel.html#overall-objective-understand-and-compare-three-fundamental-option-pricing-models.",
    "title": "3. Heston Model",
    "section": "",
    "text": "1. Black-Scholes: The benchmark model assuming constant volatility.\nProvides a great baseline and is computationally efficient, but assumes constant \\(\\sigma\\) which is unrealistic for modern markets.\n2. Merton Jump: Adds ‚Äújumps‚Äù to the asset price to model market shocks.\nCaptures ‚ÄúFat Tails‚Äù and sudden crashes via Poisson jumps.\n\n3. Heston: Adds stochastic volatility (volatility clustering and mean reversion).\nCaptures the ‚ÄúSmirk‚Äù or ‚ÄúSkew‚Äù via stochastic vol‚Äîessential for pricing OTM puts accurately."
  },
  {
    "objectID": "quarto_03_HestonModel.html#objective-capturing-the-leverage-effect-and-volatility-clustering",
    "href": "quarto_03_HestonModel.html#objective-capturing-the-leverage-effect-and-volatility-clustering",
    "title": "3. Heston Model",
    "section": "2.1 Objective: Capturing the ‚ÄúLeverage Effect‚Äù and Volatility Clustering",
    "text": "2.1 Objective: Capturing the ‚ÄúLeverage Effect‚Äù and Volatility Clustering\nThe Black-Scholes model assumes volatility is a constant number. However, looking at the VIX, we know that volatility is highly dynamic. It clusters (high vol periods follow high vol periods) and it mean-reverts (eventually calms down).\nSteven Heston (1993) solved this by modeling the asset price and its variance as two correlated, random processes:\n\n2.1.1 The Mathematical Intuition\nThe Heston model is governed by two Stochastic Differential Equations (SDEs):\n\nThe Asset Price Process: \\[\\frac{dS_t}{S_t} = \\mu dt + \\sqrt{v_t} dW_t^S\\]\nThe Variance Process (CIR Process): \\[dv_t = \\kappa(\\theta - v_t)dt + \\xi \\sqrt{v_t} dW_t^v\\]\n\nThe Parameters: * \\(v_0\\): Initial Variance. * \\(\\theta\\) (Theta): Long-term average variance. * \\(\\kappa\\) (Kappa): The rate of mean reversion (how fast vol returns to \\(\\theta\\)). * \\(\\xi\\) (Xi): Volatility of Volatility (determines the convexity of the smile). * \\(\\rho\\) (Rho): The Correlation between the two Brownian motions (\\(dW_t^S\\) and \\(dW_t^v\\)).\nThe Magic of \\(\\rho\\): In equity markets like the S&P 500, \\(\\rho\\) is heavily negative (around -0.7). When the market crashes, volatility explodes. This negative correlation is what mathematically drags the left side of the volatility smile upward, creating the ‚ÄúSkew.‚Äù\n\n\n2.1.2 Pricing via Characteristic Functions\nSince there is no simple closed-form solution like Black-Scholes, we use the Fourier Transform method. The price is an integral of the Characteristic Function \\(\\phi\\).\nImportant Note on Stability (Albrecher et al.):\nStandard implementations of the Heston characteristic function often suffer from ‚Äúbranch cut‚Äù discontinuities, leading to numerical explosions (sawtooth graphs). We use the ‚ÄúAlbrecher‚Äù representation to ensure the characteristic function remains continuous and stable."
  },
  {
    "objectID": "quarto_03_HestonModel.html#imports-and-setup",
    "href": "quarto_03_HestonModel.html#imports-and-setup",
    "title": "3. Heston Model",
    "section": "2.2 Imports and Setup",
    "text": "2.2 Imports and Setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as si\nfrom scipy.integrate import quad\nimport seaborn as sns\n\n# Set plotting style\nsns.set_style(\"darkgrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Global Parameters (Toggles)\nS0 = 100.0    # Spot Price\nK_list = np.linspace(80, 120, 50) # Range of Strikes for plotting\nT = 1.0       # Time to Maturity (1 year)\nr = 0.05      # Risk-free rate\nq = 0.0       # Dividend yield\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Heston Simulation Parameters ---\nS0 = 100.0      # Initial Stock Price\nv0 = 0.04       # Initial Variance (20% Volatility)\nkappa = 3.0     # Speed of mean reversion\ntheta = 0.04    # Long-term variance\nxi = 0.6        # Volatility of Volatility\nrho = -0.7      # Negative correlation (Equity market characteristic)\nr = 0.05\nT = 1.0         # 1 Year\nsteps = 252     # Trading days\ndt = T / steps\nn_paths = 3     # Just a few paths to keep the visual clean\n\nnp.random.seed(42)\n\n# 1. Generate Correlated Brownian Motions\n# We use Cholesky decomposition to correlate the random numbers\nZ1 = np.random.standard_normal((steps, n_paths))\nZ2 = np.random.standard_normal((steps, n_paths))\nZ_S = Z1\nZ_v = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n\n# 2. Setup Path Arrays\nS = np.zeros((steps + 1, n_paths))\nv = np.zeros((steps + 1, n_paths))\nS[0] = S0\nv[0] = v0\n\n# 3. Euler-Maruyama Integration \nfor t in range(1, steps + 1):\n    # Variance Process (with Full Truncation to prevent negative variance)\n    v_prev = np.maximum(v[t-1], 0)\n    dv = kappa * (theta - v_prev) * dt + xi * np.sqrt(v_prev) * np.sqrt(dt) * Z_v[t-1]\n    v[t] = np.maximum(v_prev + dv, 0)\n    \n    # Asset Price Process\n    dS = r * S[t-1] * dt + np.sqrt(v_prev) * S[t-1] * np.sqrt(dt) * Z_S[t-1]\n    S[t] = S[t-1] + dS\n\n# --- Plotting the Correlated Paths ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n\n# Plot Asset Price\nax1.plot(S, linewidth=1.5)\nax1.set_title(rf\"Heston Asset Price Paths ($\\rho = {rho}$)\", fontsize=13)\nax1.set_ylabel(\"Asset Price ($S_t$)\")\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Plot Variance\nax2.plot(v, linewidth=1.5)\nax2.axhline(theta, color='black', linestyle='--', label=f'Long-Term Mean ($\\theta={theta}$)')\nax2.set_title(\"Heston Variance Paths\", fontsize=13)\nax2.set_xlabel(\"Trading Days\")\nax2.set_ylabel(\"Variance ($v_t$)\")\nax2.legend()\nax2.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport warnings\nfrom quant_math_engine import heston_call_price, implied_volatility\n\n# --- Define Base Parameters ---\nS0_t = 100.0\nT_t = 0.5   # 6 months\nr_t = 0.05\nq_t = 0.0\nstrikes = np.linspace(80, 120, 30)\n\nv0_t, kappa_t, theta_t, xi_t = 0.04, 2.0, 0.04, 0.5\n\n# We will test 3 different correlation environments\ncorrelations = {\n    \"Equities (Negative Skew)\": -0.8,\n    \"FX Markets (Symmetric Smile)\": 0.0,\n    \"Commodities (Positive Skew)\": 0.8\n}\n\nprint(\"Calculating Theoretical Smiles across different markets...\")\n\nplt.figure(figsize=(11, 6))\n\nfor label, rho_test in correlations.items():\n    ivs = []\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for K in strikes:\n            # Get Heston Price\n            price = heston_call_price(S0_t, K, T_t, r_t, q_t, v0_t, kappa_t, theta_t, xi_t, rho_test)\n            # Convert to IV\n            iv = implied_volatility(price, S0_t, K, T_t, r_t, q_t)\n            ivs.append(iv * 100)\n            \n    # Plot the specific market shape\n    plt.plot(strikes, ivs, linewidth=3, label=f\"{label} ($\\\\rho = {rho_test}$)\")\n\n# Formatting\nplt.axvline(S0_t, color='black', linestyle=':', label='Spot Price (ATM)')\nplt.title(\"The Magic of $\\\\rho$: How Heston fits different asset classes\", fontsize=15)\nplt.xlabel(\"Strike Price\", fontsize=12)\nplt.ylabel(\"Implied Volatility (%)\", fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nCalculating Theoretical Smiles across different markets..."
  },
  {
    "objectID": "quarto_05_BS_RealDataCalibration.html",
    "href": "quarto_05_BS_RealDataCalibration.html",
    "title": "5. Black-Scholes Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bs_call_price, implied_volatility\n\n# 2. Load Data \nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Market IVs\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"‚úÖ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Ready! Targeting 352 liquid strikes.\n\n\n\n\nCode\n# --- BLACK-SCHOLES OBJECTIVE FUNCTION ---\ndef bs_objective(params):\n    sigma = params[0]\n    error = 0.0\n    \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for i, K in enumerate(valid_strikes):\n            # Price the option using a CONSTANT volatility\n            m_price = bs_call_price(sigma, S0, K, T, r, q)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs[i])**2\n                \n    return error / len(valid_strikes)\n\n# Guess 15% volatility\nbs_guess = [0.15] \nbs_bounds = [(0.01, 1.0)]\n\nprint(\"Optimizing Constant Black-Scholes Volatility...\")\nres_bs = minimize(bs_objective, bs_guess, method='L-BFGS-B', bounds=bs_bounds)\n\nbest_sigma = res_bs.x[0]\n\nprint(f\"‚úÖ Finished!\")\nprint(f\"Optimal Constant Volatility: {best_sigma * 100:.2f}%\")\nprint(f\"Mean Squared Error: {res_bs.fun:.6f}\")\n\n\nOptimizing Constant Black-Scholes Volatility...\n‚úÖ Finished!\nOptimal Constant Volatility: 13.94%\nMean Squared Error: 0.001505\n\n\n\n\nCode\nprint(\"Generating Black-Scholes Flat Line vs Market Smile...\")\n\n# Black-Scholes assumes IV is exactly the same across every single strike\nclean_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nbs_iv_line = np.full_like(clean_strikes, best_sigma) * 100\n\nplt.figure(figsize=(10, 6))\n\n# Plot Market Data\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\n\n# Plot Black Scholes\nplt.plot(clean_strikes, bs_iv_line, color='blue', label=f'Black-Scholes (Flat {best_sigma*100:.1f}%)', linewidth=2.5)\n\nplt.axvline(S0, color='gray', linestyle=':', label=f'Spot Price (S0 = {S0})')\nplt.title(f\"The Reality of the Market: Why Black-Scholes Fails\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nGenerating Black-Scholes Flat Line vs Market Smile..."
  },
  {
    "objectID": "quarto_07_Merton_RealDataCalibration.html",
    "href": "quarto_07_Merton_RealDataCalibration.html",
    "title": "7. Merton Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import merton_jump_call, implied_volatility # Change this import for Heston/Bates\n\n# 2. Load the data\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate target IVs for the optimizer\ntarget_ivs = []\nvalid_strikes = []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"‚úÖ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Ready! Targeting 352 liquid strikes."
  },
  {
    "objectID": "quarto_07_Merton_RealDataCalibration.html#setup-importing-and-defining-the-state",
    "href": "quarto_07_Merton_RealDataCalibration.html#setup-importing-and-defining-the-state",
    "title": "7. Merton Model Real Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport time\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# 1. Import your tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import merton_jump_call, implied_volatility # Change this import for Heston/Bates\n\n# 2. Load the data\nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate target IVs for the optimizer\ntarget_ivs = []\nvalid_strikes = []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs)\nprint(f\"‚úÖ Ready! Targeting {len(valid_strikes)} liquid strikes.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Ready! Targeting 352 liquid strikes."
  },
  {
    "objectID": "quarto_07_Merton_RealDataCalibration.html#calibration-of-the-merton-jump-model",
    "href": "quarto_07_Merton_RealDataCalibration.html#calibration-of-the-merton-jump-model",
    "title": "7. Merton Model Real Data",
    "section": "2 Calibration of the Merton-Jump Model",
    "text": "2 Calibration of the Merton-Jump Model\n\n\nCode\n# --- MERTON OBJECTIVE FUNCTION ---\ndef merton_objective(params):\n    sigma, lam, mu_j, delta = params\n    error = 0.0\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for i, K in enumerate(valid_strikes):\n            # Notice how clean this is because the math is hidden in your engine!\n            m_price = merton_jump_call(S0, K, T, r, q, sigma, lam, mu_j, delta)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs[i])**2\n                \n    return error / len(valid_strikes)\n\n# --- RUN OPTIMIZER ---\nmerton_guess = [0.15, 1.0, -0.15, 0.10] \nmerton_bounds = [(0.05, 0.30), (0.1, 5.0), (-0.5, 0.0), (0.01, 0.30)]\n\nprint(\"Optimizing Merton Parameters...\")\nstart_time = time.time()\nres_merton = minimize(merton_objective, merton_guess, method='L-BFGS-B', bounds=merton_bounds)\n\nprint(f\"‚úÖ Finished in {round(time.time() - start_time, 2)}s\")\nprint(f\"Parameters: {res_merton.x}\")\n\n\nOptimizing Merton Parameters...\n‚úÖ Finished in 62.25s\nParameters: [ 0.08582077  0.99228655 -0.0824808   0.07785328]"
  },
  {
    "objectID": "quarto_07_Merton_RealDataCalibration.html#visualising-the-model-against-live-data",
    "href": "quarto_07_Merton_RealDataCalibration.html#visualising-the-model-against-live-data",
    "title": "7. Merton Model Real Data",
    "section": "3 Visualising the Model against Live Data",
    "text": "3 Visualising the Model against Live Data\n\n\nCode\nprint(\"Generating Volatility Smile...\")\n\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nmerton_prices = [merton_jump_call(S0, k, T, r, q, *res_merton.x) for k in smooth_strikes]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    merton_iv = [implied_volatility(p, S0, k, T, r, q) for p, k in zip(merton_prices, smooth_strikes)]\n\nvalid_idx = ~np.isnan(merton_iv)\nclean_strikes = np.array(smooth_strikes)[valid_idx]\nclean_iv = np.array(merton_iv)[valid_idx] * 100\n\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\nplt.plot(clean_strikes, clean_iv, color='blue', label='Merton Fit', linewidth=2.5)\n\nplt.title(f\"Merton Jump Diffusion Calibration\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nGenerating Volatility Smile..."
  },
  {
    "objectID": "quarto_09_Model_Comparison.html",
    "href": "quarto_09_Model_Comparison.html",
    "title": "9. Model Comparison",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bs_call_price, merton_jump_call, heston_call_price, bates_call_price, implied_volatility\n\n# 2. Load Data \nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Real Market IVs for the scatter plot\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs) * 100 # Convert to percentage\n\nprint(f\"‚úÖ Market Data Loaded: {len(valid_strikes)} strikes ready for comparison.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Market Data Loaded: 352 strikes ready for comparison.\n\n\n\n\nCode\nprint(\"Loading Calibrated Parameters...\")\n\n# 1. Black-Scholes (From Notebook 04)\nbs_sigma = 0.1245  \n\n# 2. Merton Jump Diffusion (From Notebook 05)\nmerton_params = {\n    'sigma': 0.0850, \n    'lam': 0.9923, \n    'mu_j': -0.0825, \n    'delta': 0.0779\n}\n\n# 3. Heston Stochastic Volatility (From Notebook 06)\nheston_params = {\n    'v0': 0.0115, \n    'kappa': 3.0293, \n    'theta': 0.0684, \n    'xi': 1.1963, \n    'rho': -0.6572\n}\n\n# 4. Bates SVJ Model (The Ultimate Combination)\n# 4. Bates SVJ Model (The Ultimate Combination)\nbates_params = {\n    'v0': 0.0135, \n    'kappa': 3.0290, \n    'theta': 0.0530, \n    'xi': 1.1920, \n    'rho': -0.6481,\n    'lam': 0.9871,\n    'mu_j': 0.0,\n    'delta': 0.0187\n}\n\nprint(\"‚úÖ Parameters successfully loaded into memory.\")\n\n\nLoading Calibrated Parameters...\n‚úÖ Parameters successfully loaded into memory."
  },
  {
    "objectID": "quarto_09_Model_Comparison.html#setup-importing-and-defining-the-state",
    "href": "quarto_09_Model_Comparison.html#setup-importing-and-defining-the-state",
    "title": "9. Model Comparison",
    "section": "",
    "text": "Code\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n\n# 1. Import your custom engine tools\nfrom data_loader import MarketDataLoader\nfrom quant_math_engine import bs_call_price, merton_jump_call, heston_call_price, bates_call_price, implied_volatility\n\n# 2. Load Data \nBASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\nloader = MarketDataLoader(BASE_DIR)\n\nTARGET_DATE = '2024-01-10'\nTARGET_EXDATE = '2024-02-16'\nstate = loader.get_market_state(TARGET_DATE, TARGET_EXDATE, strike_bound_pct=0.10)\n\nS0, T, r, q = state['S0'], state['T'], state['r'], state['q']\nmarket_strikes, market_prices = state['strikes'], state['prices']\n\n# 3. Calculate Real Market IVs for the scatter plot\ntarget_ivs, valid_strikes = [], []\nfor i, K in enumerate(market_strikes):\n    iv = implied_volatility(market_prices[i], S0, K, T, r, q)\n    if not np.isnan(iv):\n        target_ivs.append(iv)\n        valid_strikes.append(K)\n\nvalid_strikes = np.array(valid_strikes)\ntarget_ivs = np.array(target_ivs) * 100 # Convert to percentage\n\nprint(f\"‚úÖ Market Data Loaded: {len(valid_strikes)} strikes ready for comparison.\")\n\n\nLoading Options, Spot, Yield, and Dividend Data into memory...\n‚úÖ Data Loaded Successfully.\n‚úÖ Market Data Loaded: 352 strikes ready for comparison.\n\n\n\n\nCode\nprint(\"Loading Calibrated Parameters...\")\n\n# 1. Black-Scholes (From Notebook 04)\nbs_sigma = 0.1245  \n\n# 2. Merton Jump Diffusion (From Notebook 05)\nmerton_params = {\n    'sigma': 0.0850, \n    'lam': 0.9923, \n    'mu_j': -0.0825, \n    'delta': 0.0779\n}\n\n# 3. Heston Stochastic Volatility (From Notebook 06)\nheston_params = {\n    'v0': 0.0115, \n    'kappa': 3.0293, \n    'theta': 0.0684, \n    'xi': 1.1963, \n    'rho': -0.6572\n}\n\n# 4. Bates SVJ Model (The Ultimate Combination)\n# 4. Bates SVJ Model (The Ultimate Combination)\nbates_params = {\n    'v0': 0.0135, \n    'kappa': 3.0290, \n    'theta': 0.0530, \n    'xi': 1.1920, \n    'rho': -0.6481,\n    'lam': 0.9871,\n    'mu_j': 0.0,\n    'delta': 0.0187\n}\n\nprint(\"‚úÖ Parameters successfully loaded into memory.\")\n\n\nLoading Calibrated Parameters...\n‚úÖ Parameters successfully loaded into memory."
  },
  {
    "objectID": "quarto_09_Model_Comparison.html#calibration-of-the-bates-model",
    "href": "quarto_09_Model_Comparison.html#calibration-of-the-bates-model",
    "title": "9. Model Comparison",
    "section": "2 Calibration of the Bates Model",
    "text": "2 Calibration of the Bates Model\n\n\nCode\ndef bates_objective(params):\n    v0, kappa, theta, xi, rho, lam, mu_j, delta = params\n    error = 0.0\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for i, K in enumerate(valid_strikes):\n            m_price = bates_call_price(S0, K, T, r, q, v0, kappa, theta, xi, rho, lam, mu_j, delta)\n            m_iv = implied_volatility(m_price, S0, K, T, r, q)\n            \n            if np.isnan(m_iv): error += 5.0\n            else: error += (m_iv - target_ivs[i])**2\n                \n    return error / len(valid_strikes)\n\n# Parameter Seeding: [v0, kappa, theta, xi, rho, lam, mu_j, delta]\n# We start exactly where Heston and Merton left off!\nbates_guess = [0.0115, 3.0293, 0.0684, 1.1963, -0.6572, 0.9923, -0.0825, 0.0779]\n\nbates_bounds = [\n    (0.005, 0.15), (0.5, 5.0), (0.005, 0.15), (0.05, 1.5), (-0.95, -0.2), # Heston bounds\n    (0.0, 3.0), (-0.5, 0.0), (0.01, 0.3)                                  # Merton bounds\n]\n\nprint(\"Optimizing Bates Parameters (Seeded Fast Search)...\")\nstart_time = time.time()\nres_bates = minimize(bates_objective, bates_guess, method='L-BFGS-B', bounds=bates_bounds)\n\nprint(f\"‚úÖ Finished in {round(time.time() - start_time, 2)}s\")\nprint(f\"Optimal Parameters: {res_bates.x}\")\nprint(f\"Mean Squared Error: {res_bates.fun:.6f}\")\n\n\nOptimizing Bates Parameters (Seeded Fast Search)...\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[8], line 26\n     24 print(\"Optimizing Bates Parameters (Seeded Fast Search)...\")\n     25 start_time = time.time()\n---&gt; 26 res_bates = minimize(bates_objective, bates_guess, method='L-BFGS-B', bounds=bates_bounds)\n     28 print(f\"‚úÖ Finished in {round(time.time() - start_time, 2)}s\")\n     29 print(f\"Optimal Parameters: {res_bates.x}\")\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_minimize.py:738, in minimize(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\n    735     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n    736                              **options)\n    737 elif meth == 'l-bfgs-b':\n--&gt; 738     res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n    739                            callback=callback, **options)\n    740 elif meth == 'tnc':\n    741     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n    742                         **options)\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:386, in _minimize_lbfgsb(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\n    383     x0 = np.clip(x0, bounds[0], bounds[1])\n    385 # _prepare_scalar_function can use bounds=None to represent no bounds\n--&gt; 386 sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n    387                               bounds=bounds,\n    388                               finite_diff_rel_step=finite_diff_rel_step)\n    390 func_and_grad = sf.fun_and_grad\n    392 nbd = zeros(n, np.int32)\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_optimize.py:291, in _prepare_scalar_function(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\n    287     bounds = (-np.inf, np.inf)\n    289 # ScalarFunction caches. Reuse of fun(x) during grad\n    290 # calculation reduces overall function evaluations.\n--&gt; 291 sf = ScalarFunction(fun, x0, args, grad, hess,\n    292                     finite_diff_rel_step, bounds, epsilon=epsilon)\n    294 return sf\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:232, in ScalarFunction.__init__(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\n    225 # Initial gradient evaluation\n    226 self._wrapped_grad, self._ngev = _wrapper_grad(\n    227     grad,\n    228     fun=self._wrapped_fun,\n    229     args=args,\n    230     finite_diff_options=finite_diff_options\n    231 )\n--&gt; 232 self._update_grad()\n    234 # Hessian evaluation\n    235 if callable(hess):\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:307, in ScalarFunction._update_grad(self)\n    305 if self._orig_grad in FD_METHODS:\n    306     self._update_fun()\n--&gt; 307 self.g = self._wrapped_grad(self.x, f0=self.f)\n    308 self.g_updated = True\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:48, in _wrapper_grad.&lt;locals&gt;.wrapped1(x, f0)\n     46 def wrapped1(x, f0=None):\n     47     ncalls[0] += 1\n---&gt; 48     return approx_derivative(\n     49         fun, x, f0=f0, **finite_diff_options\n     50     )\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:523, in approx_derivative(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\n    520     use_one_sided = False\n    522 if sparsity is None:\n--&gt; 523     return _dense_difference(fun_wrapped, x0, f0, h,\n    524                              use_one_sided, method)\n    525 else:\n    526     if not issparse(sparsity) and len(sparsity) == 2:\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:596, in _dense_difference(fun, x0, f0, h, use_one_sided, method)\n    594     x1[i] += h[i]\n    595     dx = x1[i] - x0[i]  # Recompute dx as exactly representable number.\n--&gt; 596     df = fun(x1) - f0\n    597 elif method == '3-point' and use_one_sided[i]:\n    598     x1[i] += h[i]\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:474, in approx_derivative.&lt;locals&gt;.fun_wrapped(x)\n    471 if xp.isdtype(x.dtype, \"real floating\"):\n    472     x = xp.astype(x, x0.dtype)\n--&gt; 474 f = np.atleast_1d(fun(x, *args, **kwargs))\n    475 if f.ndim &gt; 1:\n    476     raise RuntimeError(\"`fun` return value has \"\n    477                        \"more than 1 dimension.\")\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:21, in _wrapper_fun.&lt;locals&gt;.wrapped(x)\n     17 ncalls[0] += 1\n     18 # Send a copy because the user may overwrite it.\n     19 # Overwriting results in undefined behaviour because\n     20 # fun(self.x) will change self.x, with the two no longer linked.\n---&gt; 21 fx = fun(np.copy(x), *args)\n     22 # Make sure the function returns a true scalar\n     23 if not np.isscalar(fx):\n\nCell In[8], line 8, in bates_objective(params)\n      6 for i, K in enumerate(valid_strikes):\n      7     m_price = bates_call_price(S0, K, T, r, q, v0, kappa, theta, xi, rho, lam, mu_j, delta)\n----&gt; 8     m_iv = implied_volatility(m_price, S0, K, T, r, q)\n     10     if np.isnan(m_iv): error += 5.0\n     11     else: error += (m_iv - target_ivs[i])**2\n\nFile C:\\Github Code\\quant-structuring-workbench\\quant_math_engine.py:25, in implied_volatility(target_price, S, K, T, r, q)\n     22     return bs_call_price(sigma, S, K, T, r, q) - target_price\n     24 try:\n---&gt; 25     return brentq(objective, 1e-4, 5.0) \n     26 except ValueError:\n     27     return np.nan\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_zeros_py.py:798, in brentq(f, a, b, args, xtol, rtol, maxiter, full_output, disp)\n    796     raise ValueError(f\"rtol too small ({rtol:g} &lt; {_rtol:g})\")\n    797 f = _wrap_nan_raise(f)\n--&gt; 798 r = _zeros._brentq(f, a, b, xtol, rtol, maxiter, args, full_output, disp)\n    799 return results_c(full_output, r, \"brentq\")\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\optimize\\_zeros_py.py:94, in _wrap_nan_raise.&lt;locals&gt;.f_raise(x, *args)\n     93 def f_raise(x, *args):\n---&gt; 94     fx = f(x, *args)\n     95     f_raise._function_calls += 1\n     96     if np.isnan(fx):\n\nFile C:\\Github Code\\quant-structuring-workbench\\quant_math_engine.py:22, in implied_volatility.&lt;locals&gt;.objective(sigma)\n     21 def objective(sigma):\n---&gt; 22     return bs_call_price(sigma, S, K, T, r, q) - target_price\n\nFile C:\\Github Code\\quant-structuring-workbench\\quant_math_engine.py:14, in bs_call_price(sigma, S, K, T, r, q)\n     12 d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n     13 d2 = d1 - sigma * np.sqrt(T)\n---&gt; 14 return S * np.exp(-q * T) * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:2135, in rv_continuous.cdf(self, x, *args, **kwds)\n   2133 cond = cond0 & cond1\n   2134 output = zeros(shape(cond), dtyp)\n-&gt; 2135 place(output, (1-cond0)+np.isnan(x), self.badvalue)\n   2136 place(output, cond2, 1.0)\n   2137 if np.any(cond):  # call only if at least 1 entry\n\nFile C:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2085, in place(arr, mask, vals)\n   2047 @array_function_dispatch(_place_dispatcher)\n   2048 def place(arr, mask, vals):\n   2049     \"\"\"\n   2050     Change elements of an array based on conditional and input values.\n   2051 \n   (...)\n   2083 \n   2084     \"\"\"\n-&gt; 2085     return _place(arr, mask, vals)\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "quarto_09_Model_Comparison.html#visualising-the-model-against-live-data",
    "href": "quarto_09_Model_Comparison.html#visualising-the-model-against-live-data",
    "title": "9. Model Comparison",
    "section": "3 Visualising the Model against Live Data",
    "text": "3 Visualising the Model against Live Data\n\n\nCode\nprint(\"Generating Bates Volatility Smile...\")\n\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 50)\nbates_prices = [bates_call_price(S0, k, T, r, q, *res_bates.x) for k in smooth_strikes]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    bates_iv = [implied_volatility(p, S0, k, T, r, q) for p, k in zip(bates_prices, smooth_strikes)]\n\nvalid_idx = ~np.isnan(bates_iv)\nclean_strikes = np.array(smooth_strikes)[valid_idx]\nclean_iv = np.array(bates_iv)[valid_idx] * 100\n\nplt.figure(figsize=(10, 6))\nplt.scatter(valid_strikes, target_ivs * 100, color='black', label='Market IV', marker='x')\nplt.plot(clean_strikes, clean_iv, color='green', label='Bates (SVJ) Fit', linewidth=2.5)\n\nplt.title(f\"Bates (SVJ) Model Calibration\\nSPX Options on {TARGET_DATE}\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Implied Volatility (%)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\nGenerating Bates Volatility Smile...\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"Generating Theoretical Option Prices and Implied Volatilities...\")\n\n# Create a perfectly smooth x-axis for drawing the lines\nsmooth_strikes = np.linspace(min(valid_strikes), max(valid_strikes), 60)\n\n# Arrays to hold the y-axis values\nbs_iv_line = np.full_like(smooth_strikes, bs_sigma) * 100\nmerton_ivs, heston_ivs, bates_ivs = [], [], []\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for K in smooth_strikes:\n        # Merton Math\n        p_merton = merton_jump_call(S0, K, T, r, q, **merton_params)\n        merton_ivs.append(implied_volatility(p_merton, S0, K, T, r, q) * 100)\n        \n        # Heston Math\n        p_heston = heston_call_price(S0, K, T, r, q, **heston_params)\n        heston_ivs.append(implied_volatility(p_heston, S0, K, T, r, q) * 100)\n        \n        # Bates Math\n        p_bates = bates_call_price(S0, K, T, r, q, **bates_params)\n        bates_ivs.append(implied_volatility(p_bates, S0, K, T, r, q) * 100)\n\nprint(\"‚úÖ Math complete. Rendering Master Plot...\")\n\n# --- THE MASTER PLOT ---\nplt.figure(figsize=(13, 7))\n\n# 1. The Real Market Reality\nplt.scatter(valid_strikes, target_ivs, color='black', label='Raw Market IV (WRDS)', marker='x', alpha=0.7, s=40)\n\n# 2. The Theoretical Models\nplt.plot(smooth_strikes, bs_iv_line, color='gray', label='Black-Scholes (Constant Vol)', linewidth=2, linestyle=':')\nplt.plot(smooth_strikes, merton_ivs, color='blue', label='Merton (Jump Diffusion)', linewidth=2.5, linestyle='--')\nplt.plot(smooth_strikes, heston_ivs, color='red', label='Heston (Stochastic Vol)', linewidth=2.5)\nplt.plot(smooth_strikes, bates_ivs, color='green', label='Bates (SVJ - The Holy Grail)', linewidth=4, alpha=0.8)\n\n# Formatting\nplt.axvline(S0, color='black', linestyle='-.', alpha=0.3, label=f'Spot Price (S0 = {S0})')\nplt.title(f\"Quantitative Model Comparison: Fitting the S&P 500 Volatility Skew\\nTarget Expiry: {TARGET_EXDATE} (Maturity: {T:.2f} Years)\", fontsize=16)\nplt.xlabel(\"Strike Price\", fontsize=12)\nplt.ylabel(\"Implied Volatility (%)\", fontsize=12)\nplt.legend(fontsize=12, loc='upper right')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\nGenerating Theoretical Option Prices and Implied Volatilities...\n‚úÖ Math complete. Rendering Master Plot..."
  }
]