{"title":"Gap Challenge Solver","markdown":{"yaml":{"title":"Gap Challenge Solver","description":"Evolution of a Computer Vision pipeline: From brittle Template Matching to robust SVMs, integrated with a Backtracking solver.","author":"Jordan Chong","date":"2025-02-09","categories":["Computer Vision","Python","Streamlit","Algorithms","featured"],"image":"featured.png","image-alt":"Screenshot of the dashboard","client":"Personal Project","github-repo":"https://github.com/jordanchongja/gap-solver-app","body-class":"project-detail-view","include-before-body":{"text":"<div style='margin-top: 50px;'></div>"}},"headingText":"Live Demo","containsRefs":false,"markdown":"\n\n![](https://img.shields.io/badge/Python-3.10-3776AB?logo=python&logoColor=white)\n![](https://img.shields.io/badge/Status-Completed-success)\n[![GitHub](https://img.shields.io/badge/View_Source_Code-181717?logo=github&logoColor=white)](https://github.com/jordanchongja/gap-solver-app)\n\n<span style=\"color: gray; font-size: 0.9em;\">\nSource idea credited to Dianyi Yang: [kv9898/gap_challenge_solver](https://github.com/kv9898/gap_challenge_solver)\n</span>\n\n<style>\n  #quarto-margin-sidebar {\n    padding-top: 50px; /* Match this to your header margin */\n  }\n</style>\n\nYou can try the exercises yourself here:  \n- [Gap Challenge](https://www.gapchallenge.com/)  \n- [Gap Challenge Practice](https://switch-challenge-pratice.org/exercise/Aon/gapChallenge/test)  \n\n::: {.callout-note}\nYou can try the final SVM-powered solver here:\n[**Launch Streamlit App**](https://gap-solver-application.streamlit.app/)\n:::\n\n![Demonstration of Streamlit Application](images/0_success-streamlit-demo.gif)\n\n\n## The Challenge\n\nThe \"Gap\" puzzle is a logic game where players must fill a missing cell in a grid, ensuring no symbol repeats in any row or column. My goal was to build a tool that could solve this instantly from a simple screenshot.\n\n::: {layout=\"[[1,1], [1]]\"}\n![4x4 Grid](images/1_gapchallenge_4.png)\n\n![5x5 Grid](images/2_gapchallenge_5.png)\n\n**Gap Challenge** - Each shape can only appear in a row or column once. The player must use logic by elimination to find the shape that belongs in the question mark.\n:::\n\nWhile the logic puzzle itself is a standard Constraint Satisfaction Problem, **digitizing the grid from a raw image** proved to be the real engineering challenge. This post documents the three major iterations (\"Acts\") of my Computer Vision pipeline and the lessons learned from each:\n\n- **Act 1:** Naive Template Matching  \n- **Act 2:** Geometric Determination of Shapes  \n- **Act 3:** Image Classification with SVM  \n\n\n### Major Problem Faced with Image Recognition  \nA major issue was the different forms that the shapes could take based on different Gap Challenge applications. Some shapes were hollow, some backgrounds were black and some bounding grids were just lines.\n\n::: {layout=\"[[1,1,1], [1]]\"}\n![Normal Version](images/3_normal_gap.png)\n\n![Line Version](images/3_line_gap.png)\n\n![Dark-mode Version](images/3_dark_gap.png)\n\nNote the difference in shapes and grid colors. The hard part was building a robust solution that would be able to handle all these different versions.\n:::\n\n\n---\n\n## Act I: The Naive Approach (Template Matching)\n\nMy first instinct was to use **Template Matching**. I cropped reference images of each shape (Star, Circle, Square) and used `cv2.matchTemplate` to slide them over the screenshot, looking for pixel-perfect matches. I tried to account for the different size of the screenshots as well, scaling the reference shapes to match the screenshots.\n\n![Reference Images used for template matching](images/3_template_screenshots.png)\n\n![Heatmap matching of the reference images. Green boxes show a >90% match score against the reference image.](images/4_similarity_matching.gif)\n\n\n\n### The Cause of Failure\nThis method worked well on the base example but failed when tested on other forms of the puzzle. Although I implemented multi-scale matching to handle zoom levels, the approach hit a hard ceiling when the puzzle's **visual style** changed.\n\nWhen the game switched to \"Dark Mode\" or used \"Hollow\" shapes, my solid-colored templates failed immediately. To fix this, I would have needed to crop and save a new template for every possible theme variation.\n\n::: {layout-ncol=2}\n![Failure Case 1: Hollow Shapes](images/5_geometric_solver_fail_shapes.gif)\n\n![Failure Case 2: Dark Mode](images/5_geometric_solver_fail_darkmode.gif)\n:::\n\n> **Lesson:** Hard-coding pixel comparisons doesn't work if the inputs vary significantly. I needed a more robust method and decided to try and use the geometric differences in shapes to differentiate them.\n\n---\n\n## Act II: The Geometric Pivot (Heuristics)\n\nFor the second iteration, I moved to **Feature Extraction**. Instead of matching images, I used `cv2.findContours` to extract the shapes and analyze their geometric properties.\n\nI built a **Decision Tree** based on metrics like:  \n- **Vertices:** `cv2.approxPolyDP` (e.g., 3 vertices = Triangle).  \n- **Solidity:** Area divided by Convex Hull Area (distinguishes solid Squares from hollow Crosses).  \n- **Circularity:** $4\\pi \\times \\frac{Area}{Perimeter^2}$ (distinguishes Circles from Triangles).  \n\n### The \"Live Scanner\" Debugger\nTo refine my thresholds, I built a visualization tool that calculated these metrics for every cell so that I could debug the errors efficiently.\n\n![The Logic Dashboard. Watching the metrics update in real-time allowed me to tune the decision tree.](images/5_geometric_solver.gif)\n\n### The Bottleneck\nWhile robust against scale, this method suffered from \"Whack-a-Mole\" logic, where more issues would pop-up after fixing one and I realised I was trying to fix symptoms instead of fixing the root cause of the recognition issues:  \n- **The \"Hollow\" Edge Case:** A hollow square has low solidity, confusing the algorithm.  \n- **The \"Question Mark\" Problem:** The question mark consists of two separate blobs (hook + dot), breaking the single-contour logic.  \n\n::: {layout-ncol=2}\n![Failure Case: Hollow Shapes](images/6_error_question1.png)\n\n![Failure Case: Dark Mode](images/7_error_darkmode1.png)\n:::\n\nThe \"Whack-a-Mole\" logic resulted in decision trees like:\n```python\n# Attempts to handle edge cases manually\nif vertices > 8 and circularity < 0.6 and num_blobs == 1:\n    return \"Star\"\nelif num_blobs == 2:\n    return \"Question\"\n```\nThis resulted in more and more rules that I wasn't even sure of and I decided a needed a system to learn these rules itself.\n\n---\n\n## Act III: The Robust Solution (Support Vector Machines)\n\nIn the final phase, I treated this as a classification problem. I chose a **Support Vector Machine (SVM)** because they perform exceptionally well on high-dimensional binary data (pixel maps) with smaller datasets.\n\n### 1. The Data Pipeline (Solving Distribution Shift)\nA mistake that I made at the start was training on \"perfect\" data but testing on \"messy\" data.  \nI manually took multiple screenshots of the grids, with minor variations, to create a large train dataset for the SVM.  \nHowever, I realised that the SVM was not performing well, because the cropping method used for the test data was different from the perfectly cropped screenshots i was taking for the train data.\n\n To fix this, I used the test cropping algorithm to build an automated pipeline (`collect_feedback_data.py`) that extracted thousands of training examples **directly from screenshots**.\n\nThis ensured my training data included all the real-world noise (grid lines, anti-aliasing) that the model would see in production. I utilized **Otsu's Binarization** here to automatically find the optimal threshold for separating shapes from the background, solving the \"Dark Mode\" issue.\n\n![Automatic cropping and extraction of train data and grids for the SVM to train on.](images/8_success-testimages_extraction.gif)\n\n### 2. The \"Universal Translator\" (Preprocessing)\nTo make the model immune to lighting changes and user cropping errors, I engineered a standardization pipeline:\n1.  **Resize** to 64x64.  \n2.  **Corner Difference Strategy:** Calculate the average background color from the image corners and subtract it. This isolates the shape regardless of whether the theme is Dark or Light.  \n3.  **Centering:** Find the bounding box of the shape and center it on a black canvas.  \n\n### 3. Fine-Tuning the Brain (Grid Search) \nAn SVM is only as good as its hyperparameters. Default settings often lead to overfitting, where the model memorizes the training data but fails on new inputs.\n\nTo solve this, I implemented a **Grid Search** to exhaustively test combinations of `C` (Regularization) and `Gamma` (Kernel Coefficient). This allowed me to mathematically find the configuration that ignored minor pixel noise while maintaining a distinct decision boundary between similar shapes (like the Circle and the hollow Ring).\n\n\n### 4. X-Ray Vision (Verification)\nMachine Learning models can be \"black boxes.\" To trust the system, I built a side-by-side debugger comparing the **Human View** (RGB) against the **Machine View** (Preprocessed). This allowed me to catch preprocessing errors—like the dot of the '?' being filtered out as noise—before retraining the model.\n\n![SVM Debugger. Left is the game input; Right is what the model actually sees.](images/8_success-svm_view.gif)\n\n---\n\n## The Solver Logic (Backtracking)\n\nWith a reliable Vision pipeline returning a 2D matrix (e.g., `[['Star', 'Empty'], ['Circle', 'Square']]`), the problem reduced to a standard algorithm.\n\nI implemented a **Recursive Backtracking** solver. It works by:  \n1.  **Scanning** for an empty cell.  \n2.  **Guessing** a shape from the available options.  \n3.  **Validating** row/column constraints.  \n4.  **Recursively** attempting to solve the rest of the grid.  \n5.  **Backtracking** if a guess leads to a contradiction.  \n\n```python\ndef solve_with_backtracking(board, universe):\n    # Base Case: No empty spots left\n    if not find_empty(board): return True \n    \n    row, col = find_empty(board)\n    \n    for shape in universe:\n        if is_valid_move(board, shape, row, col):\n            board[row][col] = shape\n            # Recursive Step\n            if solve_with_backtracking(board, universe): return True\n            # Backtrack\n            board[row][col] = 'blank'\n            \n    return False\n```\n\n---\n\n## Conclusion & Tech Stack\n\nThis project was a lesson in selecting the right tool for the job. While deep learning (CNNs) could have solved this, it would have been overkill for a 5-class problem. An SVM combined with robust **Data Engineering** provided a lightweight, instant, and highly accurate solution (99% accuracy on test set).\n\n* **Language:** Python 3.10\n* **Vision:** OpenCV (Canny, Otsu's Binarization, Contours)\n* **ML:** Scikit-Learn (SVM, GridSearch)\n* **Frontend:** Streamlit","srcMarkdownNoYaml":"\n\n![](https://img.shields.io/badge/Python-3.10-3776AB?logo=python&logoColor=white)\n![](https://img.shields.io/badge/Status-Completed-success)\n[![GitHub](https://img.shields.io/badge/View_Source_Code-181717?logo=github&logoColor=white)](https://github.com/jordanchongja/gap-solver-app)\n\n<span style=\"color: gray; font-size: 0.9em;\">\nSource idea credited to Dianyi Yang: [kv9898/gap_challenge_solver](https://github.com/kv9898/gap_challenge_solver)\n</span>\n\n<style>\n  #quarto-margin-sidebar {\n    padding-top: 50px; /* Match this to your header margin */\n  }\n</style>\n\nYou can try the exercises yourself here:  \n- [Gap Challenge](https://www.gapchallenge.com/)  \n- [Gap Challenge Practice](https://switch-challenge-pratice.org/exercise/Aon/gapChallenge/test)  \n\n::: {.callout-note}\n### Live Demo\nYou can try the final SVM-powered solver here:\n[**Launch Streamlit App**](https://gap-solver-application.streamlit.app/)\n:::\n\n![Demonstration of Streamlit Application](images/0_success-streamlit-demo.gif)\n\n\n## The Challenge\n\nThe \"Gap\" puzzle is a logic game where players must fill a missing cell in a grid, ensuring no symbol repeats in any row or column. My goal was to build a tool that could solve this instantly from a simple screenshot.\n\n::: {layout=\"[[1,1], [1]]\"}\n![4x4 Grid](images/1_gapchallenge_4.png)\n\n![5x5 Grid](images/2_gapchallenge_5.png)\n\n**Gap Challenge** - Each shape can only appear in a row or column once. The player must use logic by elimination to find the shape that belongs in the question mark.\n:::\n\nWhile the logic puzzle itself is a standard Constraint Satisfaction Problem, **digitizing the grid from a raw image** proved to be the real engineering challenge. This post documents the three major iterations (\"Acts\") of my Computer Vision pipeline and the lessons learned from each:\n\n- **Act 1:** Naive Template Matching  \n- **Act 2:** Geometric Determination of Shapes  \n- **Act 3:** Image Classification with SVM  \n\n\n### Major Problem Faced with Image Recognition  \nA major issue was the different forms that the shapes could take based on different Gap Challenge applications. Some shapes were hollow, some backgrounds were black and some bounding grids were just lines.\n\n::: {layout=\"[[1,1,1], [1]]\"}\n![Normal Version](images/3_normal_gap.png)\n\n![Line Version](images/3_line_gap.png)\n\n![Dark-mode Version](images/3_dark_gap.png)\n\nNote the difference in shapes and grid colors. The hard part was building a robust solution that would be able to handle all these different versions.\n:::\n\n\n---\n\n## Act I: The Naive Approach (Template Matching)\n\nMy first instinct was to use **Template Matching**. I cropped reference images of each shape (Star, Circle, Square) and used `cv2.matchTemplate` to slide them over the screenshot, looking for pixel-perfect matches. I tried to account for the different size of the screenshots as well, scaling the reference shapes to match the screenshots.\n\n![Reference Images used for template matching](images/3_template_screenshots.png)\n\n![Heatmap matching of the reference images. Green boxes show a >90% match score against the reference image.](images/4_similarity_matching.gif)\n\n\n\n### The Cause of Failure\nThis method worked well on the base example but failed when tested on other forms of the puzzle. Although I implemented multi-scale matching to handle zoom levels, the approach hit a hard ceiling when the puzzle's **visual style** changed.\n\nWhen the game switched to \"Dark Mode\" or used \"Hollow\" shapes, my solid-colored templates failed immediately. To fix this, I would have needed to crop and save a new template for every possible theme variation.\n\n::: {layout-ncol=2}\n![Failure Case 1: Hollow Shapes](images/5_geometric_solver_fail_shapes.gif)\n\n![Failure Case 2: Dark Mode](images/5_geometric_solver_fail_darkmode.gif)\n:::\n\n> **Lesson:** Hard-coding pixel comparisons doesn't work if the inputs vary significantly. I needed a more robust method and decided to try and use the geometric differences in shapes to differentiate them.\n\n---\n\n## Act II: The Geometric Pivot (Heuristics)\n\nFor the second iteration, I moved to **Feature Extraction**. Instead of matching images, I used `cv2.findContours` to extract the shapes and analyze their geometric properties.\n\nI built a **Decision Tree** based on metrics like:  \n- **Vertices:** `cv2.approxPolyDP` (e.g., 3 vertices = Triangle).  \n- **Solidity:** Area divided by Convex Hull Area (distinguishes solid Squares from hollow Crosses).  \n- **Circularity:** $4\\pi \\times \\frac{Area}{Perimeter^2}$ (distinguishes Circles from Triangles).  \n\n### The \"Live Scanner\" Debugger\nTo refine my thresholds, I built a visualization tool that calculated these metrics for every cell so that I could debug the errors efficiently.\n\n![The Logic Dashboard. Watching the metrics update in real-time allowed me to tune the decision tree.](images/5_geometric_solver.gif)\n\n### The Bottleneck\nWhile robust against scale, this method suffered from \"Whack-a-Mole\" logic, where more issues would pop-up after fixing one and I realised I was trying to fix symptoms instead of fixing the root cause of the recognition issues:  \n- **The \"Hollow\" Edge Case:** A hollow square has low solidity, confusing the algorithm.  \n- **The \"Question Mark\" Problem:** The question mark consists of two separate blobs (hook + dot), breaking the single-contour logic.  \n\n::: {layout-ncol=2}\n![Failure Case: Hollow Shapes](images/6_error_question1.png)\n\n![Failure Case: Dark Mode](images/7_error_darkmode1.png)\n:::\n\nThe \"Whack-a-Mole\" logic resulted in decision trees like:\n```python\n# Attempts to handle edge cases manually\nif vertices > 8 and circularity < 0.6 and num_blobs == 1:\n    return \"Star\"\nelif num_blobs == 2:\n    return \"Question\"\n```\nThis resulted in more and more rules that I wasn't even sure of and I decided a needed a system to learn these rules itself.\n\n---\n\n## Act III: The Robust Solution (Support Vector Machines)\n\nIn the final phase, I treated this as a classification problem. I chose a **Support Vector Machine (SVM)** because they perform exceptionally well on high-dimensional binary data (pixel maps) with smaller datasets.\n\n### 1. The Data Pipeline (Solving Distribution Shift)\nA mistake that I made at the start was training on \"perfect\" data but testing on \"messy\" data.  \nI manually took multiple screenshots of the grids, with minor variations, to create a large train dataset for the SVM.  \nHowever, I realised that the SVM was not performing well, because the cropping method used for the test data was different from the perfectly cropped screenshots i was taking for the train data.\n\n To fix this, I used the test cropping algorithm to build an automated pipeline (`collect_feedback_data.py`) that extracted thousands of training examples **directly from screenshots**.\n\nThis ensured my training data included all the real-world noise (grid lines, anti-aliasing) that the model would see in production. I utilized **Otsu's Binarization** here to automatically find the optimal threshold for separating shapes from the background, solving the \"Dark Mode\" issue.\n\n![Automatic cropping and extraction of train data and grids for the SVM to train on.](images/8_success-testimages_extraction.gif)\n\n### 2. The \"Universal Translator\" (Preprocessing)\nTo make the model immune to lighting changes and user cropping errors, I engineered a standardization pipeline:\n1.  **Resize** to 64x64.  \n2.  **Corner Difference Strategy:** Calculate the average background color from the image corners and subtract it. This isolates the shape regardless of whether the theme is Dark or Light.  \n3.  **Centering:** Find the bounding box of the shape and center it on a black canvas.  \n\n### 3. Fine-Tuning the Brain (Grid Search) \nAn SVM is only as good as its hyperparameters. Default settings often lead to overfitting, where the model memorizes the training data but fails on new inputs.\n\nTo solve this, I implemented a **Grid Search** to exhaustively test combinations of `C` (Regularization) and `Gamma` (Kernel Coefficient). This allowed me to mathematically find the configuration that ignored minor pixel noise while maintaining a distinct decision boundary between similar shapes (like the Circle and the hollow Ring).\n\n\n### 4. X-Ray Vision (Verification)\nMachine Learning models can be \"black boxes.\" To trust the system, I built a side-by-side debugger comparing the **Human View** (RGB) against the **Machine View** (Preprocessed). This allowed me to catch preprocessing errors—like the dot of the '?' being filtered out as noise—before retraining the model.\n\n![SVM Debugger. Left is the game input; Right is what the model actually sees.](images/8_success-svm_view.gif)\n\n---\n\n## The Solver Logic (Backtracking)\n\nWith a reliable Vision pipeline returning a 2D matrix (e.g., `[['Star', 'Empty'], ['Circle', 'Square']]`), the problem reduced to a standard algorithm.\n\nI implemented a **Recursive Backtracking** solver. It works by:  \n1.  **Scanning** for an empty cell.  \n2.  **Guessing** a shape from the available options.  \n3.  **Validating** row/column constraints.  \n4.  **Recursively** attempting to solve the rest of the grid.  \n5.  **Backtracking** if a guess leads to a contradiction.  \n\n```python\ndef solve_with_backtracking(board, universe):\n    # Base Case: No empty spots left\n    if not find_empty(board): return True \n    \n    row, col = find_empty(board)\n    \n    for shape in universe:\n        if is_valid_move(board, shape, row, col):\n            board[row][col] = shape\n            # Recursive Step\n            if solve_with_backtracking(board, universe): return True\n            # Backtrack\n            board[row][col] = 'blank'\n            \n    return False\n```\n\n---\n\n## Conclusion & Tech Stack\n\nThis project was a lesson in selecting the right tool for the job. While deep learning (CNNs) could have solved this, it would have been overkill for a 5-class problem. An SVM combined with robust **Data Engineering** provided a lightweight, instant, and highly accurate solution (99% accuracy on test set).\n\n* **Language:** Python 3.10\n* **Vision:** OpenCV (Canny, Otsu's Binarization, Contours)\n* **ML:** Scikit-Learn (SVM, GridSearch)\n* **Frontend:** Streamlit"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"include-before-body":{"text":"<div style='margin-top: 50px;'></div>"},"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.27","theme":"cosmo","title":"Gap Challenge Solver","description":"Evolution of a Computer Vision pipeline: From brittle Template Matching to robust SVMs, integrated with a Backtracking solver.","author":"Jordan Chong","date":"2025-02-09","categories":["Computer Vision","Python","Streamlit","Algorithms","featured"],"image":"featured.png","image-alt":"Screenshot of the dashboard","client":"Personal Project","github-repo":"https://github.com/jordanchongja/gap-solver-app","body-class":"project-detail-view"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}